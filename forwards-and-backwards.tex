\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{amssymb}
\usepackage{ntheorem}
\usepackage{graphics} 
\usepackage{amsmath}
\usepackage[color=yellow]{todonotes}
\usepackage{url}
\usepackage{xcolor}
\usepackage{longtable}
\usepackage[hidelinks]{hyperref}
\usepackage{bbm}

\usepackage{natbib}

\newenvironment {proof}{{\noindent\bf Proof }}{\hfill $\Box$ \medskip}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{hypothesis}[theorem]{Hypothesis}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}[theorem]{Example}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{assumptions}[theorem]{Assumptions}
\newtheorem{terminology}[theorem]{Terminology}
\newtheorem*{terminology-non}[theorem]{Terminology}

\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}
\def \non{{\nonumber}}
\def \hat{\widehat}
\def \tilde{\widetilde}
\def \bar{\overline}
\newcommand{\IP}{\mathbb P}
\newcommand{\IQ}{\mathbb Q}
\newcommand{\IE}{\mathbb E}
\newcommand{\IR}{\mathbb R}
\newcommand{\IZ}{\mathbb Z}
\newcommand{\IN}{\mathbb N}
\newcommand{\IT}{\mathbb T}
\newcommand{\IC}{\mathbb C}
\newcommand{\ind}{\mathbf{1}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\grad}{\nabla}
\newcommand{\dif}{\mathrm{d}\,}

%%%%%%% notation
\newcommand{\DG}{\mathcal{B}}  % generator of the dispersal process
\newcommand{\DD}{\mathcal{D}}  % the second-order part of the generator of dispersal
\newcommand{\meanq}{\vec b}    % mean of dispersal, times theta
\newcommand{\covq}{C}     % covariance matrix of dispersal, times theta
\newcommand{\kernel}{\rho}  % interaction kernels
\newcommand{\smooth}[1]{\kernel_{#1} \! * \!}  % convolution by the interaction kernel
\newcommand{\wavespeed}{\mathfrak{c}}    % speed (vector) of a wave
\newcommand{\Lgen}{\mathcal{L}}    % generator of a lineage
\newcommand{\Pgen}{\mathcal{P}}    % generator of the population process
\newcommand{\lp}{\xi}              % process with levels
\newcommand{\labelspace}{\mathcal{I}} % space of labels
\newcommand{\concat}{\oplus}   % concatenation of labels
\newcommand{\measures}{\mathcal{M}_F(\IR^d)} % finite measures on Rd
\newcommand{\cmeasures}{\mathcal{M}_F(\overline{\IR}^d)} % finite measures on compactified Rd
\newcommand{\lpmeasures}{\mathcal{M}(\overline{\IR^d} \times [0,\infty))} % locally finite measures on space x level


\newcommand{\plr}[1]{\todo[inline]{Peter: #1}}
\newcommand{\comment}[1]{{\color{blue} \it #1}}

\begin{document}

\title{\large{\bf
Looking forwards and backwards through locally regulated populations
}}

\author{ \begin{small}
\begin{tabular}{ll}                              
Alison M. Etheridge 
 & Thomas G. Kurtz \\   
Department of Statistics & Departments of Mathematics and Statistics\\       
Oxford University & University of Wisconsin - Madison \\                   
24-29 St Giles & 480 Lincoln Drive\\                                                         
Oxford OX1 3LB & Madison, WI  53706-1388\\
UK & USA \\                        
etheridg@stats.ox.ac.uk & kurtz@math.wisc.edu     \\
\url{http://www.stats.ox.ac.uk/~etheridg/} & 
\url{http://www.math.wisc.edu/~kurtz/}  \\       \\
\\
Ian Letter&  Peter L. Ralph 
\\   
Department of Statistics & Department of Mathematics \\
Oxford University &University of Oregon\\                   
24-29 St Giles & Fenton Hall\\
Oxford OX1 3LB & Eugene, OR 97403-1222\\
UK & USA \\
restucci@stats.ox.ac.uk  & plr@oregon.edu \\
\url{https://www.stats.ox.ac.uk/~restucci/}&
\url{https://math.uoregon.edu/profile/plr} \\
\\
Terence Tsui 
 &  \\   
Department of Statistics & \\
Oxford University & \\                   
24-29 St Giles& \\
Oxford OX1 3LB & \\
UK & \\
terence.tsui@sjc.ox.ac.uk &      \\
\url{https://www.maths.ox.ac.uk/people/terence.tsui}&  \\
\end{tabular}
\end{small}}

\date{\today}
\maketitle


\begin{abstract}

We introduce a broad class of mechanistic spatial models to describe how spatially 
heterogeneous populations live, die, and reproduce. Birth and death rates can depend 
both on spatial position and local population density, defined at a 
spatial position $x$ to be the convolution 
of the point measure encoding the distribution of individuals in the population
with a suitable non-negative integrable kernel
centred on $x$. We pass to three different scaling limits:
an interacting superprocess, a nonlocal partial differential equation (PDE), 
and a classical PDE. 
The classical PDE is obtained both by a two-step convergence argument, in which we 
first scale time and population size and
pass to the nonlocal PDE, 
and then scale the kernel that 
determines local population density; and 
in the important special case in which the limit is a 
reaction diffusion equation, directly 
by simultaneously scaling the kernel and the
timescale and population size in our individual based model.

A novelty of our model is that in describing the dynamics we explicitly
model a juvenile phase: the number of juveniles produced by an individual depends
on local population density at the location of the parent. 
Juvenile offspring are 
thrown off in a (possibly heterogeneous, anisotropic) Gaussian distribution 
around the location of the parent; they then reach (instant) maturity with a probability
that can depend on the local population density (possibly measured through convolution
with a different kernel from that used in determining fecundity) 
at the location at which they land. 
Although we only record mature individuals, a trace of this two-step description 
remains in our population models, resulting in novel limits in which the spatial
motion is governed by a nonlinear diffusion. 

Using a lookdown representation, we are able to retain information about 
genealogies relating individuals in our population and, in the case of 
deterministic limiting models, we use this to deduce 
the backwards in time motion of the ancestral lineage of an individual
sampled from the population. We observe that
knowing the history of the population density is not enough to determine the motion of
ancestral lineages in our model. We also investigate (and
contrast) the behaviour of lineages for 
three different deterministic models of a population expanding its range as a 
travelling wave: the Fisher-KPP equation, the Allen-Cahn equation, and a porous 
medium equation with logistic growth.


\vspace{.1in}

	\comment{Need to thank KAVLI, Paris Cit\'e, various funders}

\vspace{.1in}

\noindent {\bf Key words:}  population model, interacting superprocess, 
lookdown construction, porous medium equation, 
reaction-diffusion equation, travelling waves, genealogies,
Fisher-KPP equation

\vspace{.1in}



\noindent {\bf MSC 20}10 {\bf Subject Classification:}  Primary:  
%60J25, 92D10, 92D15, 92D25 92D40  
\\Secondary:   %60F05, 60G09, 60G55, 60G57, 60H15, 60J68
 
\end{abstract}
\tableofcontents
\newpage


%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{introduction}

\comment{TO CITE: \citet{ghosh2022emergent}}

As one takes a journey, long or short, the landscape changes:
forests thicken or thin or change their composition;
springtime grasslands host intergrading mosaics of different types of flowers.
The aim of this paper is to introduce and study
a broad class of mechanistic spatial models
that might describe how spatially heterogeneous populations live, die, and reproduce.
Questions that we (start to) address include:
How does population density change across space and time?
How might we learn about the underlying dynamics from genealogical or genetic data?
And, how does genetic ancestry spread across geography
when looking back through time in these populations?

Reproduction of individuals naturally leads to spatial branching process models,
including branching random walk, 
branching Brownian motion, and the Dawson-Watanabe superprocesses.  
However, as a result of the branching assumption (once born, individuals behave 
independently of one another), 
a population evolving according to any of these models will
either die out or grow without bound and, in so doing, it can
develop clumps of arbitrarily large density and extent. 
Our starting point here is an individual-based model of a single species 
in continuous space in which
birth, death, and establishment may all depend on local population density
as well as on spatial location,
allowing for stable populations through density-dependent feedback.

Although it is often mathematically convenient to assume that individuals follow
Brownian motion during their lifetime, 
in our model, offspring
are thrown off according to some 
spatial distribution centred on the location of the parent.   
This is particularly appropriate for modelling plant populations, in which
this dispersal of offspring around the parent is the only source of
spatial motion.

Most models do not distinguish between juveniles and adults, so,
for example, the number of adults produced by a single parent is determined
by the degree of crowding at the location of the parent. A novelty
of the models that we introduce here is that, although we shall only
follow the adult population, in formulating the dynamics of the
models we shall distinguish
between production of juveniles, which will depend upon the location of 
the adult, and their successful establishment, which will depend on the
location in which a juvenile lands. The result is that not only the absolute 
number, but also the spatial distribution
around their parent, 
of those offspring that survive to adulthood
will depend upon the local population 
density. 

We shall consider three different classes of scaling limits for our model.
The first yields a class of (generalised) superprocesses in which coefficients 
governing both the spatial motion and the branching components of the process can depend
on local population density; the second is a corresponding class of deterministic
non-local differential equations; and the third are classical PDEs.
We measure local population density around a point by convolving with
a smooth kernel $\rho(\cdot)$, which may differ
for the two stages of reproduction. 
When the limit is deterministic,
it is a (weak) solution of an equation of the form
\begin{equation}
	\label{general deterministic limit}
        \partial_t \varphi_t(x)
        =
        r\left(x, \varphi_t \right)
        \DG^* \left[
            \varphi_t(\cdot)
            \gamma\big( x, \varphi_t \big)
        \right](x)
        +
        \varphi_t(x)
        F\left(x, \varphi_t \right)
        ,
\end{equation}
where $\DG^*$ is (the adjoint of) a strictly uniformly elliptic
second order differential operator, typically
the Laplacian.
We shall be more specific about the parameters below. 

By replacing $\rho$ by
$\rho^\epsilon(\cdot)=\rho(\cdot/\epsilon)/\epsilon^d$, 
we can also scale the ``width'' of the 
region over which we measure local population density. 
When the population follows~(\ref{general deterministic limit}), 
we can expect that taking a second limit by scaling the kernels appearing
in $r$, $\gamma$, and $F$ in this way, so that
in the limit as $\epsilon\to 0$
interactions are pointwise, we should
recover a nonlinear PDE.
We verify that this is indeed the case in two important examples:
a special case of the porous medium equation with a logistic growth term, in which we 
retain a signal of our two stage approach to modelling in the nonlinear diffusion term
and the limiting equation takes the form 
\begin{equation}
	\label{PME1}
	\partial_t \varphi = \Delta (\varphi^2)+\varphi(1-\varphi);
\end{equation}
and a wide class of semi-linear PDEs of the form 
\begin{equation}
\label{semilinear PDE}
\partial_t \varphi = \DG^*\varphi+ \varphi F(\varphi),
\end{equation}
which includes the Fisher-KPP equation and the Allen-Cahn equation.

It is of interest to understand under what conditions we can replace the two-step limiting
process described above by one in which we simultaneously scale the 
kernels
and the other parameters in 
our population model to arrive at the PDE limit.
This is mathematically much more challenging, but we establish such 
one-step convergence in cases for which the limit is a classical 
reaction diffusion equation of the form~(\ref{semilinear PDE}) with $\DG=\Delta$
and $\rho_F$ is a Gaussian density. We allow a wide class of reaction terms, $F$, so
that the Fisher-KPP equation emerges as a special case.

We remark that such results on (one-step) convergence to reaction diffusion 
equation limits have been
achieved for a variety of interacting particle systems. The vast majority of these
results concern spin systems \comment{REFs}. However,~\cite{flandoli/huang:2021} 
obtain the Fisher-KPP equation (that is equation~(\ref{semilinear PDE}) with 
$\DG=\Delta$ and $F(\varphi)=1-\varphi$) as a scaling limit of interacting diffusions in
$\IR^d$. In that setting a major challenge (also apparent in our models), not
faced in spin systems, is 
the control of concentration of particles. We also note that the scaling 
considered in~\cite{flandoli/huang:2021} is different from that considered
below. In their language we consider only `moderate interactions', meaning that
the number of individuals in the neighbourhood over which we measure local
population density tends to infinity, whereas they also consider the situation in which
that number remains finite; furthermore, whereas we shall consider a scaling in which 
the reproduction mechanism is near-critical
and look at large timescales,~\cite{flandoli/huang:2021}
only scale the number of
particles in the system and the size of the neighbourhood over which indivduals
interact. 

The history of a natural population is often only accessible indirectly, 
through patterns of genetic diversity that have been laid down; from 
genetic data, one can try to infer the genealogical trees that relate 
individuals in a sample from the population, and these have been shaped 
by its history. It is therefore of interest
to establish information about the distribution of genealogical trees 
under our population model.
To achieve this we exploit a lookdown construction. 
Lookdown constructions were first introduced in~\cite{donnelly/kurtz:1996} to provide 
a mechanism for retaining information about genealogical relationships between individuals
sampled from a population evolving according to the Moran model when passing to the 
infinite population limit.
Since then, they have been extended to a huge range of models. Of particular
relevance to our work here are the papers of~\cite{kurtz/rodrigues:2011} 
and~\cite{etheridge/kurtz:2018}, in which
lookdown constructions are provided for a wide variety of population models, including
spatially structured branching processes.

In general, even armed with a lookdown construction, calculation of relevant statistics 
of the genealogy remains a difficult question. 
However, in special
circumstances, some progress can be made. As an illustration, 
we shall consider a scenario that
has received a great deal of attention in recent years, in which 
a population is expanding into new territory as a travelling wave. 
In Section~\ref{ancestral lineages for travelling waves}
we shall consider three different (deterministic) wavefronts, moving 
across $\IR^1$, and we shall explore the position of a single ancestral 
lineage relative to that front. 

Most work has focused on models that caricature the classical Fisher-KPP equation with a 
stochastic term, i.e.
$$dw=\big(\Delta w +sw(1-w)\big)dt +\sqrt{\frac{\alpha(w)}{N}}W(dt,dx),$$
where $W$ is space-time white noise, and $N$ is a measure of the 
local population density. The coefficient
$\alpha(w)$ is generally taken to be either
$w$, corresponding to a superprocess limit, or $w(1-w)$ giving a 
spatial analogue of a Wright-Fisher diffusion. 
For very large $N$, and on suitable timescales, 
starting with the pioneering work of~\cite{brunet/derrida/mueller/munier:2006},
a considerable body of evidence has been amassed to underpin the
conjecture that for this, and a wide class of 
related models, 
genealogies converge to a 
Bolthausen-Sznitman coalescent. %, e.g.~\cite{brunet/derrida/mueller/munier:2007}.
This reflects the fact that, for this equation, 
ancestral lineages become trapped in 
the wavefront, where the growth rate of the population is highest. 
Once there, they will experience 
rapid periods of coalescence 
corresponding to significant proportions of individuals in the front being 
descended from particularly reproductively successful ancestors. 

If one replaces the logistic growth term of the classical
Fisher-KPP equation with a nonlinearity that reflects cooperative
behaviour in the population, such as
\begin{equation}
	\label{Birzu nonlinearity}
	wF(w)=w(1-w)(Cw-1),
\end{equation}
then, for sufficiently large $C$ (strong cooperation),
the nature of the deterministic
wave changes from ``pulled'' to ``pushed'', \cite{birzu/hallatschek/korolev:2017}.
In that setting, the genealogies will be quite different
from the Fisher-KPP case. \comment{Could refer to more recent Birzu et al paper here.}
For example, \cite{etheridge/penington:2022}
show that for a discrete space model corresponding to this 
nonlinearity with $C>2$, after suitable scaling, the genealogy of a
sample converges not to a Bolthausen-Sznitman coalescent, but to
a Kingman coalescent. 
The reason, roughly, is that ancestral lineages
settle to a stationary distribution relative to the position of the 
wavefront which puts very little weight close to the `tip' of the wave, so
that when ancestral lineages meet
it is typically at a location in which population density is
high, where no single ancestor produces a disproportionately large number of 
descendants in a short space of time. 

The shape of the wave is not determined solely by the reaction term.
For example, as a result of the nonlinear diffusion, 
for suitable initial conditions, the solution to the one-dimensional 
porous medium equation with logistic growth~(\ref{PME1}) converges to a travelling
wave with a sharp cut-off; i.e., in contrast to the classical 
Fisher KPP equation, the solution at time $t$
vanishes beyond $x=x_0+ct$ for some constant wavespeed
$c>0$, \cite{kamin/rosenau:2004}.
As a first step towards understanding what we should expect in models with
nonlinear diffusion, one can ask about the position of an ancestral lineage
relative to the wavefront in the deterministic models. In 
Section~\ref{ancestral lineages for travelling waves}
we shall see that in our framework, even with logistic growth, the nonlinear diffusion 
corresponding to the porous medium equation results
in a stationary distribution that is concentrated behind the wavefront, leading 
us to conjecture that in the stochastic equation
the cooperative behaviour captured by the nonlinear diffusion will also 
result in a qualitatively different pattern of coalescence to that seen under the 
stochastic Fisher-KPP equation. Indeed, we believe that it should be feasible to 
show that in an appropriate limit one recovers a Kingman coalescent.

% % % % % % % % % % % % % %
\paragraph{Structure of the paper}

\comment{This paragraph to be rewritten last}

In this paper we study scaling limits of spatial population models,
obtaining convergence of both the population process
(i.e., the population density as a function of time,
although strictly speaking it is a measure that may not have a density)
and of lineages traced back through such a population.
We retain information about lineages as we pass to the scaling limit
by means of a lookdown construction.
First, in Section~\ref{sec: Model and main results},
we describe the model and the main results
(Theorems~\ref{thm:nonlocal_convergence}, \ref{thm:local_convergence}, and~\ref{thm:lineages}).
Next, in Section~\ref{sec:applications}, we discuss a few striking consequences of these results,
namely, X Y and Z \comment{TODO}.
In Section~\ref{sec:heuristics}, we provide heuristic explanations
of why the theorems ought to be true,
and some key ideas behind them.
In Section \ref{sec:lookdown} we define and discuss the lookdown construction.
This is followed by the technical proofs in Section~\ref{sec:proofs}.

\comment{What's in the Appendix?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model and main results}
    \label{sec: Model and main results}

Our model is one of individuals distributed across a continuous space 
which we shall take to be $\IR^d$. 
For applications, $d=1$ or $d=2$, but our main results apply more generally.
At time zero, the population is distributed over a bounded region, with 
$\bigO(N)$ individuals per unit area in that region,
so the total number of individuals will also be $\bigO(N)$.
The population changes in continuous time, and
we encode the state of the population at time $t$ by a counting measure $X(t)$,
which assigns one unit of mass to the location of each individual.

Population dynamics are controlled by three quantities,
birth ($\gamma$), establishment ($r$), and death ($\mu$),
each of which can depend on spatial location and local population density
in a way specified below.
Each individual gives birth at rate $\gamma$ to a single (juvenile) offspring,
which is dispersed according to a kernel $q(x, \cdot)$ away from the location $x$ of the parent.
We assume that $q$ is the density of a multivariate Gaussian,
allowing a nonzero mean and anisotropic variance.
Both the mean and covariance can change across space,
but do not depend on population density.
The offspring does not necessarily survive to be counted in the population:
it ``establishes'' with probability $r$,
or else it dies immediately.
Independently, each individual dies with rate $\mu$.

We aim to capture universal behaviour by passing to a scaling limit. Specifically, 
we shall take the ``density'', $N$, to infinity,
and also scale time by a factor of $\theta$,
in such a way that defining $\eta^N(t) = X(\theta t) / N$,
the process $\{\eta^N(t)\}_{t \ge 0}$
will converge to a suitable measure-valued process
as $N$ and $\theta$ tend to infinity,
with the nature of the limit depending on how they tend to infinity together.
Evidently, we shall also need to scale the dispersal kernel if we are to obtain a
nontrivial limit. We write $q_\theta(x,\cdot)$ for the density of the 
multivariate Gaussian obtained by
multiplying the mean and variance components corresponding to $q(x,\cdot)$ by $1/\theta$.

Birth, establishment, and death can depend on the location of the individual
and the local population density.
Since we would like the population density to scale with $N$,
these are functions of $X/N$, i.e.,
the counting measure with mass $1/N$ placed at the location of each individual.
First consider birth rates, defined by
a nonnegative function $\gamma(x, m) : \IR^d \times \IR_{\ge 0} \to \IR_{\ge 0}$
of location $x$ 
and local population density $m$.
Local population density is defined as the convolution of $X/N$ with 
a smooth (non-negative, integrable) kernel $\rho_\gamma(\cdot)$.
We write this convolution as $\smooth{\gamma} X/N$.
Then, when the state of the population is $X$, an individual at location $x$ 
gives birth to a single juvenile offspring at rate
$\gamma(x, \smooth{\gamma} X(x) / N)$.
Similarly, the establishment probability of an offspring at location $y$ is
is $r(y, \smooth{r} X(y) / N)$,
where $r(y, m) : \IR^d \times \IR_{\ge 0} \to [0, 1]$
and again $\smooth{r} X$ is the convolution of 
$X/N$ with the smooth kernel $\rho_r$.

We shall write $\mu_\theta(x, X/N)$ for the per-capita death rate of mature individuals
in the population.
In order for the 
population density to change over timescales of order $\theta$, we should like the net 
per capita reproductive rate 
to scale as $1/\theta$. 
In classical models, in which $r$, $\gamma$, and $\mu$ are constant, 
this quantity is simply $r\gamma -\mu$. 
Here, because production of juveniles and their establishment are
mediated by population density measured relative to different points, 
the net reproductive rate will take a more complicated form. 
In particular, the total rate of production of mature offspring by an individual at $x$ will be
\[
	\gamma\big(x,\smooth{\gamma}X(x)/N\big)\int r\big(y, \smooth{r}X(y)/N\big)q_{\theta}(x,dy).
\]
Nonetheless it will be convenient to define the death rate $\mu_\theta$ in terms of 
its deviation from $r\gamma$. 
To this end, we define
the death rate of an individual at $x$ to be
\begin{align} \label{eqn:mu_defn}
    \mu_\theta(x)
    =
    \max\left\{0, 
        r(x, \smooth{r} X(x) / N) \gamma(x, \smooth{\gamma} X(x) / N)
        - \frac{1}{\theta} F(x, \smooth{F} X(x) / N)
    \right\} ,
\end{align}
where $F(x, m) : \IR^d \times \IR_{\ge 0} \to \IR$ and
$\rho_F$ is again a smooth kernel. 

Note that each of the three demographic parameters $r$, $\gamma$, $F$,
depends on local density,
measured by convolution with a smooth kernel. Each of the three kernels can be different.
As a result, death rate depends (in principle) on population densities measured in 
three different ways, 
so that we could write 
$\mu_\theta(x) = \mu_\theta(x, \smooth{\gamma} X(x) / N, \smooth{r} X(x) / N, \smooth{F} X(x) / N)$.
This may seem unnecessarily complex.
However, not only is it natural from a 
biological perspective, it also turns out to be necessary if we are
to capture nontrivial examples in our scaling results.

\begin{remark}
Although this model allows fairly general birth and death mechanisms,
there are a number of limitations.
Perhaps most obviously, to simplify the notation
individuals give birth to only one offspring at a time,
although this restriction could be easily lifted
\citep[as in Section 3.4 of][]{etheridge/kurtz:2018}.
Furthermore, individuals do not move during their lifetime,
and the age of an individual does not affect its fecundity or death rate.
Finally, there is no notion of mating
(although limitations on reproduction due to availability of mates can be incorporated into the birth rate, $\gamma$),
so the lineages we follow will be uniparental.
For these reasons, the model is most obviously applicable to bacterial populations or selfing plants,
although we do not anticipate that incorporation of these complications
will change the general picture.
\end{remark}


For each $N$ and $\theta$, the rescaled process
$\big(\eta^N_t\big)_{t\geq 0} := \big(X(\theta t)/N\big)_{t\geq 0}$, 
takes values in the space of c\`adl\`ag paths in
$\measures$ (the space of finite measures on $\IR^d$
endowed with the weak topology).

\begin{notation}
Expressions like $\gamma(x, \smooth{\gamma} \eta(x))$ will appear repeatedly in what follows.
To make formulae more readable, we overload notation to define
$$
    \gamma(x, \eta) := \gamma(x, \smooth{\gamma} \eta(x)) ,
$$
and similarly write $r(x, \eta)$ for $r(x, \smooth{r} \eta(x))$
and $\mu_\theta(x, \eta)$ for the expression of equation \eqref{eqn:mu_defn}.
When convenient, we may also suppress the arguments completely,
writing simply $\gamma$, $r$, and $\mu_\theta$ for these quantities.
\end{notation}

\begin{terminology*}
In our prelimiting model,
the population is represented by a point measure in which each
individual is assigned a mass $1/N$. We use the term ``population density'' for
this process, as it is supposed to measure population size relative to a 
nominal occupancy of $N$ individuals per unit area. There is no implication 
that the measure representing the population is absolutely continuous with
respect to Lebesgue measure; indeed in the prelimit it is certainly not.
\end{terminology*} 

In summary, at each time $t$, $\eta^N_t$ is purely atomic, consisting of atoms of mass $1/N$.
At instantaneous rate $\theta \gamma(x, \eta^N_t) N \eta^N_t(dx)$ an offspring of mass $1/N$ is 
produced at location $x$; it disperses to 
a location $y$ offset from $x$
by an independent Gaussian with mean $\meanq(x) / \theta$
and covariance matrix $\covq(x) / \theta$, and once there establishes instantaneously
with probability $r(y, \eta^N_t)$, or else dies. 
At instantaneous rate $\theta \mu_\theta(x, \eta^N_t) N \eta^N_t(dx)$ an individual at location 
$x$ dies.
Note that the process 
$\left(\eta^N_t\right)_{t\ge 0}$, which
records numbers and locations of adult individuals, is just a scaled spatial birth and 
death process. If, for example, we insist that $\gamma(x,m)$ is bounded, then 
existence (and in particular non-explosion) is guaranteed by comparison with
a pure birth process. We do not dwell on this, as we shall require more stringent conditions
if we are to pass to the limit as $\theta$ and $N$ tend to infinity.

It is convenient to characterise the process as a solution to a martingale problem.
We write $C_b^\infty(\IR^d)$ for the space of bounded smooth functions on $\IR^d$, and, where
convenient, we
write $\langle f, \eta \rangle = \int_{\IR^d} f(x) \eta(dx)$.
\begin{definition}[Martingale Problem Characterisation]
    \label{defn:mgale_construction}
	For each value of $N$ and $\theta$, and each purely atomic 
$\eta_0^N\in\measures$ with atoms of mass $1/N$,
$(\eta^N_t)_{t \geq 0}$ is the (scaled) empirical measure of a birth-death 
process with c\`adl\`ag paths in $\measures$ for which,
for all $f \in C^{\infty}_{b}(\IR^d)$,
writing $q_\theta(x, dy)$ for the Gaussian kernel
with mean $x + \meanq(x)/\theta$ and covariance $\covq(x) / \theta$,
\begin{equation}
    \label{eqn:eta_martingale}
\begin{aligned}
M^N_t(f)
&:=  \langle f, \eta^N_t \rangle
        -\langle f, \eta^N_0 \rangle
 \\ &\qquad {}
 -  \int_{0}^{t}\bigg\{
        \int\left( \int \theta
             \left(
                f(z)r(z, \eta^N_{s})
                - f(x)r(x, \eta^N_{s})
            \right)
        q_\theta(x,dz) \right)
\\ & \qquad \qquad \qquad \qquad {}
        \times \gamma(x, \eta^N_{s}) \eta^N_{s}(dx)
%\\ & \qquad \qquad {}
    + \int f(x) F(x, \eta^N_{s}) \eta^N_{s}(dx)
    \bigg\} ds
\end{aligned}    
\end{equation}
is a martingale (with respect to the natural filtration), 
with quadratic variation
    \begin{equation} \label{eqn:prelimit_martingale_variation}
\begin{aligned} \relax
[ M^N(f)]_t =& 
\frac{\theta}{N} \int_{0}^{t}\bigg\{
    \Big\langle \gamma(x, \eta^N_{s})
        \int f^2(z)r(z, \eta^N_{s}) q_\theta(x,dz) 
    , \eta^N_{s}(dx) \Big\rangle \\
& \qquad \qquad {}
    + \Big\langle \mu_\theta(x, \eta^N_{s}) f^2(x) 
    , \eta^N_{s}(dx)\Big\rangle
    \bigg\}ds. 
\end{aligned}    
\end{equation}
\end{definition}
The form of~(\ref{eqn:eta_martingale}) and~(\ref{eqn:prelimit_martingale_variation})
is explained in Section~\ref{sec:heuristics}.
Note that since individuals are produced at rate $N \gamma \eta$, but each
has mass $1/N$,
these factors of $N$ cancel in~\eqref{eqn:eta_martingale}. 
Under our scaling, $N$ and $\theta=\theta(N)$ will tend to infinity in such a 
way that $\alpha:=\lim_{N\to\infty}\theta(N)/N$ exists. From the 
expression~(\ref{eqn:prelimit_martingale_variation}) it is easy to guess that 
whether the limit points will be deterministic or stochastic processes 
is determined by whether $\alpha$ is zero or nonzero.
%c.f.~Section~\ref{sec:heuristics}.


%In order to specify the assumptions that we shall need on the parameters of our model
%if we are to pass to a scaling limit, 
It is convenient to record some notation for the 
generator of the diffusion 
limit of a random walk with jump distribution determined by $q_\theta(x,dy)$.

\begin{definition}[Dispersal generator]
    \label{def:dispersal_generator}
    We define the dispersal kernel,
    $q_\theta(x, dy)$,
    to be the density of a multivariate Gaussian
    with mean $\meanq(x)/\theta$ and covariance matrix $\covq(x)/\theta$
    (although sometimes we omit the dependence of $\meanq$ and $\covq$ on $x$).
    This implies that if we define, for $f : \IR^d \to \IR$,
    \begin{align}
    \DG f(x)
        =
        \sum_{ij} \covq(x)_{ij} \partial_{x_i} \partial_{x_j} f(x)
        + \sum_i \meanq(x)_i \partial_{x_i} f(x)
    \end{align}
    then
    \begin{align}
        \theta \int \left(
            f(y) - f(x)
        \right) q_\theta(x, dy)
    \to \DG f(x) 
        \qquad \text{as } \theta \to \infty .
    \end{align}
    We will also denote the adjoint of $\DG$ by
    \begin{align}
	    \nonumber
    \DG^* f(x)
        &=
        \sum_{ij} \partial_{x_i} \partial_{x_j} (\covq(x)_{ij} f(x))
        - \sum_i \partial_{x_i} (f(x) \meanq(x)_i) 
        \\
	    \label{drift in adjoint}
        &=
        \sum_{ij} \partial_{x_i} \partial_{x_j} (\covq(x)_{ij} f(x))
	    -\nabla f(x)\cdot \meanq(x)-f(x)\nabla\cdot\meanq(x).
        %\text{TODO} .
    \end{align}
\end{definition}

\begin{remark}
\label{ancestral lineages: first guess}
An equivalent way to describe the model would be to say that
when the state of the population is $\eta$,
an individual at $x$ gives birth at rate
$$
    \theta \gamma(x, \smooth{\gamma} \eta(x))
    \int r(y, \smooth{r} \eta(y)) q(x, dy) ,
$$
and that offspring disperse according to the kernel
$$
    q_\theta^\mathfrak{m}(x,\eta,  dy)
    :=
    \frac{
        r(y, \smooth{r} \eta(y)) q_\theta(x, dy)
    }{
        \int r(z, \smooth{r} \eta(z)) q_\theta(x, dz)
    } .
$$
Clearly, the random walk driven by this dispersal kernel
is biased towards regions of higher establishment probability.
For comparison with future results,
it is interesting to write down the limiting generator:
\begin{align}
	\label{weighted dispersal}
    \lim_{\theta \to \infty}
    \theta \int (f(y) - f(x)) q_\theta^\mathfrak{m}(x, \eta, dy)
    &=
    \frac{
        \DG\left[ f(\cdot) r(\cdot, \smooth{r} \eta(\cdot)) \right](x)
        - 
        f(x) \DG\left[ r(\cdot, \smooth{r} \eta(\cdot)) \right](x)
    }{
        r(x, \smooth{r} \eta^{N}(x))
    } .
\end{align}
In the simplest case of unbiased isotropic dispersal
(i.e.,~$\meanq = 0$ and $\covq = I$), $\DG = \Delta$,
and~(\ref{weighted dispersal}) is equal to
\begin{align*}
    \Delta f(x) + 2 \grad f(x) \cdot \grad \log r(\cdot, \smooth{r} \eta(\cdot))(x) .
\end{align*}
One might guess that the spatial motion described by following  
the ancestral lineage of an individual 
back through time
would be described (in the limit) by the adjoint of this generator.
However, we will see in Section~\ref{sec:ancestral lineages}
that this is not in fact the case.
\end{remark}
In order to pass to a scaling limit, we will need to impose some 
conditions on the parameters of our model.
\begin{assumptions}%[Assumptions]
\label{def:model_setup}
We shall make the following assumptions on the parameters of our model.

{\bf Dispersal generator:}
We assume that
\begin{enumerate}
    \item $\meanq(x)$, $\covq(x)$ are uniformly bounded in each component;
    \item $\meanq(x)$ and $\covq(x)$ are $\alpha$-H\"older continuous in each component, 
	    for some $\alpha \in (0,1]$;
    \item the operator $\DG$ is uniformly strictly elliptic.
\end{enumerate}

{\bf Reproduction parameters:}

We assume that 
\begin{enumerate}
\item The function $F(x,m)$ satisfies 
\begin{enumerate}
\item $F(x,m)$ is locally Lipschitz in $m$;
\item $F(x,m)$ is uniformly bounded above (but not necessarily below);
\item for each fixed $m$, $\sup_{x\in\IR^d}\sup_{k\leq m}|F(x,k)|<\infty$;
\end{enumerate}
\item The functions $r(x,m)$, $\gamma(x,m)$ have bounded first and second 
derivatives in both arguments;
\item $\gamma(x,m)$ is uniformly bounded above;
\item Either
\begin{enumerate}
\item
\label{control through r} 
$|\partial_{x_i}r(x,\smooth{r}\eta(x)|$ and 
$|\partial_{x_ix_j}r(x,\smooth{r}\eta(x)|$
are uniformly bounded for $x\in\IR^d$, $\eta\in\measures$;
\item 
\label{control through gamma}
or, $m^2\gamma(x,m)$ is uniformly bounded and there exists $C<\infty$
such that for $\theta$ sufficiently large, and all $x\in\IR^d$, $\eta\in\measures$,
\[
\theta\int\big(\smooth{r}\eta(y)-\smooth{r}\eta(x)\big)q_\theta(x,dy)\leq C\smooth{\gamma}\eta(x),
\]
and
\[\theta\int\big(\smooth{r}\eta(y)-\smooth{r}\eta(x)\big)^2q_\theta(x,dy)
\leq C(\smooth{\gamma}\eta(x))^2.
\]
\end{enumerate} 
\end{enumerate}

To keep expressions manageable, we shall also assume that 
\[
\mu_\theta(x) = r(x, \smooth{r} X(x) / N) \gamma(x, \smooth{\gamma} X(x) / N)
        - \frac{1}{\theta} F(x, \smooth{F} X(x) / N),
\]
that is, this expression is non-negative so that there is no need to take the maximum
with zero in~(\ref{eqn:mu_defn}).
\end{assumptions}

The purpose of the conditions that we have placed on the reproduction parameters is to 
ensure that the net per capita reproduction rate is order $1/\theta$. As remarked above, 
because of the non-local reproduction mechanism, it no longer suffices to assume that 
$r(x,\smooth{r}\eta(x))\gamma(x, \smooth{\gamma}\eta(x))-\mu_\theta(x)$ is of order 
$1/\theta$. The conditions that we have imposed are to ensure that the difference 
between the establishment probability of a juvenile born of a parent at $x$ and dispersed
to a position $y$, determined by $q_{\theta}(x,dy)$, and its establishment probability had
it remained at $x$ are also order $1/\theta$. 

If $r(x,m)$ is independent of $m$, then the conditions are easy to 
satisfy; they just require some regularity of $r$ as a function of $x$. 
Condition~\ref{control through r} is satisfied if 
$\|\nabla\rho_r\|\leq C\rho_r$ 
and $m\partial_mr(x,m)$, $m^2\partial_{mm}r(x,m)$ are bounded.
This will be satsified if $\rho_r$ decays 
no faster than
exponentially.
On the other hand, it might, for example, seem more natural to take $\rho_r$ to be a Gaussian 
density with parameter $\sigma_r$, say. Then, as we check in Lemma~\ref{old 6.1}, 
Condition~\ref{control through gamma} is satisfied if $\rho_\gamma$ is also
Gaussian with parameter $\sigma_\gamma$ and $\sigma_\gamma>\sigma_r$. For large enough
$\theta$, this condition guarantees that 
$\epsilon_r+1/\theta <\epsilon_\gamma$, so that the establishment probability of a juvenile
is controlled by individuals that are already `felt' by the fecundity kernel $\rho_\gamma$
at the location of their parent. 


% % % % % % % % % % % % % % %
\subsection{Scaling limits of the population process}

Our main results depend on two dichotomies:
Is the limiting process deterministic or a (generalized) superprocess?
And, are interactions pointwise in the limit or nonlocal?
Below we have results for deterministic limits with pointwise and nonlocal interactions,
and for superprocess limits with nonlocal interactions.
See Figure \ref{fig:super_vs_det_2d} for illustrative simulations.


\begin{figure}
    \begin{center}
        %\includegraphics{figures/ex1a/fkpp_123.locations}
        %\includegraphics{figures/ex1b/fkpp_123.locations}
    \end{center}
    \caption{
        Snapshots of two simulations, with small $\alpha=\theta/N$ (left) and large 
	$\alpha =\theta/N$ (right).
        Simulations are run with a Fisher-KPP-like parameterization:
        birth and establishment are constant, while death increases linearly with density,
        at slope $1/\theta$.
        Left: $\alpha=0.1$. Right: $\alpha=10$.
        Other parameters were the same:
        dispersal and interactions distance were set to 1,
        and the equilibrium density is 10 individuals per unit area.
        \label{fig:super_vs_det_2d}
    }
\end{figure}

\paragraph{Scaling limits with nonlocal interactions}

Recall that the process $(\eta_t^N)_{t\geq 0}$ takes its values in the space
${\cal D}([0,\infty),\measures)$ of c\`adl\`ag
paths on $\measures$. We endow $\measures$ with the topology of weak convergence
and 
${\cal D}([0,\infty),\measures)$ with the Skorohod topology.
A sequence of processes taking values in 
${\cal D}([0,\infty),\measures)$ is said to be tight if the corresponding
sequence of distributions
is tight, so that any infinite sequence has a weakly convergent subsequence.
Our first result establishes tightness of our rescaled population processes in 
the case in which interactions remain nonlocal under the scaling, and 
characterises limit points as solutions to a martingale problem. 


\begin{theorem} \label{thm:nonlocal_convergence}
    Let $(\eta^N_t)_{t \geq 0}$
    be as defined in Definition~\ref{defn:mgale_construction}
	and assume that as $N \to \infty$, $\theta(N) \to \infty$
	in such a way that $\theta(N)/N \to \alpha$.
    (However, the kernels $\rho_r$, $\rho_\gamma$, and $\rho_F$
    remain fixed.) 
Suppose that Assumptions~\ref{def:model_setup} hold and, 
further, that $\{\eta_0^N\}_{N\geq 1}$ is tight 
in $\measures$.
    Then the sequence of processes
    $(\eta^N_t)_{t \geq 0}$ is tight, and for any limit point
    $(\eta_t)_{t \geq 0}$, for every $f \in C^\infty_b(\IR^d)$,
    \begin{align} \label{eqn:limiting_mgale_problem}
        \begin{split}
        M_t(f)
            &:=
            \langle f(x), \eta_t(dx) \rangle
            -
            \langle f(x), \eta_0(dx) \rangle
            \\ & \qquad
            -
            \int_0^t \big\langle
                \gamma(x, \eta_s)
                \mathcal{B}\left(
                    f(\cdot) r(\cdot, \eta_s)
                \right)(x)
                %\\ &\qquad \qquad \qquad {}
                +
                f(x)
                F(x, \eta_s),
                \eta_{s}(dx)
            \big\rangle ds
        \end{split}
    \end{align}
    is a martingale,
    with quadratic variation
    \begin{align} \label{eqn:limiting_mgale_variation}
        [ M(f) ]_t
        =
        \alpha
        \int_0^t
        \big\langle
            %\left(
	    2\gamma\left( x, \eta_{s} \right)
            r\left(x, \eta_{s} \right)% + \mu\left(x, \eta_{s} \right)\right)
            f^2(x),
            \eta_{s} (dx)
        \big\rangle ds. 
    \end{align}
    Furthermore, if $\alpha = 0$ the limit is deterministic.
\end{theorem}


Recall when interpreting~\eqref{eqn:limiting_mgale_problem}
that, for instance, $r(x, \eta_s) = r(x, \smooth{r} \eta_s(x))$,
and so $\DG(fr)(x) = \DG(f(\cdot) r(\cdot, \smooth{r} \eta_s(\cdot)))(x)$.


Theorem \ref{thm:nonlocal_convergence} provides tightness of the
rescaled processes. If the limit points are unique, then this
is enough to guarantee convergence.

\begin{corollary} \label{cor:superprocess_uniqueness}
    Under the assumptions of Theorem~\ref{thm:nonlocal_convergence},
    if the martingale problem
    defined by equations \eqref{eqn:limiting_mgale_problem} and \eqref{eqn:limiting_mgale_variation}
    has a unique solution,
    then $(\eta^N_t)_{t \ge 0}$ converges weakly
to that solution
    as $N \to \infty$.
\end{corollary}
When $\alpha>0$, the limit points can be thought of as interacting superprocesses. For
example, when $r$ and $\gamma$ are constant, and 
$F(x,\eta_s)=1-\smooth{F} \eta_s(x)$,
we recover the superprocess with nonlinear death rates corresponding to logistic growth.
We are not aware of a general result to determine when we will have uniqueness of 
solutions to the martingale problem of Theorem~\ref{thm:nonlocal_convergence}
when $\alpha>0$.
However, the Dawson Girsanov transform tells us that we have uniqueness in this special 
case of the superprocess with nonlinear death rates, and 
Perkins stochastic calculus (and its adaptation to a 
lookdown setting) provides uniqueness for cases with interactions in the spatial component of
the superprocess. We refer 
to~\cite{dawson:1993, perkins:1992}, and \cite{donnelly/kurtz:1999} 
for approaches to showing that the martingale problem is well-posed in the case $\alpha>0$.

Recall the following notion of solution to a PDE.
\begin{definition}[Weak solutions]
    \label{defn:weak_solutions}
    We say that $(\eta_t)_{t \ge 0}$, with $\eta_t \in \measures$,
    is a \emph{weak solution} to the PDE
   \[ 
	\dot \varphi = r \DG^*(\gamma \varphi) + \varphi F
    \]
	(where $r$, $\gamma$ and $F$ can all be functions of $\varphi$)
    if, for all $f \in C_b^\infty(\IR^d)$,
    \[
        \frac{d}{dt} \langle f, \eta_t \rangle
        =
        \langle
	    \gamma \DG(rf) + f F,
            \eta_t
        \rangle .
    \]
\end{definition}
the notation $\varphi$ is meant to be suggestive of a density.
Because Theorem~\ref{thm:nonlocal_convergence} only tells us about 
weak convergence, in the case $\alpha=0$ we can only deduce that 
any limit point $\eta_t$ is a weak solution to the nonlocal PDE
    \begin{equation} \label{eqn:nonlocal_pde}
        \partial_t \varphi_t(x)
        =
        r\left(x, \smooth{r} \varphi_t(x) \right)
        \DG^* \left[
            \varphi_t(\cdot)
            \gamma\big( x, \smooth{\gamma} \varphi_t(\cdot) \big)
        \right](x)
        +
        \varphi_t(x)
        F\left(x, \smooth{F} \varphi_t(x) \right)
        .
    \end{equation}

\citet{KURTZ1999103} provides general conditions under which we
have existence and uniqueness of solutions to~(\ref{eqn:nonlocal_pde}) which
have an $L^2$-density with respect to Lebesgue measure. 
Recall that the Wasserstein metric, defined by 
\[\rho (\nu_1,\nu_2)=\sup\Big\{\Big|\int fd\nu_1-\int fd\nu_2\Big|:\sup_x|f(x
)|\leq 1,|f(x)-f(y)|\leq \|x-y\|\Big\},\]
determines the topology of weak convergence on $\measures$.

We write 
$\gamma(x,\eta)C(x)=J(x,\eta)J(x,\eta)^T$, and 
$\beta(x,\eta)=\gamma(x,\eta)\big(\meanq(x)+2C(x)\nabla\log r(x,\eta)\big)$.
If $J$, $\beta$, and $F$ are bounded and Lipschitz in the sense 
that
\begin{equation}
\label{lipschitz for kurtz xiong}
|J (x_1, \nu_1)-J (x_2, \nu_2)|,|\beta (x_1,\nu_1)-\beta 
	(x_2,\nu_2)|, |F(x_1,\nu_1)-F(x_2,\nu_2)|
	%,|b(x_1,\nu_1)-b(x_2,\nu_2)|
	\leq C(\|x_1-x_2\|+\rho (\nu_ 1,\nu_2))
\end{equation}
for some $C>0$,   
the methods of \cite{KURTZ1999103} show that %under the conditions
%of Theorem~\ref{thm:kurtzxiong}, 
if the initial condition $\eta_0$ for our 
population process has
an $L^2$ density, then so does $\eta_t$ for $t>0$. 
Although the necessary estimates (for which we refer to the original paper)
are highly nontrivial, 
the idea of the proof is simple. Take a solution to the
equation and use it to calculate the coefficients $r$, $\gamma$ and $F$ that depend
on local population density. Then $\eta$ solves the 
{\em linear} equation obtained by regarding those values of $r$, $\gamma$ and $F$ as given.
It remains to prove that the solution to the linear equation has a density
which is achieved by obtaining
$L^2$ bounds on its convolution with the heat semigroup at time $\delta$ and letting 
$\delta\to 0$.
We also have the following uniqueness result.
\begin{theorem}[Special case of \cite{KURTZ1999103}, Theorem~3.5]
	\label{thm:kurtzxiong}
Suppose  $J$, $\beta$, and $F$ are bounded and Lipschitz in the sense 
of~(\ref{lipschitz for kurtz xiong}).
%that
%\[|J (x_1, \nu_1)-J (x_2, \nu_2)|,|\beta (x_1,\nu_1)-\beta 
%	(x_2,\nu_2)|, |F(x_1,\nu_1)-F(x_2,\nu_2)|
	%,|b(x_1,\nu_1)-b(x_2,\nu_2)|
%	\leq C(\|x_1-x_2\|+\rho (\nu_ 1,\nu_2))\]
%for some $C>0$.  
%
If $\eta_0$ has an $L^2({\mathbb R}^d)$-density, then there 
exists a unique $L^2({\mathbb R}^d)$-valued  
	solution of (\ref{eqn:nonlocal_pde}) in the sense of 
	Definition~\ref{defn:weak_solutions}.
\end{theorem}
\begin{remark}
\cite{KURTZ1999103} considers an infinite system of stochastic differential equations
for the locations and weights of a collection of particles that interact through their 
weighted empirical measure, which is shown to be the unique solution to a stochastic
	PDE. As we shall see through our lookdown representation in 
Section~\ref{sec:lookdown}, the
	solution to our
	deterministic equation can be seen as the empirical measure of a 
	countable number of particles which, in the notation above, evolve according
	to 
	\[X(t)=X(0)+\int_0^t\beta\big(X(s), \eta_s\big)ds
+\int_0^tJ\big(X(s), \eta_s\big)dW(s)\]
	(with an independent Brownian motion $W$ for each particle).
\end{remark}

To understand the form of equation~(\ref{eqn:nonlocal_pde}) a little better, 
and, in particular, why we see $\DG^*$ rather than $\DG$ in the equation for the 
limiting population density, we consider the two terms that contribute to the drift in our 
expression~(\ref{drift in adjoint})
for $\DG^*$. First consider a special case in which dispersal has a mean drift 
to the right, so that the drift term in $\DG$ is $d/dx$, and the effect of the drift is 
just to shift the population profile to the right. If we are at a point where the 
slope of the population density is positive, then the shift will {\em reduce} the
population density at the point; the change in population density is proportional
to $-d/dx$. Similarly, if dispersal tends to move offspring away from a region, that
is if $\grad\cdot\meanq >0$, 
then it will push down the population density, hence the term $-\grad\cdot\meanq$ in
equation~(\ref{eqn:nonlocal_pde}).

\paragraph{Two-step convergence to PDE}

Although the coefficients at $x$ in \eqref{eqn:nonlocal_pde} are nonlocal,
we can choose our kernels $\rho_\gamma$, $\rho_r$, and $\rho_F$
in such a way that they depend only on the population in a region close to $x$,
and so we expect that under rather general conditions
solutions of the nonlocal PDE will be close to the corresponding 
classical PDE.
The following propositions provide two concrete situations in which this is true.
In the first, the PDE is a reaction diffusion equation, and we borrow
an idea from~\cite{penington:2017} to express the solutions to both the nonlocal
equation and the classical PDE through a Feynman-Kac formula. 
\begin{proposition}
    \label{prop:nonlocal_to_local}
Recall the notation $\rho^\epsilon_F(x)=\rho_F\big(x/\epsilon)/\epsilon^d$.
Assume $\varphi_0\in L^2(\IR)$ is a positive, uniformly Lipschitz, and uniformly bounded function. 
Suppose that $\varphi^\epsilon\in L^2(\IR^d)$ is a weak solution to the equation
\begin{equation}
\label{nonlocalPDEv1} 
%\begin{cases}
\partial_t \varphi^\epsilon = \DG^* \varphi^\epsilon + 
\varphi^\epsilon F(\rho_F^\epsilon*\varphi^\epsilon),  
	%& 
\qquad x \in \mathbb{R}^d,\, t >0, %\\ 
%	\varphi^\epsilon(0,x) = \varphi_0(x) & x \in \mathbb{R}^d;
%\end{cases}
\end{equation}
with initial condition $\varphi_0(\cdot)$,
and that $\varphi$ is a weak solution to the equation
 \begin{equation}
\label{localPDE} 
%\begin{cases}
\partial_t \varphi = \DG^* \varphi + \varphi F(\varphi),  
%	& 
\qquad x \in \mathbb{R}^d,\, t >0, %\\ 
%	\varphi(0,x) = \varphi_0(x) & x \in \mathbb{R}^d.
%\end{cases}
\end{equation}
also with initial condition $\varphi_0(\cdot)$.
%Assume $\varphi_0\in L^2(\IR)$ is a positive, Lipschitz, and uniformly bounded function; 
Suppose further that $F$ is a Lipschitz function which is bounded above, and that
$b(x)$ and $C(x)$, the drift and 
covariance matrix of $\DG$, satisfy the conditions of 
Assumptions~\ref{def:model_setup}.
Then, for all $T>0$ there exists a constant $K=K(T, \Vert \varphi_0 \Vert_\infty)>0$,
and a function $\delta(\epsilon)$ (dependent on $\rho_F$) with $\delta(\epsilon)\to 0$ 
as $\epsilon\to 0$,
such that, for all $0 \leq t \leq T$, and $\epsilon$ small enough,
\[ 
\Vert \varphi_t(\cdot) - \varphi_t^\epsilon(\cdot) \Vert_\infty\leq K\delta(\epsilon). 
\]
In particular, as $\epsilon\to 0$, we have that $\varphi^\epsilon$ converges 
uniformly in compact intervals of time to $\varphi$.
\end{proposition}
\begin{remark}
Note that Theorem~\ref{thm:kurtzxiong} guarantees uniqueness of solutions
to equation~(\ref{nonlocalPDEv1}). 
\end{remark}
Our second example is a nonlocal version of a porous medium equation with
logistic growth. That is, 
we consider non-negative solutions to the equation
\begin{equation}
\label{mollified equation}
\partial_t \psi^\epsilon=
\Delta\left(\psi^\epsilon \cdot \rho^{\epsilon}_\gamma*\psi^\epsilon\right)
+\psi^\epsilon\left(1-\rho^{\epsilon}_\gamma*\psi^\epsilon\right).
\end{equation}
The case without the reaction term (and with $\IR^d$ replaced by a torus) is considered 
by~\cite{lions/mas-gallic:2001} who use
it as a basis for a particle method for numerical solution of the porous medium equation.
Of course this does not quite fit into our framework, since in the notation of our 
population models this would necessitate $\gamma(x,m)=\smooth{\epsilon}m$ which is
not bounded. However, this can be overcome by an additional layer of approximation
(c.f.~our numerical experiments of Section~\ref{beyond linear diffusion})
and we do not allow this to detain us here. Existence and uniqueness of solutions 
to~(\ref{mollified equation}) can be obtained using the approach
of~\cite{lions/mas-gallic:2001}.

We should like to prove that as $\epsilon\to 0$ we have
convergence to the solution to the porous medium equation with
logistic growth:
\begin{equation}
\label{PME}
\partial_t\psi=
\Delta\left(\psi^2\right)
+\psi\left(1-\psi\right).
\end{equation}

\begin{notation}
We use $\rightharpoonup$ to denote weak convergence in the sense of analysts;
that is, $\psi^\epsilon\rightharpoonup \psi$ in $L^1$ means 
$\int \psi^\epsilon v d x
\rightarrow\int \psi v d x$ for all $v\in L^\infty$.

We write $L_t^2(H^1)$ for functions for which the $H^1$ norm in
space is in $L^2$ with respect to time, i.e.
$$\int_0^T\int \left\{\psi_t(x)^2+\left\| \nabla \psi_t(x)\right\|^2\right\}
dx dt <\infty,$$
and $C_t(L^1)$
will denote functions for which the $L^1$ norm in space is continuous in time.
\end{notation}

\begin{proposition}
	\label{nonlocalPME to PME}
Suppose that
we can write $\rho_\gamma=\zeta*\check{\zeta}$, where $\check{\zeta}(x)=\zeta(-x)$ and
$\zeta\in\mathcal{S}(\IR^d)$ (the Schwartz space of rapidly decreasing functions).
Furthermore, suppose that %$\psi_0^\epsilon\in L^1(\IR^d)$, 
$\psi_0^\epsilon\geq 0$ is such that
there exist $\lambda\in (0,1)$ and $C\in (0,\infty)$ (independent
of $\epsilon$) such that 
\begin{equation*}
\int\exp(\lambda \|x\|)\psi_0^\epsilon(x)dx<C,\quad\mbox{ and }
\sup_\epsilon\int \psi_0^\epsilon|\log \psi_0^\epsilon|dx<\infty,
\end{equation*}
with
$\psi_0^\epsilon\rightharpoonup \psi_0$ as $\epsilon\to 0$. Then
writing $\psi^\epsilon$ for the solution to~(\ref{mollified equation})
on $[0,T]\times \IR^d$ with
initial condition $\psi_0^\epsilon$,
$\psi^\epsilon\rightharpoonup \psi$ as $\epsilon\to 0$ where
$\psi\in L_t^2(H^1)\cap C_t(L^1)$, $\int \psi|\log \psi| dx<\infty$, and
$\psi$ solves~(\ref{PME}) on $[0,T]\times \IR^d$. 
%Moreover, strong convergence
%of the initial conditions implies strong convergence of the solutions.
\end{proposition}

\begin{remark}
Although it seems hard to formulate an all-encompassing result,
Propositions~\ref{prop:nonlocal_to_local} and~\ref{nonlocalPME to PME} are by no
means exhaustive. When our scaling limit is deterministic, one can 
expect analogous results under rather general conditions. However, when the limit
points are stochastic, they resemble `nonlinear superprocesses' and so one cannot
expect a density with respect to Lebesgue measure in $d\geq 2$. It is then not
reasonable to 
expect to be able to make sense of the limit if we scale the kernels in this way.
Moreover, in one dimension, where the classical superprocess does have a density with 
respect to Lebesgue measure, the form of~(\ref{eqn:limiting_mgale_problem})
suggests that even if one can remove the local averaging from $\gamma$, we
would need to retain averaging of $r$ in order to obtain a well-defined limit. 
\end{remark}

\paragraph{One-step convergence to PDE}

Theorem \ref{thm:nonlocal_convergence} combined with Proposition \ref{prop:nonlocal_to_local}
implies that we can take the limit $N \to \infty$
followed by the limit $\epsilon \to 0$
to obtain solutions to the PDE \eqref{localPDE}.
However, it is of substantial interest to know whether
we can take those two limits simultaneously.
The general case seems difficult,
but we prove such ``diagonal'' convergence in the following situation.

\begin{theorem}[Convergence to a PDE]
    \label{thm:local_convergence}
\comment{Need to add conditions on initial condition}

\comment{Need to introduce notion of $C$-tight and say that's what we prove.}

    Let $(\eta^N_t)_{t \geq 0}$
    be as defined in Definition~\ref{defn:mgale_construction} with
$r(x,m)\equiv 1\equiv \gamma(r,m)$, $F(x,m)\equiv F(m)$, 
$\rho^\epsilon_F$ a Gaussian density with parameter $\epsilon^2$,
and $\DG=\Delta$. 
Further suppose that $F(m)$ is a polynomial with $F(m)\ind_{m\geq 0}$ bounded above.
%there exists $k\in\IN$ such that $F$ is $k$-times differentiable
%with 
%$\|F^{(k)}(m)\|_{\infty}<\infty$.
Finally assume that $N \to \infty$, $\theta \to \infty$
and $\epsilon \to 0$ in such a way that
\begin{equation}
\frac{1}{\theta\epsilon^2}+\frac{\theta}{N\epsilon^d}\to 0.
\end{equation}
Then the sequence of stochastic processes $\big(\rho_F^\epsilon*\eta^N_t\big)_{t \ge 0}$
converges weakly
to a measure-valued process with a density $\varphi(t, x)$
that solves
\begin{align}
        \partial_t \varphi(t, x)=\Delta \varphi (t,x) +\varphi(t,x) F(\varphi(t,x)).
\end{align}
\end{theorem}
\begin{remark}
In fact,
our proof goes through without significant change under that conditions that
$F(m)\ind_{m\geq 0}$ is bounded above (but not necessarily below), and that
for all $m,n\in[0,\infty)$
\[
|F(m)|\leq\sum_{j=1}^ka_jm^j, \quad\mbox{ and }
|F(n)-F(m)|
\leq|n-m|\sum_{j=1}^{k'}b_j\Big(n^j+m^j),
\]
for some non-negative constants $\{a_j\}_{j=0}^k$, $\{b_j\}_{j=0}^{k'}$.
We take $F$ to be polynomial to somewhat simplify notation in the proof.
\end{remark}

% % % % % % % % % % % %
\subsection{Ancestral lineages in the scaling limit}
\label{sec:ancestral lineages}

Now that we have established what we can say about how 
population density changes with time,
we turn to results on ancestral lineages,
i.e., how genealogical ancestry can be traced back across the landscape.
Informally,
a \emph{lineage} $(L_t^N)_{t \ge 0}$,
begun at spatial location $L_0 = x$,
can be obtained by picking a focal individual uniformly at 
random from the population at $x$,
and then, for each time $t$,
setting $L_t^N$ to be the spatial location of the individual alive at time $t$
before the present from
whom the focal individual is descended.
Since in our model individuals have only one parent, this is unambiguous.
Although we did not explicitly retain such information,
it is clear that for finite $N$, since individuals are born one at a time, 
one could construct the distribution of a lineage $(L_t^N)_{t=0}^T$
given the history of the population $(\eta^N_t)_{t = 0}^T$,
for each starting location to which $\eta^N_T$ assigns positive mass.
It is less clear, however, how to formally retain such information when we 
pass to the scaling limit.

The \emph{lookdown construction} in Section~\ref{sec:lookdown}
will enable us to recover information about ancestry in the infinite
population limit.
Roughly speaking,
each particle is assigned a unique ``level'' from $[0,\infty)$
that functions as a label and thus allows reconstruction of lineages.
The key to the approach is that levels are assigned in such a way as to be exchangeable, 
so that sampling a finite number, $k$ say, of
individuals from a given 
region is equivalent to looking at the individuals in that region with the $k$ lowest
levels. Moreover, as we pass to 
the infinite population (and therefore infinite level) limit,
the collection of (individual, level) pairs 
for which the levels are below 
any fixed value converge, Theorem~\ref{}. 
See~\citet{etheridge/kurtz:2018} for an introduction to these ideas.
In particular, even in the infinite
population limit, we can sample an individual
from a region (it will be the individual in that region with the lowest level) 
and trace its line of descent. 
This will, in principle, allow us to calculate, for each $y\in\IR^d$, the proportion of
the population at location $x$ in the present day population that are descended from 
a parent who was at location $y$ at time $t$ in the past. To make
sense of this in our framework, in Section~\ref{ancestral lineages}, 
we justify a weak reformulation of this idea.

We are interested in two questions.
When is the motion of an ancestral 
lineage, given complete knowledge of the population process,
 a well-defined Markov process?
In other words, is knowledge of the process $(\eta_t)_{t=0}^T$ that records 
numbers of individuals
but not their ancestry sufficient to define the distribution of $(L_t)_{t=0}^T$?
Second, does the process have a tractable description?

We focus on the simplest situation in which the population process is deterministic.
However, the results here apply when the population process solves 
either a nonlocal or a classical PDE.
There will be no 
coalescence of ancestral lineages in the deterministic limit, but our results can
be seen as a first step towards understanding genealogies for very high population 
densities. 
\begin{definition}[Ancestral lineage] 
\label{def:lineage_generator}
    Let $(\varphi_t(x))_{0 \le t \le T}$
    denote the density of the scaling limit of our population model, 
solving \eqref{eqn:nonlocal_pde},
    and let $y$ be a point with $\varphi_T(y) > 0$.
    We define $(L_s)_{s=0}^T$,
    the ancestral lineage of an individual sampled from the
population at $y$ at time $T$, by setting $L_s$
    to be the position of the unique ancestor of that individual at time $T - s$.
    We define
    $(Q_s)_{s \geq 0}$
    to be the time inhomogeneous semigroup satisfying
    \begin{align*}
        Q_s f(y) := \IE_y[ f(L_s) ] .
    \end{align*}
\end{definition}

Our next result identifies the ancestral lineage as a diffusion
by characterizing its generator.

\begin{theorem} \label{thm:lineages}
    For $\varphi: \IR^d \to \IR$, define
    \begin{align}
        \label{eqn:lineage_generator}
        \Lgen_\varphi f
        &=
        \frac{r}{\varphi}
        \left[
            \DG^*(\gamma \varphi f) 
            - f \DG^* (\gamma \varphi)
        \right] \\
        &= \label{eqn:lineage_generator2}
        r\gamma
        \left[
            \sum_{ij} \covq_{ij} \partial_{x_ix_j} f
            + \sum_j \vec{m}_j \partial_{x_j} f
        \right] ,
    \end{align}
    where $\vec{m}$ is the vector
    $$
    \vec{m}_j
    =
    2 \sum_i C_{ij} \partial_{x_i} \log(\gamma \varphi)
    + 2 \sum_i \partial_{x_i} C_{ij}
    - \meanq_j .
    $$
    Then the generator of the semigroup $Q_s$
    of Definition~\ref{def:lineage_generator} is given by
	$\partial_sQ_sf(y)= \Lgen_{\varphi_{T-s}}Q_sf(y).$ 
\end{theorem}
\begin{remark}
As usual,
to make the generator readable, we've written it in concise notation,
omitting the dependencies on location and population density,
which itself changes with time.
When interpreting this,
remember that everything depends on location and density at that location and time --
for instance, ``$r$'' is actually $r(x, \varphi(x))$ (in the classical case),
or $r(x, \smooth{r} \eta(x))$ (in the nonlocal case).

Moreover, we haven't proved any regularity of the population density process $\varphi$,
so, as written, the generator~(\ref{eqn:lineage_generator})
may not make sense. Instead, it should be interpreted in a weak sense which is made precise
in Section~\ref{ancestral lineages}.
\end{remark}

\begin{corollary} \label{cor:lineages_simple}
    In addition to the assumptions of Theorem~\ref{thm:lineages},
    if the dispersal process is isotropic
    (i.e., $\covq = \sigma^2 I$ and $\meanq$ is constant),
    then
    \begin{equation}
        \Lgen_\varphi f
        =
        r \gamma
        \left(
            \sigma^2 \Delta f
            +
            \left(
                2 \sigma^2 \grad \log(\gamma \varphi)
                - \meanq
            \right)
            \cdot \grad f
        \right) .
    \end{equation}
\end{corollary}

In other words, 
the lineage behaves as a diffusion driven by Brownian motion 
run at speed $\sigma^2$ multiplied by the local per-capita production of 
mature offspring 
($r \gamma$)
in a potential equal to the local output of juveniles tilted by migration bias
($\varphi_s \gamma \exp(-\meanq \cdot x / 2 \sigma^2)$). In particular,
lineages are drawn to regions of high fecundity (production of juveniles), but their
speed is determined by the rate of production of mature offspring.
This can be compared to Remark~\ref{ancestral lineages: first guess}.

\begin{corollary} \label{cor:wavefront}
    In addition to the assumptions of Corollary~\ref{cor:lineages_simple},
    suppose that the population process is described by a travelling wave with 
velocity $\wavespeed$,
    i.e., the population has density
    $\varphi(t, x) = w(x - t \wavespeed)$
    where $w$ solves
    \begin{align*}
        r \DG* (\gamma w) + w F + \wavespeed \cdot \grad w = 
r\sigma^2\Delta (\gamma w) -\meanq \cdot\grad (\gamma w) +\wavespeed \cdot \grad w =0 .
    \end{align*}
    Then the semigroup $Q_s$
    of the motion of a linages in this population 
    is time-homogeneous with generator
    \begin{align}
\label{Lgen}
        \Lgen f
        &=
        \sigma^2 r \gamma
        \left(
            \Delta f
            +
            2 \grad \log (\gamma w)
            \cdot \grad f
        \right)
        + (\wavespeed - r\gamma \meanq) \cdot \grad f .
    \end{align}
\end{corollary}


%%%%%%%%%%%%%%%%%%%%%%
\section{Examples and applications}
\label{sec:applications}

We now turn to some examples and applications. %Next,
%we cover some illustrative examples
%and interesting applications.

% % % % % % % % % % % %
\subsection{Beyond linear diffusion}
\label{beyond linear diffusion}

Equation~(\ref{eqn:nonlocal_pde}) is a nonlocal version of a reaction diffusion equation with nonlinear
diffusion; in other words the diffusivity of the population depends on the population
density. Passing to the classical limit, we recover equations
like~(\ref{PME}).
Such equations are widely used in a number of contexts in biology in which
it is clear that motility within a population varies with population density.
For example, density dependent dispersal is a common feature in spatial
models in ecology, eukaryotic cell biology, and avascular tumour growth;
see~\cite{sherratt:2010} and references therein for further discussion. 
In particular, it has been suggested as a model for
the expansion of a certain type of bacteria % of the type {\em Paenbacillus dendritiformis}
on a thin layer of agar in a petri dish 
\citep{cohen/golding/kozlovsky/benjacob/ron:1999}. 
We shall pay particular attention to the case in which the equation can be 
thought of as modelling the density of an expanding population. 
\cite{li/buenzli/simpson:2022} write down an interacting particle
system on a hexagonal
lattice (with at most one individual per site) that incorporates an effect
of local crowding on individual motion. Their equation
corresponds to a reaction-diffusion equation with nonlinear diffusion and a 
bistable reaction term of the form~(\ref{Birzu nonlinearity}) and they
investigate the interplay between nonlinear diffusion and long-term 
survival or extinction of the population. We focus instead on 
the monostable reaction of~(\ref{PME}).

Comparing %\eqref{PME} 
with \eqref{eqn:nonlocal_pde},
we see that to set up a limit in which the population density $\varphi$ follows the 
porous medium equation with logistic growth of~(\ref{PME}),
we need $r=1$,
$\gamma = \varphi$, and $F = 1 - \varphi$.
Consulting equation~\eqref{eqn:mu_defn},
this implies that $\mu_\theta = \max\big(0, (1 + 1/\theta) \varphi - 1/\theta\big)$.
In other words,
establishment is certain
and birth rates increase linearly with population density,
but to compensate, death rates increase slightly faster (also linearly),
as shown in Figure~\ref{fig:pme_waves}.
Alert readers will notice that
the assumption from Assumptions~\ref{def:model_setup} 
that $\gamma (x,m)$ be uniformly bounded 
is violated.
This can be corrected by use of a cut-off, and in fact the downwards drift
provided by the logistic control of the population size prevents $m$ from 
getting too big. 
In practice in our simulations we choose a suitably small value for $\beta$, define
\begin{align*}
    \gamma(x, m) = (1 - \exp(- \beta m)) / \beta, \qquad
    F(x, m) = \exp(- \beta m) / \beta ,
\end{align*}
and set $\mu_\theta(x, m) = \max(0, \gamma - F / \theta)$.
Birth and death rates are equal at density $m = 1$,
corresponding to an unscaled density of $N$ individuals per unit area.

\begin{figure}
    \begin{center}
        %\includegraphics[width=0.45\textwidth]{figures/ex2a/pme_123.steps}
        %\includegraphics[width=0.45\textwidth]{figures/ex2b/pme_123.steps}
    \end{center}
    \caption{
        Two panels, showing simulated 1D populations under a
porous medium equation with logistic growth~(\ref{PME}), comparing the simulated
        profile to the analytical solution;
        $\theta/N$ not small on left; small on right.
        (Note: noisier wave should move slower, we'll see this.)
        \label{fig:pme_waves}
    }
\end{figure}

In one dimension, equation~(\ref{PME}) has an explicit travelling wave solution
\begin{align} \label{eqn:pme_wave}
    w^P(t, x)
    :=
    \left( 1 - e^{ \frac{1}{2} (x - x_0 - t) } \right)_+ .
\end{align}
Notice that the wave profile has a sharp boundary at $x = x_0 + t$.
There are also travelling wave solutions with $c>1$ \citep{gilding/kersner:2005},
which lack this property.
However, for initial conditions that decay sufficiently rapidly at infinity,
such as one might use in modelling a population invading new territory,
the solution converges to \eqref{eqn:pme_wave} \citep{kamin/rosenau:2004}.
In Figure~\ref{fig:pme_waves} we show simulations of the individual based model
described above. They
display travelling wave solutions
similar to numerical solutions of~(\ref{PME}),
with increasingly good agreement for larger $\theta$ and $N$.
As expected, noise slows down the wave.
\comment{Not sure that noise will slow things drastically because the 
biggest production of
mass happens behind the wavefront.}


% % % % % % % % % % % %
\subsection{Ancestry in different types of travelling waves}
\label{ancestral lineages for travelling waves}

\begin{figure}
    \begin{center}
        %\includegraphics{figures/ex3_fkpp/fkpp_123.lineages}
        %\includegraphics{figures/ex3_fkpp/fkpp_123.lineagehist}
        %\includegraphics{figures/ex3_pme/pme_123.lineages}
        %\includegraphics{figures/ex3_pme/pme_123.lineagehist}
        %\includegraphics{figures/ex3_allen-cahn/allen-cahn_123.lineages}
        %\includegraphics{figures/ex3_allen-cahn/allen-cahn_123.lineagehist}
    \end{center}
    \caption{
        Six panels: one pair for FKPP, one for the porous medium 
equation with logistic growth~(\ref{PME}), and one for 
the Allen-Cahn equation~(\ref{eqn:allen_cahn}).
        Each pair shows (top) lineage traced back in wavefront,
        and (bottom) stationary distribution of lineage location,
        compared to analytical solution for the porous medium case.
        \label{fig:pme_vs_fkpp}
    }
\end{figure}

Although it remains challenging to establish the distribution of genealogical
trees relating individuals sampled from our population model, as described in
the introduction, we can gain some insight by investigating the motion of a
single ancestral lineage. Here we do that in the context of a one-dimensional
population expanding into new territory as a travelling wave. 
We focus on three cases in which we have 
explicit information about the shape of the travelling wave profile:
the Fisher-KPP equation,
a special case of the Allen-Cahn equation with a bistable nonlinearity, and
equation~(\ref{PME}).
We work here in one dimension,
and take $\sigma^2 = 1$ and $\meanq = 0$.

%%%%%%%%%%

%%%%%%%%%%
\paragraph{Fisher-KPP equation:}
Consider the classical Fisher-KPP equation,
\begin{align} \label{eqn:fkpp}
    \partial_t\varphi = \partial_{xx}\varphi + \varphi (1-\varphi) .
\end{align}
Even though we do not have an explicit formula for the wave shape in this case,
our methods provide information about ancestral lineages.
The equation has non-negative travelling wave solutions of 
speed $c$ for all $c \geq 2$, 
but started from any compact perturbation of a Heaviside function, the 
solution will converge to the profile $w^F$ with the minimal wavespeed, $c=2$
\citep{kolmogorov/petrovsky/piscounov:1937,bramson:1983}.
No matter what initial condition,
for any $t>0$ the support of the 
solution will be the whole real line. 
In this case, we must have $r = \gamma = 1$,
and $F(x, m) = 1 - m$ so $\mu_\theta(x, m) = 1 + (m-1)/\theta$.
This implies that
the generator of the motion of an ancestral lineage is
\begin{equation} \label{eqn:fkpp_generator}
    \Lgen_1  f
    =
    \partial_{xx}f + 2 \frac{\partial_xw^F}{w^F} \partial_x f + 2 \partial_xf .
\end{equation}
Near the tip of the wave (for $x$ large), $w^F(x) \sim e^{-x}$,
so \eqref{eqn:fkpp_generator} implies that
the motion of a lineage is close to unbiased Brownian motion.
On the other hand, in the ``bulk'', %for $x < 0$ 
a lineage behaves approximately as
Brownian motion with drift at rate two to the right.
This implies that
ancestral lineages are pushed into the tip of the wave,
and there is no stationary distribution,
so that long-term dynamics of genetic inheritance
depend on the part of the wave not well-approximated by a smooth profile,
in agreement with the results described in Section~\ref{introduction}.


%%%%%%%%%%
\paragraph{Allen-Cahn equation:}
Now take the following case of the Allen-Cahn equation:
\begin{align} \label{eqn:allen_cahn}
    \partial_t\varphi = \partial_{xx}\varphi + \varphi(1-\varphi)(2\varphi-1+s),
\end{align}
for a given $s \in (0,2)$.
Once again we have taken $r=\gamma=1$, but now the reaction term is bistable.
This equation can be used to model the motion of so-called 
hybrid zones in population genetics; see, for example,
\cite{barton:1979, gooding:2018, etheridge/gooding/letter:2022}.
This equation has an explicit travelling wave solution with speed $s$
and shape
\[ w^A(x) = (1+e^{x})^{-1}. \]
That is $\phi_t(x)=w^A(x-st)$ solves~(\ref{eqn:allen_cahn}).
Substituting $w^A$ in place of $w^F$ in~(\ref{eqn:fkpp_generator}),
we find that the generator of an ancestral lineage is now, 
\begin{align*}
\Lgen_2 f
    &=
    \partial_{xx}f
    + 
    2 \frac{\partial_xw^A}{w^A} \partial_xf
    +
    s \partial_x f\\
    \qquad &=
    \partial_{xx}f
    -
    2 \frac{e^x}{1+e^x} \partial_xf 
    + 
    s \partial_xf,
\end{align*}
so lineages in the tip are pushed leftwards into the bulk of the 
wave at a rate $-2e^x/(1+e^{x})$.
The strength of this drift decreases exponentially as $x\to -\infty$.
The density of the speed measure for this diffusion is
$$
    m(x) \propto e^{sx}(1+e^x)^{-2},
$$
which is integrable, and so determines the unique stationary distribution.
Thus the position of the ancestral lineage will converge to a stationary 
distribution which is maximised away from the extreme tip of the wave.
This is consistent with~\cite{etheridge/penington:2022}, who consider an analogous
stochastic population model, although the stronger result there (that the genealogy
of a sample from behind the wavefront is approximately a Kingman coalecsent) requires
the stronger condition $s<1$.

% % % % % % % % % % % % % % % %

\paragraph{Porous Medium equation with logistic growth:}
Finally, consider equation~(\ref{PME}). Setting $x_0=0$ (for definiteness) and substituting
the form of $w^P$ from equation \eqref{eqn:pme_wave}
into Corollary~\ref{cor:wavefront},
with $c=1$,
$\gamma(x, m) = m$,
$r(x,w) = 1$,
and $F(x, m) = (1 - m)$,
the generator of the diffusion governing the position of the ancestral lineage 
is, for $x < 0$,
%with $f_x$ denoting the spatial derivative of $f$,
\begin{align*}
    \Lgen_3 f
    &=
        w^P
        \left(
        \partial_{xx} f
         +
         2 \frac{\partial_x((w^P)^2)}{(w^P)^2} \partial_xf
        \right)
        + \partial_xf \\
    &=
        \left(1 - e^{\frac{1}{2} x} \right)
        \partial_{xx}f
        -
        2 e^{\frac{1}{2} x} \partial_xf
        +
        \partial_xf .
\end{align*}
The speed measure corresponding to this diffusion has density
%again for $\xi < 0$,
\begin{align*}
    m(\xi)
    &\propto
        \frac{ 1 }{ 2 (1 - e^{\xi/2}) }
        \exp\left(
            \int_\eta^\xi \left\{
                1 - \frac{e^{x/2}}{1 - e^{x/2}}
            \right\} dx
        \right) \\
    &\propto
        e^\xi\left(1-e^{\xi/2}\right),
        \quad
        \text{for } \xi < 0 ,
\end{align*}
which is integrable and so when suitably normalised gives the unique stationary distribution.
Notice that even though we have the same reaction term as in the Fisher-KPP equation, with this 
form of nonlinear diffusion, at stationarity
the lineage will typically be significantly behind the front. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Clumping from nonlocal interactions}

\comment{
    Description of the process;
    discussion of when it happens;
    (TODO: how's it affected by $\theta$?)
}

\begin{figure}
    \begin{center}
        FIGURE
    \end{center}
    \caption{
        Two panels: left is 2D picture of clumped population;
        right is a bumpy expanding wavefront.
        \label{fig:clumping}
    }
\end{figure}


% % % % % % % % % % % % % % % %
\subsection{Lineage motion is not uniquely determined by population density}

It is natural for applications to wonder about identifiability:
when can the observed quantities like population density
or certain summaries of lineage movement uniquely determine
the underlying demographic parameters?
Consider a deterministic,
continuous population generated by parameters $\gamma$, $\mu_\theta$, and $r$
with $\meanq = 0$ and $\covq = I$.
Suppose it has a stationary profile $w(x)$, that must satisfy
$$
   r \Delta(\gamma w) + (r \gamma - \mu_\theta) w = 0 .
$$
It is easy to see that $w$ does not uniquely specify $\gamma$, $\mu_\theta$, and $r$:
let $\lambda(x)$ be a smooth, nonnegative function on $\IR^d$,
and let $r'(x, m) = \lambda(x) r(x, m)$ and $\mu_\theta'(x, m) = \lambda(x) \mu_\theta(x, m)$
(and, let $\gamma' = \gamma$).
Then the population with parameters $\gamma'$, $\mu_\theta'$, and $r'$
has the same stationary profile(s) as the original population.

Can these two situations be distinguished from summaries of lineage movement?
The first has lineage generator
\[
    f \mapsto \Lgen f = r \gamma \left( \Delta f + 2 \grad \log(\gamma w) \cdot \grad f \right),
\]
while the second has lineage generator $f \mapsto \lambda(x) \Lgen f(x)$.
In other words,
although the stationary profile of the population is unchanged when we scale
local establishment and death by $\lambda$,
the motion of lineages is sped up locally by $\lambda$.
This corresponds to making areas with $\lambda > 1$ more ``sink-like'' 
and $\lambda < 1$ more ``source-like'':
if $\lambda(x) > 1$, then at $x$ both the death rate and probability of 
establishment of new individuals are higher.
As a result, lineages in the second model spend more time in areas with $\lambda < 1$,
i.e., those areas have higher reproductive value, %long-term fitness
something that is, in principle, discernible from genetic data.


%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Heuristics}
    \label{sec:heuristics}

In this section we perform some preliminary calculations and use them 
to provide heuristic arguments for our main results,
to build intuition before the formal proofs.

% % % % % % % % % % % % % % % % % %
\subsection{The population density}
    \label{sec:population_heuristics}

{\em We reiterate that in our prelimiting model,
the population is represented by a point measure in which each
individual is assigned a mass $1/N$. We use the term ``population density'' for
this process, as it is supposed to measure population size relative to a 
nominal occupancy of $N$ individuals per unit area, but it is not
absolutely continuous with respect to Lebesgue measure.}

We write $\Pgen^N$
for the generator of the scaled population process $\eta^N$
of Definition~\ref{defn:mgale_construction}
acting on test functions of the form $G( \langle f, \eta \rangle )$,
where $f \geq 0$ is smooth and bounded on $\IR^d$ and 
$G \in C^\infty ([0,\infty))$.
Recall that $\theta =\theta(N) \to \infty$ as $N\to\infty$ in such 
a way that $\theta(N)/N\to\alpha$.
%,
%although we don't remind the reader with extra superscripts.

A Taylor expansion allows us to write
\begin{multline}
	\label{generator prelimit}
    \Pgen^N
    G(\langle f,\eta \rangle)
    =
    G'(\langle f, \eta \rangle)
    \lim_{\delta t\downarrow 0} \frac{1}{\delta t}
    \IE\left[
        \left. \langle f, \eta_{\delta t} \rangle
        -
        \langle f, \eta \rangle
        \right| \eta_0=\eta
    \right]
    \\
    \qquad {}
    + \frac{1}{2}
        G''(\langle f,\eta\rangle)
    \lim_{\delta t\downarrow 0}\frac{1}{\delta t}
    \IE\left[
        \left.\big(\langle f,\eta_{\delta t}\rangle
        -
        \langle f, \eta\rangle\big)^2 \right|\eta_0=\eta
    \right]
    +
    \epsilon_N(f, G, \eta),
\end{multline}
where the terms that make up 
$\epsilon_N(f, G, \eta)$
will be negligible in our scaling limit. 

% % % % % % % 
\subsubsection*{Mean measure}

Recall that in our parameterization only death rates $\mu_\theta$
and the dispersal kernel $q_\theta$ depend on $\theta$.
For a suitable test function $f$, we find
\begin{equation} \label{mean measure}
    \begin{split}
    \Pgen^N \langle f, \eta \rangle
    &=
    \lim_{\delta t\downarrow 0} \frac{1}{\delta t}
    \IE\left[ \left.
        \langle f, \eta_{\delta t} \rangle
        -
        \langle f, \eta\rangle
        \right| \eta_0 = \eta
    \right]
    \\
    &=
    \theta \int
        \int f(z) r(z,\eta) q_\theta(x,dz)
    \gamma(x, \eta) \eta(dx)
    -
    \theta \int f(x)\mu_\theta(x, \eta)
    \eta(dx).
    \end{split}
\end{equation}
The first term is the increment in $\langle f,\eta\rangle$
resulting from a birth event (recalling that
we don't kill the parent) integrated against the rate of such events,
and the second reflects death events.
In both terms,
the rate of events has a factor of $N$ (because events happen at a rate 
proportional to the number of individuals,
whereas $\eta$ has mass $1/N$ for each individual)
which is offset by the fact that  
the birth or loss of a single 
individual at the point $y$, say, changes $\langle f,\eta\rangle$
by $f(y)/N$.

We use the fact that $\int q_\theta(x,dz)=1$ to rewrite~(\ref{mean measure})
as 
\begin{equation}
\label{eqn:rewritten mean measure}
\begin{split}
    \int\left(
        \int \theta \left( f(z) r(z,\eta)- f(x) r(x,\eta) \right) q_\theta(x,dz)
    \right)
    \gamma(x,\eta)
    \eta(dx)
    \\
    + \int \int f(x) \theta \Big(
        r(x,\eta) \gamma(x,\eta)
        - \mu_\theta(x,\eta)
    \Big) \eta(dx).
\end{split}
\end{equation}
We have defined $\mu_\theta$ so that the second term is simple:
\begin{align*}
    \theta \Big( r(x,\eta) \gamma(x,\eta) - \mu_\theta(x,\eta) \Big)
    = F(x, \eta) .
\end{align*}
Furthermore, recall from Definition~\ref{def:dispersal_generator} that
\begin{align} \label{eqn:near_critical}
    \int \theta \Big(
        r(z,\eta) f(z)
        -
        r(x,\eta) f(x)
    \Big) q_\theta(x,dz) 
    \qquad \stackrel{\theta\to\infty}{\longrightarrow} \qquad  
    \DG \big(r(\cdot,\eta)f(\cdot)\big)(x) .
\end{align}
In particular,
if dispersal is simply standard multivariate Gaussian
with mean zero and covariance $\sigma^2 I / \theta$,
then $\DG = \sigma^2 \Delta$, where $\Delta$ denotes the Laplacian.

In summary, equation~\eqref{eqn:rewritten mean measure} converges to
\begin{equation} \label{limit of mean measure equation}
\int \gamma(x,\eta)
\DG \big(f(\cdot)r(\cdot,\eta)\big)(x)
\eta(dx)
+
\int f(x)
F(x,\eta)
\eta(dx) .
\end{equation}
This explains the form of the martingale of Theorem~\ref{thm:nonlocal_convergence}.
%More generally, we expect everything to work
%if the remaining parameters ($r$, $\gamma$, and $F$)
%also depend on $N$, but converge as $N \to \infty$.


% % % % % % % 
\subsubsection*{Quadratic variation}

% Note: this bit in firstversionplus.tex has things written out in detail for
% the Poisson offspring number case.

We now look at the second order term in~(\ref{generator prelimit}).
An individual at location $x$ gives birth 
to a surviving offspring at $y$ at rate
$$
\gamma(x,\eta) r(y,\eta) q_{\theta}(x, dy) ,
$$
and since this increments $\langle f, \eta \rangle$ by $f(y) / N$,
the contribution to the quadratic variation from birth events,
which occur at rate $\theta$ per individual 
(so, rate $N\theta |\eta|$ overall), is
$$
\big\langle
    N \theta \gamma(x,\eta)
    \int \frac{1}{N^2} f^2(y) r(y,\eta)
    q_\theta(x,dy) 
    , \eta(dx)
\big\rangle .
$$
Similarly, the increment in $\langle f, \eta\rangle$ resulting from 
the death of an individual at $x$ is $f(x)/N$, and so combining with the 
above, the second order term in the generator takes the form
\begin{align*}
& G''(\langle f,\eta\rangle)
\frac{1}{2} N \theta
\left\{
    \Big\langle
        \gamma(x,\eta)
        \int \frac{1}{N^2}f^2(y)r(y,\eta)q_\theta(x,dy) 
    , \eta(dx)\Big\rangle
    +
    \Big\langle
        \mu_\theta(x,\eta)\frac{1}{N^2}f^2(x) 
    ,\eta(dx)\Big\rangle
\right\} \\
&\qquad
= \frac{1}{2} G''(\langle f, \eta \rangle)
    \frac{\theta}{N}
    \big\langle
        \gamma(x, \eta) \int f^2(y) r(y, \eta) q_\theta(x, dy) + f^2(x) \mu_\theta(x, \eta),
        \eta(dx)
    \big\rangle .
\end{align*}
Since $\int f^2(y) r(y, \eta) q_\theta(x, dy) \to f^2(x) r(x, \eta)$
and $r\gamma  + \mu_\theta = 2 r \gamma  - F / \theta \to 2 r \gamma$
as $\theta \to \infty$,
this converges to
\begin{align*}
\frac{\alpha}{2} G''(\langle f, \eta \rangle)
    \big\langle
	2 r(x, \eta) \gamma(x, \eta),
        \eta(dx)
    \big\rangle .
\end{align*}

An entirely analogous argument shows that if $G'''$ is bounded, then
the term $\epsilon_{\theta,N}(f, G, \eta)$ 
in~(\ref{generator prelimit}) will be $\bigO(\theta/N^2)$. 

If we hold $\rho_\gamma$, $\rho_r$, $\rho_F$ fixed, then 
by choosing $\theta/N \rightarrow 0$, the second order term in the generator 
will vanish and we expect a deterministic limit,
for which $\partial_t \langle f, \eta_t \rangle$ is equal to~\eqref{limit of mean measure equation}.
In other words, the limit is a weak solution to the deterministic equation
\begin{equation}
\label{deterministic limit}
\partial_t\varphi_t(x)
=
    r(x,\varphi_t)
    \DG\big(
        \gamma(\cdot,\varphi_t) \varphi_t(\cdot)
    \big)(x)
    + F(x, \varphi_t) \varphi_t(x) 
\end{equation}
in the sense of Definition~\ref{defn:weak_solutions},
where $\varphi_t$ is the density of $\eta_t$, if it has a density.
On the other hand, if $N = \alpha \theta$ for some $\alpha > 0$,
the second order term remains, and we expect a ``generalised superprocess'' limit.
The limiting quadratic variation
is exactly as seen in Theorem~\ref{thm:nonlocal_convergence}.

\paragraph{One-step convergence:}
In order to pass directly to a classical PDE limit
in Theorem~\ref{thm:local_convergence}
we impose the stronger condition
that $\theta/N\epsilon^d\to 0$ and also require that
$\theta\epsilon^2\to\infty$. 
Recall that in this case, we take $\rho_F$ to be a Gaussian density with parameter 
$\epsilon^2$. The condition $\theta\epsilon^2\to\infty$ 
ensures that $\epsilon^2$ is large enough relative to $1/\theta$
that the regularity gained by smoothing our population density by convolution with
$p_{\epsilon^2}$ is preserved under the dynamics dictated by $q_{\theta}$.
To understand the first condition, note that we are aiming to obtain a 
deterministic expression for the limiting population density. 
It is helpful to think
about a classical Wright-Fisher model (with no spatial structure and just two types, say). 
We know then that if the
timescale $\theta$ is on the same order as population size $N$, we see stochastic
fluctuations in the frequencies of the two types in the limit as $N\to\infty$; to 
obtain a deterministic limit, we look over shorter timescales relative to population
size. In our setting, the total population size is replaced by the local population
size, as measured by convolution with $p_{\epsilon^2}$, which we expect to be 
of order $N\epsilon^d$, and so in order to ensure a deterministic limit we 
take $\theta/(N\epsilon^d)\to 0$.


% % % % % % % % % % % % % % % % % %
\subsection{Motion of ancestral lineages}

Although our proof of Theorem~\ref{thm:lineages}
uses the explicit representation in terms of the lookdown process,
the result can be understood through informal calculations.
Suppose that we have traced a lineage back to an individual 
at location $y$ at time $t$.
Looking further back through time, at the time of the birth of that 
individual, the lineage will jump to the location of the parent of the
individual.
Now, the rate at which new individuals are born to parents at $x$ 
and establish at $y$ is
$$
\theta N\eta^N_t(dx)    \gamma(x, \eta^N_t) q(x, dy) r(y, \eta^N_t) .
$$
Suppose that $\eta^N$ did have a density (in the prelimit it does not),
say $\eta^N_t(dx) = \varphi^N_t(x) dx$. Informally, the 
probability that a randomly chosen individual near $y$
is a new offspring from a parent at $x$ in $[t, t+dt)$ is
\begin{equation} \label{eqn:informal_rates}
\frac{\theta
    \gamma(x, \eta^N_t) r(y, \eta^N_t) \varphi^N_t(x)
}{
    \varphi^N_t(y)
} \frac{ q_\theta(x, dy) }{ dy } dx dt .
\end{equation}
Leaving aside questions of whether a lineage can be treated 
as a randomly chosen individual, we define
a continuous-time jump process 
whose transition rates, conditional on $(\varphi^N_t)_{t=0}^T$, 
are given by \eqref{eqn:informal_rates}. Because we are tracing the
lineage backwards 
in time we make the substitution $s=T-t$ and write
$(L^N_s)_{s=0}^T$ for the location of a lineage that moves 
according to these jump rates.
Then, abusing notation to write 
$q_\theta(x, y)$ for the density of $q_\theta$,
\begin{align} \label{eqn:lineage_generator_Edt}
    \begin{split}
    &\IE[f(L^N_{s+ds}) - f(y) \;|\; L^N_s = y]
    \\&\qquad 
    =
    ds \theta \int \left(f(x) - f(y)\right)
    \frac{
        \varphi^N_{T-s}(x) \gamma(x, \eta^N_{T-s}) r(y, \eta^N_{T-s})
    }{
        \varphi^N_{T-s}(y)
    }
    q_\theta(x, y) dx .
    \end{split}
\end{align}
(Note that this integral is with respect to $x$.)
Referring back to Definition~\ref{def:dispersal_generator},
a quick calculation shows that as $N \to \infty$,
%\begin{align*}
%    \theta \int (h(x) - h(y)) q_\theta(x, y) dx
%    &\to
%    \DG^* h(y)  - h(y) \DG^* 1 .
%\end{align*}
%Consequently,
\begin{align*}
    &
    \theta \int (f(x) - f(y)) g(x) q_\theta(x, y) dx \\
    &\qquad =
    \theta \int \left\{
        (f(x) g(x) - f(y) g(y)) - f(y) (g(x) - g(y))
    \right\} q_\theta(x, y) dx \\
    &\qquad \to
        \DG^*(fg)(y) - f(y) \DG^* g(y) . 
\end{align*}
Applying this to~\eqref{eqn:lineage_generator_Edt} with $g = \varphi_{T-s} \gamma$,
this suggests that the generator of the limiting process is
\begin{align} \label{eqn:heuristic_lineage_generator}
    \Lgen_s f
    &=
    \frac{r}{\varphi_{T-s}}
    \left\{
        \DG^*(\gamma \varphi_{T-s} f) - f \DG^*(\gamma \varphi_{T-s})
    \right\} .
\end{align}
This agrees with Theorem~\ref{thm:lineages}.


%%%%%%%%%%%%%%%%%%%
\section{The lookdown process}
    \label{sec:lookdown}

Our characterisation of the motion of lines of descent (from which we establish
that of ancestral lineages) when we pass
to the scaling limit in our model will be justified via a lookdown construction.
In this section 
we present a lookdown construction for the general population model 
of Definition~\ref{defn:mgale_construction}. It will be
in the spirit of~\cite{kurtz/rodrigues:2011}. 
The general set-up is as follows.
Each individual will be labelled with a ``level'',
which will be a number in $[0, N]$.
We will still encode the process embellished by these levels
as a point measure:
if the $i^\mathrm{th}$ individual's spatial location is $x_i$
and level is $u_i$, then we will write:
$$
    \lp = \sum_i \delta_{x_i, u_i} ,
$$
which is a measure on $\IR^d \times [0, N]$.
Note that each individual contributes mass 1 to the measure,
not $1/N$ as above. If we assign mass $1/N$ to each individual and 
ignore the levels we will recover our population model. 
Moreover, at any time, the levels of 
individuals in a given spatial region 
will be exchangeable and conditionally uniform on $[0, N]$:
in particular, choosing the $k$ individuals with the lowest levels 
in that region
is equivalent to taking a uniform random sample of size $k$ 
from the population in the region.
However, an individual's level 
encodes information about their future reproductive output;
individuals with lower levels tend to live longer, and have more offspring.
For more explanation of the set-up and how this is possible,
see \citet{kurtz/rodrigues:2011} and \citet{etheridge/kurtz:2018} 
(and note that our $N$ corresponds to the $\lambda$ of those papers).
The power of this approach is that we can pass to a limit under the
same scalings as described in Theorem~\ref{thm:nonlocal_convergence}, 
and 
the limiting ``spatial-level'' process will still be a point measure.
In this way, 
we explicitly retain the notion of individuals and lineages as we pass to the 
infinite-population limit.


% % % % % % % % % % % % % % % %
\subsection{Lookdown representation of the model of 
Definition~\ref{defn:mgale_construction}}
\label{sec:lookdown_defn}


For the remainder of this section, when there is no risk of ambiguity we shall
suppress the superscript $N$ on the processes $\eta$ and $\xi$.
In this section,
we'll define the process $(\lp_t)_{t \ge 0}$ in terms of the 
dynamics of labelled particles,
and write down its generator.
The dynamics depend on the spatial locations of particles,
and in this section $\eta_t$ is the corresponding spatial measure,
i.e.,
$$
    \eta_t(\cdot) = \frac{1}{N} \lp_t(\cdot \times [0, N])  .
$$
A nontrivial consequence of the way we define $\lp_t$ will be that
this process has the same distribution as the process $(\eta_t)_{t \ge 0}$ 
of Definition~\ref{defn:mgale_construction}.
(This provides our justification for using the same notation for both.)


We describe the representation of the scaled process.
Following~\cite{etheridge/kurtz:2018}, we build the generator step by step
from its 
component parts. 
Suppose that the initial population is composed of $O(N)$ particles
with levels uniformly distributed between $[0, N]$,
and that the current state of the population is $\lp$,
with spatial projection $\eta$.
An individual at spatial location $x$ with level $u$
produces one juvenile offspring at rate 
$$
2 \theta \left(1 - \frac{u}{N}\right) \gamma(x, \eta) ,
$$
which disperses to a location relative to $x$ drawn 
from the kernel $q_{\theta}(x, \cdot)$.
Averaging over the uniform distribution of the level $u$,
we recover the birth rate $\theta\gamma(x, \eta)$.
This juvenile -- suppose its location is $y$ --
either survives, with probability $r(y, \eta)$, or immediately dies.
(As before, ``maturity'' is instantaneous.)

A new level $u_1$ is sampled independently and uniformly from $[u,N]$,
and the parent and the offspring are assigned in random order to the 
levels $\{u, u_1\}$.
This random assignment of levels to parent
and offspring will ensure that assignment of individuals to levels will
be exchangeable.

Evidently this mechanism increases the proportion
of individuals with higher levels.
To restore the property that
the distribution of levels is conditionally uniform
given $\eta$,
we impose that 
the level $v$ of an individual at location $x$
evolves according to the differential equation
$$
    \dot{v}
    =
    % \gamma^{\mathfrak{m}}(x, \eta) \left\{N^{-1}(N -v)^{2} -(N -v)\right\}.
    -\theta \frac{v}{N} \left(N - v\right)
    \gamma(x, \eta) \int_{\IR^d} r(y, \eta) q_\theta(x, dy) .
$$
Since $v \in [0, N]$, this moves levels down;
see~\cite{etheridge/kurtz:2018}, Section~3.4 for a detailed explanation.

In the lookdown model, levels never cross below 0,
while particles whose levels move above $N$ are regarded as dead
(and are removed from the population).
Therefore, in order to incorporate death,
the level of the individual at location $x$ with level $u$
moves upwards at an additional rate $\theta\mu_\theta(x,\eta) u$.
Since levels are uniform,
it is easy to check that if $\mu_\theta$ were constant,
this would imply an exponential lifetime for each individual;
see~\cite{etheridge/kurtz:2018}, Section~3.1
for more general justification.

Putting these together,
the level $u$ of an individual at $x$ evolves according to:
\begin{equation} \label{eqn:dot_u}
    \dot u
    =
    - \theta\frac{1}{N} u \left(N - u\right)
    \gamma(x, \eta) \int_{\IR^d} r(y, \eta) q_\theta(x, dy) 
    +
    \theta\mu_\theta(x,\eta) u .
\end{equation}
We shall write 
$$
    b_\theta(x, \eta)
    :=
    \theta\left(
    \gamma(x,\eta) \int_{\IR^d} r(y, \eta) q_\theta(x, dy)
    -
    \mu_\theta(x,\eta)
    \right) ,
$$
which captures the local net difference between reproduction and death.
%i.e., the deviation from criticality of the branching mechanism.
Recall from equation \eqref{eqn:mu_defn} that
$F(x,\eta) = \theta(r(x,\eta)\gamma(x,\eta) - \mu_\theta(x,\eta))$,
and so 
\begin{equation*} 
b_\theta(x, \eta)
=    \theta \gamma(x, \eta) \int_{\IR^d} \left( r(y, \eta) - r(x, \eta) \right) 
	q_\theta(x, dy)
    + F(x, \eta). 
\end{equation*}
Under Assumptions~\ref{def:model_setup}
this will tend to
\begin{equation}
\label{eqn:b_limit}
    \gamma(x, \eta) \DG r(x, \eta) + F(x, \eta)
\end{equation}
as $\theta \to \infty$.

We can then rewrite the differential equation 
governing the dynamics of the level of each individual as
\begin{align}
\dot{u}
    &=
    \theta\gamma(x,\eta) \int_{\IR^d} r(y, \eta) q_\theta(x, dy)
    \left\{
        -\frac{u}{N}\left(N - u\right)
        + u
    \right\}
    -
	b_\theta(x,\eta) u
    \nonumber \\
    &=
    \frac{\theta}{N} \gamma(x,\eta) \int_{\IR^d} r(y, \eta) q(x, dy) u^2
    -
	b_\theta(x, \eta)u
    . \label{differential equation for level}
\end{align}

Now, we can write down the generator for $(\lp_t)_{t \ge 0}$,
the lookdown process.
In what follows, we will write sums (and, products) over ``$(x, u) \in \xi$''
to mean a sum over the (location, level) pairs of each individual in the population.
Test functions for $\lp$ will take the form
\begin{equation} \label{eqn:test_functions}
f(\lp)=\prod_{(x,u)\in \lp}g(x,u)=\exp\left(\int \log g(x,u)\lp(dx, du)\right),
\end{equation}
where
$g(x,u)$ is differentiable in $u$ and 
smooth in $x$.
We will also assume that $0\leq g(x,u) \leq 1$ for all $u\in [0,N]$,
and $g(x,u)\equiv 1$ for $u\geq N$.
In the expressions that follow,
we shall often see one or more factor of $1/g(x,u)$;
it should be understood that if $g(x,u)=0$,
then it simply cancels 
the corresponding factor in $f(\lp)$.

First consider the terms in the generator that come from birth events.
When a birth successfully establishes,
a new level is generated above the parent's level,
and this new level is assigned to either the offspring or the parent.
Since the probability of each is 1/2,
the contribution of birth to the generator maps $f(\lp)$ to
\begin{align}
\label{partition over level assignment}
    &
    f(\lp)
    \sum_{(x, u) \in \lp}
    2 \frac{\theta}{N} \gamma(x, \eta)
    \int_u^N
    \int_{\IR^d}
    \left(
    \frac{1}{2}
    \bigg\{
            g(y, u_1)
        + \frac{ g(y, u) g(x, u_1) }{ g(x, u) }
    \bigg\}
        - 1
    \right)
    r(y, \eta) q_\theta(x, dy)
    du_1
    \\
    \begin{split} \label{eqn:birth_generator}
&=
    f(\lp)
    \sum_{(x, u) \in \lp}
    2 \gamma(x, \eta)
    \bigg\{
        \frac{1}{2 N}
        \int_u^N
        g(x, u_1) du_1
        \frac{
            \theta \int_{\IR^d} (g(y, u) - g(x, u)) r(y, \eta) q_\theta(x, dy)
        }{
            g(x, u)
        }
    \\ & \qquad \qquad \qquad {}
        + \frac{\theta}{N}
        \int_u^N \int_{\IR^d}
        \left( \frac{g(y, u_1) + g(x, u_1)}{2} - 1 \right)
        r(y, \eta) q_\theta(x, dy)
        du_1
    \bigg\}
    .
    \end{split}
\end{align}
In (\ref{partition over level assignment}),
$u_1$ is the new level and $y$ is the offspring's location,
and so the two terms in the integral correspond to the two situations:
in the first, we have added an individual at $(y, u_1)$,
while in the second, we replace an individual at $(x, u)$
by one at $(x, u_1)$ and another at $(y, u)$.
We've rewritten it in the form~(\ref{eqn:birth_generator})
because each of the two pieces
naturally converges to a separate term in the limit.

The remaining term in the generator is due to the motion of particles' levels.
Reading off from~(\ref{differential equation for level}),
it takes the form
\begin{align} \label{eqn:level_generator}
    f(\lp)
    \sum_{(x, u) \in \lp}
    \left(
    \frac{\theta}{N}
        \gamma(x,\eta) \int_{\IR^d} r(y, \eta) q_\theta(x, dy) u^2
        -
        b_\theta(x, \eta)u
    \right)
    \frac{\partial_u g(x,u)}{g(x,u)} .
\end{align}



We can now define the spatial-level process
explicitly as a solution to a Martingale Problem,
whose generator is just the sum of
\eqref{eqn:birth_generator} and \eqref{eqn:level_generator}.

% % % % % % % %
\begin{definition}[Martingale Problem Characterisation]
    \label{defn:lookdown_mgale}
For given positive values of $N$ and $\theta$
and $\lp_0 \in \mathcal{M}_F(\IR^d \times [0,N])$,
we define $(\lp^{N}_t)_{t \geq 0}$
to be the unique solution to the martingale problem
with initial condition $\lp_0$ and generator $A^{N}$ that satisfies
\begin{equation}
	\label{eqn:lookdown_generator}
\begin{split}
& A^{N}f(\lp ) \\
&\quad =
    f(\lp)
    \,\sum_{(x,u)\in \lp}\,
    2 \gamma(x, \eta)
    \Bigg\{ \frac{1}{2N} \int_u^N g(x,u_1) du_1
            \frac{
                \theta \int_{\IR^d}
                (g(y,u) - g(x,u))
                r(y, \eta) q_{\theta}(x,dy)
            }{ g(x,u) }
        \\
    &\qquad\qquad\qquad\qquad\qquad\qquad\qquad {} +
        \frac{\theta}{N} \int_u^N
        \int_{\IR^d}\left(
            \frac{ g(y,u_1) + g(x,u_1) }{ 2 } - 1
        \right)
        r(y, \eta) q_{\theta}(x,dy)
        du_1
    \Bigg\}\\
    &\qquad\qquad {} +
    f(\lp) \sum_{(x,u)\in\lp}\,
    \left(
        \frac{\theta}{N} \gamma(x,\eta) \int_{\IR^d} r(y, \eta) q_\theta(x, dy) u^2 -b_{\theta}(x,\eta)u
    \right)
    \frac{\partial_u g(x,u)}{g(x,u)}
    ,
\end{split}
\end{equation}
where $f(\lp) = \prod_{(x, u) \in \lp} g(x, u)$ is as defined in \eqref{eqn:test_functions},
and $\eta(\cdot) = \lp(\cdot \times [0, N]) / N$
as before.
\end{definition}

The martingale problem for finite $N$ has a unique solution.
Next we state the limiting martingale problem,
for which we do not have uniqueness.
%(although we construct an explicit solution later).
As before, the parameter $\alpha$ will correspond to 
$\lim_{N\to\infty}\theta(N)/N$. Whereas for finite $N$, conditional
on the population process $\eta_t^N$, the levels of particles are
independent and 
uniformly distributed on $[0,N]$, in the infinite population limit, 
conditional on $\eta_t$ the process $\xi_t$ is 
Poisson distributed on 
$\IR^d\times [0,\infty)$ with mean measure $\eta_t\times\ell$, where $\ell$
is Lebesgue measure.

\begin{definition}[Martingale Problem Characterisation, scaling limit]
    \label{defn:limiting_lookdown_mgale}
    Fix $\alpha \in [0, \infty)$, and define  
    test functions $f$ by 
    $f(\xi) = \prod_{(x, u) \in \xi} g(x, u)$ 
	with $g$ differentiable in $u$, smooth in $x$, satisfying 
$0\leq g(x,u)\leq 1$ and
    such that there exists a $u_0$ with $g(x, u) = 1$ for all $u > u_0$.
    Then, define the operator $A$ on such test functions by 
    \begin{align} \begin{split} \label{eqn:limiting_lookdown_generator}
    A f(\lp)
    &=
        f(\lp)  \sum_{(x, u) \in \xi}
        \gamma(x, \eta)
            \frac{
                \DG(g(\cdot, u) r(\cdot, \eta))(x) - g(x,u) \DG r(x,\eta)
            }{
                g(x, u)
            }
    \\ &\qquad {} +
        f(\lp) \sum_{(x, u) \in \xi}
        2 \alpha \gamma(x, \eta) r(x, \eta \int_u^\infty (g(x, u_1) - 1) du_1
    \\ &\qquad {} +
        f(\lp) \sum_{(x, u) \in \xi}
        \left(
            \alpha \gamma(x, \eta) r(x, \eta) u^2
            -
            \left\{
                \gamma(x, \eta) \DG r(x, \eta) + F(x, \eta)
            \right\} u
        \right)
        \frac{\partial_u g(x, u)}{ g(x,u) }  .
    \end{split} \end{align}
    Write ${\cal C}(\IR^d\times [0,\infty))$ for the point measures on $\IR^d\times [0,\infty)$.
We say that a ${\cal C}(\IR^d\times [0,\infty))$-valued process $(\xi_t)_{t \ge 0}$
    is a solution to the $(A, \lp_0)$ martingale problem
    if it has initial distribution $\lp_0$
    and $f(\lp_t) - f(\lp_0)-\int_0^t Af(\lp_s) ds$ is a martingale for all 
test functions $f$
    as defined above.
\end{definition}

\comment{We talked about having the next paragraph be a Proposition or something?
I am putting off rewriting this paragraph until I know what has been proved.}


The lookdown processes have been carefully constructed so that
past observations about the spatial positions of individuals in 
the population do not give us any information about the assignment of 
individuals to levels. In other words, the dynamics of the lookdown
process preserve the conditionally uniform (resp.~conditionally Poisson)
structure.
Moreover, if we average over levels in the expression for the generator
(equation~\eqref{eqn:lookdown_generator} or~\eqref{eqn:limiting_lookdown_generator})
we recover the generator for the population process.
%(visible in equation~\eqref{eqn:eta_martingale}
%or~\eqref{eqn:limiting_mgale_problem}, respectively).
Once this is verified (along with some boundedness conditions)
in Appendix~\ref{sec: Markov Mapping Theorem Application},
\comment{Refer more precisely}
the Markov Mapping Theorem \citep[Thm A.2 in][]{etheridge/kurtz:2018}
tells us that by ``removing labels'' in this way we recover the Markov
process describing our population.
In particular, for any %population process $(\eta^N_t)_{t \ge 0}$
%(i.e.,
solution to the martingale problem of Definition~\ref{defn:mgale_construction})
% of equations~\eqref{eqn:eta_martingale} and \eqref{eqn:prelimit_martingale_variation}
there is a % lookdown representation $(\lp^N_t)_{t \ge 0}$
%(i.e., 
solution to the martingale problem of Definition~\ref{defn:lookdown_mgale})
for which $\eta^N_t(\cdot) = \lp^N_t(\cdot \times [0, N])/N$
and such that at each $t$, conditioned on $(\eta^N_s)_{s \le t}$, the levels of $\lp_t^N$
are independently and uniformly distributed on $[0, N]$.
Similarly, in the scaling limit:
for any solution $(\eta_t)_{t \ge 0}$ to the martingale problem of 
Theorem~\ref{nonlocal_convergence}
%there is a lookdown process $(\lp^N_t)_{t \ge 0}$
there is a solution to the martingale problem of Theorem~\ref{lookdown_convergence}
for which $\eta_t(\cdot) = \lim_{u_0 \to \infty} \frac{1}{u_0} \lp_t(\cdot \times [0, u_0))$
and such that at each $t$, conditional on $(\eta_s)_{s\leq t}$, 
$\lp_t$ is conditionally Poisson with mean measure $\eta_t(dx) \times du$.
Furthermore, convergence of the lookdown processes would imply convergence of the population processes
\citep[Thm A.9 in][]{kurtz/rodrigues:2011}.

Now that the lookdown process has been defined,
we can present the main convergence theorem that is analogous
to Theorem~\ref{thm:nonlocal_convergence} for the population process.

\comment{the next theorem needs rewriting along the lines of Thm A.12 in 
Kurtz and Rodrigues.}

\begin{theorem}
    \label{thm:lookdown_convergence}
    Let $(\lp_t^N)$ be as defined in Definition~\ref{defn:lookdown_mgale}
    and assume that as $N \to \infty$,
    $\theta \to \infty$ in such a way that $\theta/N \to \alpha$.
    Suppose also that $\eta^N_0 \to \eta_0$ (as measures on $\IR^d$),
    and that for each $N$, $\lp^N_0(dx, du)$ is conditionally Poisson on $\IR^d \times [0, N]$
    with Cox measure $\eta^N_0(dx) \times du$.
    Then, the sequence of stochastic processes $(\lp_t^N)_{t \ge 0}$
    converges along subsequences in distribution as $N \to \infty$
    to a measure-valued process $(\lp_t)_{t \ge 0}$
    with $\lp_0$ conditionally Poisson with Cox measure $\eta_0(dx) \times du$,
    that is a solution to the martingale problem~\ref{defn:limiting_lookdown_mgale}.
\end{theorem}


%% %% %% %% %% %% %% %% %% %% %% %%
\subsection{Explicit construction of lines of descent}
    \label{sec: individual lines of descent}

 
The main interest in using a lookdown construction for our population
processes is that it allows us to retain information about the 
relatedness of individuals as we pass to the infinite population limit.
In order to exploit this, in this section we write down stochastic 
equations for the locations and levels of individuals in the lookdown
representation of the prelimiting model.
We will then be able to pass to the scaling limit.
This explicit description of the solution to
the limiting martingale problem of Definition~\ref{defn:limiting_lookdown_mgale}
will enable us to identify all 
individuals in the current population that are descendants of a
given ancestor at time zero. In theory at least, this allows us to
recover all the information about genealogies relating individuals 
sampled from the present day population. This idea draws on the notion
of ``tracers'', popular in statistical physics and used in population
genetics by a number of authors including 
\cite{biswas/etheridge/klimek:2018, durrett/fan:2016, hallatschek/nelson:2008}.

We will construct the process using a Ulam-Harris indexing scheme.
First, we assign each extant individual a unique label from $\IN_{\ge 0}$.
Suppose an individual with label $a$ and level $u$ reproduces,
and as a result there are two individuals, one with level $u$ and one with a new level $u_1 > u$.
The parent individual, previously labeled $a$, might be assigned either level.
We will track chains of descendant individuals forwards through time
following levels, rather than individuals, and will call this a \emph{line of descent}.
So, after reproduction, we give a new label to \emph{only} the individual
that is given the new level $u_1$,
retaining the label $a$ for the individual with the old level $u$.
In this way, at each birth event, a unique label is 
assigned to the resulting individual with the higher level,
and the label of an individual may change throughout its lifetime.

Concretely, then: for each label $a$ in
$\labelspace = \bigcup_{k \ge 1} \IN_{\ge 1}^k$,
let $\Pi_a$ be an independent Poisson process on 
$[0, \infty)^2 \times \IR^d \times \{0,1\}$.
The mean measure of each $\Pi_a$ is a product of Lebesgue 
measure on the first two coordinates,
the density of the standard Gaussian $N(0,1)$ on $\IR^d$, and 
$(\delta_0 + \delta_1)/2$ on $\{0, 1\}$.
It will also be convenient
to suppose that for each label $a$ we have an enumeration of the points in $\Pi_a$,
so we may refer to ``the $j^\text{th}$ point in $\Pi_a$'',
although the precise order of this enumeration is irrelevant.
If $(\tau, v, z, \kappa)$ is the $j^\text{th}$ point in $\Pi_a$,
then $\tau$ will determine a possible birth time,
$v$ will determine the level of the offspring,
$z$ will give the spatial displacement of the offspring relative to the parent,
$\kappa$ will be used to determine whether parent or offspring is assigned the new level,
and the new label produced will be $a \concat j$,
i.e., the label $a$ with $j$ appended
(so, if $a = (a_1, \ldots, a_k)$ then $a \concat j = (a_1, \ldots, a_k, j)$).
Each label $a$ has a birth time $\tau_a$,
when it is first assigned,
and a death time $\sigma_a$, when its level first hits $N$.
For any $\tau_a \le t \le \sigma_a$ we denote by $X_a(t)$ and $U_a(t)$ the spatial location and level
of the individual carrying label $a$ at time $t$, respectively.
Now, since we have defined labels so that the level does not jump,
$U_a$ satisfies~\eqref{differential equation for level}
	for $\tau_a \le t \le \sigma_a$, i.e.,
\begin{equation} \label{eqn:U_line_of_descent}
    \begin{split}
& U_a(t)
    =
    U_a(\tau_a) \\
&\qquad {}   
    + \int_{\tau_a}^{t}
    \left(
        \frac{\theta}{N} \gamma(X_a(s),\eta_s)
        \int_{\IR^d} r(z,\eta_s) q(X_a(s),dz) U_a(s)^2
        -
        b(X_a(s),\eta_s) U_a(s)
    \right)
    ds ,
\end{split}
\end{equation}
and, of course, $\sigma_a = \inf\{t \ge \tau_a : U_a(t) > N\}$.

Potential reproduction events occur at times $\tau$
for each point $(\tau, v, z, \kappa) \in \Pi_a$ with $\tau \ge \tau_a$.
We say ``potential'' since if the resulting level is greater than $N$,
the event does not happen.
If this is the $j^\text{th}$ point in $\Pi_a$,
the new label is $a \concat j$, the (potential) birth time is $\tau_{a \concat j} = \tau$.
Similarly, the potential birth location is $y(X(\tau-), z)$, where
$$
    y(x, z)
    :=
    \frac{1}{\theta}\meanq(x)
    +
    \frac{1}{\sqrt{\theta}}K(x) z,
$$
and $K(x)K^{T}(x) = \covq(x)$.

Next we must choose the new level created at the birth event.
We would like an individual with level $u$ and at spatial position $x$
to produce offspring at $y$ at instantaneous rate
\begin{equation}
	\label{rate of birth}
	2 \Big(1 - \frac{u}{N}\Big) \theta \gamma(x, \eta) r(x + y, \eta).
\end{equation}
To do this
we will associate the point $(\tau, v, z, \kappa) \in \Pi_a$ with 
level $u + v \ell$,
where $\ell$ is chosen so that the rate of appearance of points in 
$\Pi_a$ with level below $N$, that is points with
$\ell v<N-u$, is given by~(\ref{rate of birth}).
Since the mean measure of $\Pi_a$ is Lebesgue measure
in the $t$ and $v$ directions, we must take
\begin{equation}
	\label{expression for ell}
    \ell(x, y, \eta)
    =
    \frac{N-u}{2 (1-u/N) \theta \gamma(x, \eta) r(x + y, \eta) } 
    =\frac{1}{2N^{-1} \theta \gamma(x, \eta) r(x + y, \eta) } 
    ,
\end{equation}
and using this, the (potential) new level is
\begin{equation*}
    U_{a \concat j}(\tau)
    =
    U_a(\tau)
    +
    v \ell(X_a(\tau-), y(X_a(\tau-), z), \eta(\tau-)) .
\end{equation*}
If $U_{a \concat j}(\tau) < N$,
the new individual labeled $a \concat j$ is produced,
and $\kappa$ determines which label, $a$ or $a\concat j$,
is associated with the new location,
so
\begin{align*}
    X_{a \concat j}(\tau)
    &=
    X_a(\tau-) + (1 - \kappa) y(X_a(\tau-), z) .
\end{align*}
On the other hand
if $U_{a \concat j}(\tau) \ge N$, then $X_a$ is unchanged and 
$X_{a \concat j}$ is undefined,
so
\begin{align} \label{eqn:X_line_of_descent}
    X_a(\tau)
    &=
    X_a(\tau-) + \kappa y(X_a(\tau-), z) \ind_{U_{a \concat j}(\tau) < N} .
\end{align}
Recall that the parental \emph{individual} always retains their spatial location,
so that $\kappa = 0$ corresponds to the parent being assigned a new level,
and our line of descent switching to the offspring.
Combining these, $X_a$, for $\tau_a \le t < \sigma_a$, solves the equation 
\begin{align*}
    X_a(t)
    &=
    \int_{[\tau_a, t) \times [0, \infty) \times \IR \times [0, 1]}
    y(X_a(\tau-), z)
    \kappa
    \ind_{
        U_a(\tau) + v \ell(X_a(\tau-), y(X_a(\tau-), z), \eta_{\tau-})<N
    }
    d\Pi_a(\tau, v, z, \kappa) .
\end{align*}

Although we have described the evolution of a line of descent only for a given label
(i.e., for $\tau_a \le t < \sigma_a$),
we can extend the definition to times
$0 \le t < \sigma_a$ by setting
$X_a(t)$ equal to $X_{[a]_t}(t)$,
where $[a]_t$ is the label of the ancestor of label $a$ alive at time $t$,
and similarly for $U_a(t)$.
It is then straightforward, albeit tedious,
to write down the time evolution of $(X_a(t), U_a(t))$ for all time back 
to $t=0$
in terms of the driving Poisson processes.

We can now express the population processes using these levels:
\begin{align*}
    \eta^N_t = \frac{1}{N} \sum_{a : \tau_a \le t < \sigma_a} \delta_{X_a(t)}
    \qquad \text{and} \qquad
    \lp^N_t = \sum_{a : \tau_a \le t < \sigma_a} \delta_{(X_a(t), U_a(t))} .
\end{align*}

\begin{remark}
    Note here that although we have a single construction that couples
    the processes across all $N$, the actual 
trajectories do not necessarily coincide for 
different values of $N$, since they are affected by the whole 
population process.
    However, this does suggest approximating the genealogies
    by simulating up until a sufficiently high level
    that we have a good approximation to the population process.
\end{remark}

% % % % % % % % % % % % %  % % %

%% %% %% %% %% %% %% %% %% %% %% %%
\subsection{Limiting processes for lines of descent}
\label{sec:limiting_lines_of_descent}

The previous section constructed the lookdown process
using the same underlying Poisson processes $\{\Pi_a\}_{a \in \labelspace}$ for
different values of $N$.
As a result, individual lines of descent converge pointwise as $N \to \infty$.
To see this, first note that if the Poisson processes are fixed
then the set of events to which a given label $a \in \labelspace$ is associated
is also fixed -- this is the sequence $(\tau_k, v_k, z_k, \kappa_k)$ associated 
with the label $a$.
To conclude that the lines of descent converge, 
first, we clearly need that the spatial projections $\eta$ converge.
%
Supposing that they do,
consider how a line of descent $(X_a(t), U_a(t))$ evolves.
It throws off a new line of descent at a higher level
when there is a point $(\tau, v, z, \kappa)$ in $\Pi_a$ with $\tau > \tau_a$ and
\begin{equation} \label{eqn:line_points}
    v < 2 \frac{\big(N - U_a(\tau-)\big)}{N} \theta \gamma(X_a(\tau-), \eta_{\tau-}) 
	r\Big(X_a(\tau-) + y\big(X_a(\tau-),z\big), \eta_{\tau-}\Big) .
\end{equation}
Since the mean measure of the $v$ coordinate is Lebesgue measure,
$\theta/N \to \alpha$,
and $q_\theta(x, dy) \to \delta_x(dy)$,
this corresponds in the limit to new lines of descent being thrown off 
according to a Poisson Point Process with intensity
%at all times,
%i.e., according to a Poisson process with rate
$$
2 \alpha \gamma(X_a(t), \eta_t) r(X_a(t), \eta_t)dt \times du.
$$
%and uniform intensity on higher levels.
Now consider the location of the line of descent:
at each birth event, with probability one half
the line of descent jumps to $X_a(t) + y$.
Taking $g$ to be a suitable test function on $\IR^d$, 
and rewriting~\eqref{eqn:line_points}, when the level is $u$
and the state of the population is $\eta$, the generator of the
spatial motion of the line of descent
applied to $g(x)$ is
\begin{align*}
    &
    \left(1 - \frac{u}{N}\right) \gamma(x, \eta)
    \theta \int_{\IR^d} r(x+y, \eta) (g(x+y) - g(x)) q_\theta(x, dy) \\
    &\qquad {}
    =
    \left(1 - \frac{u}{N}\right) \gamma(x, \eta)
    \bigg\{
        \theta \int_{\IR^d} (r(x+y, \eta) g(x+y) - r(x, \eta) g(x)) q_\theta(x, dy) \\
        &\qquad \qquad {}
        -
        \theta \int_{\IR^d} (r(x+y, \eta) - r(x, \eta) ) g(x) q_\theta(x, dy) 
    \bigg\} \\
    &\qquad {}
    \to
    \gamma(x, \eta)
    \left(
        \DG(rg)(x) - g(x) \DG(r)(x)
    \right) ,
    \qquad \text{as } N, \theta \to \infty .
\end{align*}
Notice that the factors of 2 have cancelled,
and that the result is independent of $u$.
Also recall that $r(x, \eta)$ depends on $\eta$ only
through $\smooth{r}\eta(x)$,
which is guaranteed to be smooth, so that $\DG(r)$ 
and $\DG(gr)$ are well-defined.


We write out the differential operator above in more detail.
Recall that $\DG g(x) = \sum_i \meanq_i \partial_i g(x) + \sum_{ij} \covq_{ij} \partial_{ij} g(x)$,
and for the moment write $r(x)$ for $r(\cdot, \eta)(x)$,
so that
\begin{align}
\DG(rg)(x) - g(x) \DG(r)(x)
    &= \nonumber
    r(x) \sum_i \meanq_i \partial_i g(x)
    + 2 \sum_{ij} \meanq_i \partial_i r(x) \covq_{ij} \partial_j g(x)
    + r(x) \sum_{ij} \covq_{ij} \partial_{ij} g(x) \\
    &= \label{eqn:limiting_generator}
    r(x) \left\{
        \left(
        \meanq
        + 2 \covq \grad \log r(x)
        \right)
        \cdot
        \grad g(x)
        +
        \sum_{ij} \covq_{ij} \partial_{ij} g(x)
    \right\} .
\end{align}

The only thing that remains is to describe how the levels change,
but this is immediate from applying limit~\eqref{eqn:b_limit}
to equation~\eqref{differential equation for level}.

We summarize the results in a proposition.

\begin{proposition}[Line of descent construction]
    \label{prop:limiting_construction}
Let 
$J(x,\eta)$ be such that $\gamma(x,\eta)\covq(x) = J(x,\eta) J(x,\eta)^T$, and set
$\beta(x, \eta) = \gamma(x,\eta)\big(\meanq(x) + 2 \covq(x) \grad \log r(x, \eta)\big).$
Associate with each label $a \in \labelspace$
an independent $d$-dimensional Brownian motion $W_a$
and an independent Poisson process $R_a$ on $[0, \infty)^2$
with Lebesgue mean measure,
as before with points ordered in some way.
Given $\eta_0 \in \measures$,
let $(x_i, u_i)$ be the points of a Poisson process
with mean measure on $\IR^d \times [0, \infty)$ the product of $\eta_0$ and Lebesgue measure.
For each point begin a line of descent
with label $i$, location $X_i(0) = x_i$, level $U_a(0) = u_i$, and birth time $\tau_i = 0$.
We write $\tau_a$ for the birth time of the label $a$ and 
$\sigma_a = \lim_{K \to \infty} \inf\{t \ge 0: U_a(t) > K\}$
is the first time the level corresponding to the label $a$ exceeds $K$.
Suppose that the spatial locations and level of each line of descent $a$
solve, for $\tau_a \le t < \sigma_a$,
\begin{align*}
X_a(t)
    &=
    X_a(\tau_a)
    + \int_{\tau_a}^{t}
        \beta(X_a(s), \eta_s) ds
    + \int_{\tau_a}^{t}
        J(X_a(s),\eta_s) dW_a(s)
    \\
U_a(t)
    &=
    U_a(\tau_a)
    + \int_{\tau_a}^{t}
    \bigg(
        \alpha \gamma(X_a(s),\eta_s)
        r(X_a(s), \eta_s) U_a(s)^2
\\ &\qquad \qquad \qquad {}   
        -
        \big\{
            \gamma(X_a(s),\eta_s) \DG r(X_a(s),\eta_s)
            + F(X_a(s), \eta_s)
        \big\}
        U_a(s)
    \bigg)
    ds .
\end{align*}
Each point in each $R_a$ denotes a potential birth time for $a$:
if the $j^\text{th}$ point in $R_a$ is $(\tau, v)$, with $\tau_a \le t < \sigma_a$,
then a new line of descent with label $a \concat j$ is produced,
with birth time $\tau_{a \concat j} = \tau$,
    location $X_{a \concat j}(\tau) = X_a(\tau)$, and level
$$
    U_{a \concat j}(\tau) = U_a(\tau)
    + \frac{v}{ 2 \alpha \gamma(X_a(\tau), \eta_\tau) r(X_a(\tau), \eta_\tau) } .
$$
For any solution to the equations above, the processes defined by
\begin{align*}
    \eta_t
    =
    \lim_{u_0 \to \infty} \frac{1}{u_0}
        \sum_{a : \tau_a \le t < \sigma_a \; ; \; U_a(t) < u_0} \delta_{X_a(t)}
    \qquad \text{and} \qquad
    \lp_t = \sum_{a : \tau_a \le t < \sigma_a} \delta_{(X_a(t), U_a(t))} 
\end{align*}
are solutions to the martingale problems of
Theorems~\ref{thm:nonlocal_convergence} and~\ref{thm:lookdown_convergence},
respectively.
\end{proposition}

In particular, note that if $\alpha=0$,
no new lines of descent are produced.
More precisely, comparing with~(\ref{expression for ell}),
they are produced, but ``at infinity'',
and their trace is seen in the 
spatial motion of the line of descent which results from
the production of these lineages.

\comment{TODO: find the right place for this proof?}
\begin{proof}[of Proposition~\ref{prop:limiting_construction}]
    The fact that a solution to the system of equations
    is a solution to the martingale problem of Theorem~\ref{thm:lookdown_convergence}
    is an application of It\^o's theorem.
    The calculations in \comment{XXX} leading to our application
    of the Markov Mapping Theorem then show that 
    the conditional Poisson property of $\lp_0$ is preserved
    (i.e., holds for $\lp_t$ for all $t$), and so
    $(\eta_t)_{t \ge 0}$ is well-defined,
    and furthermore that $\eta_t$ is a solution
    to the martingale problem of Theorem~\ref{thm:nonlocal_convergence}.
\end{proof}

\comment{Don't we have to state everything in terms of subsequences along which the
empirical measures converge?}

%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Motion of ancestral lineages}
\label{ancestral lineages}

We now show how, in the case in which the scaling limit of the population 
process is deterministic, one can read off a description of 
the motion of ancestral lineages from that of 
the lines of descent derived in the previous subsection.

Consider then a population evolving according to one of our deterministic
limiting models. There are no births of new levels; 
changes in population size are reflected only through the motion 
of levels. 

We shall restrict ourselves
to the setting in which the population is expanding as a travelling wave
through $\IR$, with fixed profile $w(x)$ moving at rate $c$. That is 
$$\Xi_t(x)=w(x-ct),$$
for some fixed function $w$ and some $c>0$.
The profile $w$ will satisfy
\begin{equation}
\label{profile}
r\DG^*(\gamma w)+Fw+c\partial_x w=0.
\end{equation} 
It is convenient to work in a moving frame so that the wavefront is fixed.
Let $Y$ denote the diffusion followed by a single line of descent in this frame; 
i.e.~the diffusion with generator 
${\cal L}^Yf=\gamma \DG(rf)-\gamma f\DG(r)-c\partial_x f$.
%\overline{B}(w)\phi-c\phi_x$.
For the motion of its level, note that 
$b=\gamma\DG(r)+F$, and,  
since we are in the deterministic limit, $\alpha=0$.

Informally, the intensity of levels at $y$ at time $t$ that are descended from 
individuals that were at $x$ at time $0$ is
\begin{equation}
\label{formal intensity}
w(x)\IE_x\left[\exp\left(\int_0^t b(Y_s)ds\right)\ind_{Y_t=y}\right]dy,
\end{equation}
where the subscript $x$ in the expectation indicates that $Y_0=x$.
To see why this should be true, 
suppose that an ancestor has level $u$. Conditional on its 
spatial motion $\{Y_s\}_{0\leq s\leq t}$, its level at time $t$ will
be $u\exp(-\int_0^tb(Y_s)ds)$. This will be less than a given level 
$\lambda$ if $u<\lambda \exp(\int_0^tb(Y_s)ds)$. 
The intensity of levels at $y$ that are descended from individuals at
$x$ can therefore be obtained as the limit as $\lambda\to\infty$ of 
$1/\lambda$ times the number of levels at $x$ at time zero with
$u<\lambda \exp(\int_0^tb(Y_s)ds)$ and for which
the corresponding individual is at $y$ at time $t$, which is 
precisely the quantity in~(\ref{formal intensity}). 

Note that at time $0$ the expression in~(\ref{formal intensity}) is satisfied, and that
writing 
\[
\psi(t,x)=
\IE_x\left[\exp\left(\int_0^t b(Y_s)ds\right)\ind_{Y_t=y}\right],
\]
and substituting the expressions for ${\cal L}^Y$ and $b$ from above, we have
\begin{eqnarray*}
\frac{d}{dt}\int
w(x)\psi(t,x)dx&=& \int w(x)\Big\{\Big(\gamma\DG(r\psi)-\gamma\psi\DG(r)-c\partial_x\psi\Big)
+\psi\gamma\DG(r)+\psi F\Big\}dx
\\
&=&\int\psi\Big\{r\DG^*(\gamma w)+Fw+c\partial_x w\Big\}dx=0.
\end{eqnarray*}
Thus when we integrate~(\ref{formal intensity})
with respect to $x$ we recover $w(y)dy$. 
As a consequence of~(\ref{formal intensity}), writing $p(t,y,x)$ for
the probability density that the ancestor at time $t$ units before the present 
of an individual sampled 
at $y$ was at $x$, then, still informally,
\begin{equation}
\label{ptyx}
p(t,y,x)=\frac{w(x)}{w(y)}\IE_x\left[\exp\left(\int_0^t b(Y_s)ds\right)
\ind_{Y_t=y}\right].
\end{equation}

To make~(\ref{ptyx}) meaningful, we multiply by suitable test functions and 
integrate.
$$\int\int \psi(y)w(y)p(t,y,x)\phi(x)dydx=\int\phi(x)w(x)
\IE_x\left[\exp\left(\int_0^tb(Y_s)ds\right)\psi(Y_t)\right]dydx.$$

Writing $\hat{T}_t$ for the semigroup corresponding to the motion of 
ancestral lineages backwards in time (that is the semigroup corresponding
to $p(t,x,y)$), we can write this as 
$$\int \psi(y)w(y)\hat{T}_t\phi(y)dy=\int\phi(x)w(x)
\IE_x\left[\exp\left(\int_0^tb(Y_s)ds\right)\psi(Y_t)\right]dx.$$
Differentiating with respect to $t$ at $t=0$, and writing 
$\hat{\mathcal L}$ for the generator of $\hat{T}_t$, we find
\begin{eqnarray*}
\int \psi(y)w(y)\widehat{\mathcal L}\phi(y)dy&=&
\int\phi(x)w(x)\left({\mathcal L}^Y+b\right)\psi(x)dx\\
&=&\int\phi(x)w(x)\left(\gamma\DG(r\psi)-\gamma\psi\DG r -c\nabla\psi 
+b\psi\right)dx\\
&=&\int\phi(x)w(x)\left(\gamma\DG(r\psi)-c\nabla\psi+F\psi\right)dx\\
&=&
\int\psi(x)\left[r\DG^*(\gamma \phi w)+F\phi w+c\nabla(\phi w)\right]dx.
\end{eqnarray*}
Since $\psi$ was arbitrary,

\begin{eqnarray*}
\hat{\mathcal L}\phi&=&\frac{1}{w}\left[ r\DG^* (\gamma\phi w)+F\phi w
+c\nabla(\phi w)\right]\\
&=&\frac{\phi}{w}\left[r\sigma^2\Delta(\gamma w)+Fw+-\meanq \grad(\gamma\phi) w +c\nabla w\right]
\\
&& +\frac{1}{w}\left[r\gamma w\sigma^2\Delta \phi +2r\sigma^2\nabla (\gamma w)
\nabla\phi -\meanq \gamma\phi \grad w+cw\nabla\phi\right]\\
&=&r\gamma\left(\sigma^2\Delta\phi+2\sigma^2r\frac{\nabla(\gamma w)}{\gamma w}\nabla\phi\right)
-\meanq\gamma\nabla\phi+c\nabla\phi,
\end{eqnarray*}
where we have used that $w$ solves~(\ref{profile}), and we have recovered~(\ref{Lgen}).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proofs}
\label{sec:proofs}

\comment{TODO structure of this section}

% % % % % % % % % % % % %
%\subsection{Convergence of the population density process}
\subsection{Proof of Theorem~\ref{thm:nonlocal_convergence}: nonlocal limit points}
    \label{sec:population_density_proof}

In this section we prove 
Theorem~\ref{thm:nonlocal_convergence}.
This would be implied by convergence of the lookdown process
(see \citet{kurtz/rodrigues:2011} and \citet{etheridge/kurtz:2019});
however in our setting,
because the parameters in the lookdown process
depend on the empirical distribution,
we actually use tightness of the sequence of population processes
in the proofs of tightness for the corresponding lookdown processes.

\begin{proof}[Proof of Theorem~\ref{thm:nonlocal_convergence}.]
The proof follows a familiar pattern. First we extend $\IR^d$ 
to its one-point compactification $\overline{\IR}^d$ and 
establish, in Lemma~\ref{lem:eta_compact_containment},
compact containment of the 
sequence of scaled population
processes in $\cmeasures$ %$\cal{M}_F(\widehat{\IR}^d)$ 
(for which, since we have compactified $\IR^d$,
it suffices to consider the sequence of total masses); armed with this, 
%Theorem~3.9.1 of~\cite{ethier/kurtz:1986} tells us that 
tightness of the population processes 
in $\mathcal{D}_{\cmeasures}([0,\infty))$
will follow if we can 
establish tightness of the 
real-valued processes $(G(\eta_t))_{t\geq 0}$ for a sufficiently large
class of test functions $G$, which we check 
through an application of the
Aldous-Rebolledo criterion in Lemma~\ref{lem:eta_projections_tightness}.
%{compactnedd projections}.
These ingredients are gathered together in 
Proposition~\ref{tightness in one point compactification}
to deduce tightness of the scaled population processes in 
the larger space $\mathcal{D}_{\cmeasures}([0,\infty))$.

We then characterise
limit points as solutions to a martingale problem in
Lemma~\ref{lem:limit_mgale}; finally in Lemma~\ref{no mass at infty}
we check that 
in the process of passing to the limit, no mass `escaped to infinity', so that
in fact the limit points take values in $\mathcal{D}_{\measures}([0,\infty))$.
\end{proof}


% % % % % % % % % % % % %
\subsubsection{Compact Containment Condition}

As advertised, we work with the one-point compactification of $\IR^d$ and
consider $(\eta^N_t)_{t\geq 0}$ as a sequence of 
$\cmeasures$-valued processes. Since, for each $K>0$,
$\{\eta: \langle 1,\eta\rangle\leq K\}$
is a compact set in $\cmeasures$, we shall focus on 
controlling $(\langle 1,\eta^N_t\rangle)_{t\geq 0}$.
the key is that our Assumptions~\ref{def:model_setup}
are precisely chosen to guarantee boundedness
of the net per-capita reproduction rate.

\begin{lemma}
    \label{lem:eta_f_bound}
    Under Assumptions~\ref{def:model_setup},
    for all $f \in C^2_b(\IR^d)$ there exists a $C_f < \infty$, independent of $N$,
    such that
    \begin{equation}
\label{eta_f_bound}
        \IE[\langle f, \eta^N_t \rangle]
        \le
        \IE[\langle f, \eta^N_0 \rangle]
        e^{C_f t}
    \end{equation}
    for all $N \geq 1$.
\end{lemma}

\begin{proof}
    Consider the semimartingale decomposition of Equation \eqref{eqn:eta_martingale}:
    \begin{align} \label{eqn:eta_f_mgale_decomp}
        \langle f, \eta^N_t \rangle
        &=
        \langle f, \eta^N_0 \rangle
        + \int_0^t \int_{\IR^d} \left\{
            \gamma(x, \eta^N_s) B_f(x, \eta^N_s)
            + f(x) F(x, \eta^N_s)
            \right\} \eta^N_s(dx) ds
        + M^N_t(f) ,
    \end{align}
    where $M^N_t(f)$ is a martingale and, for the moment, we are defining
    \begin{equation*}
        B_f(x, \eta) = \int_{\IR^d} \theta (f(y) r(y, \eta) - f(x) r(x, \eta)) q_\theta(x, dy) .
    \end{equation*}

We need a bound on $\gamma(x,\eta)B_f(x,\eta)+F$. First suppose that
Condition~\ref{control through r} of  
Assumptions~\ref{def:model_setup} is satisfied.
We write
    \begin{align*}
        f(y)r\big(y,\rho_r*\eta(y)\big) - f(x)r\big(x,\rho_r*\eta(x)\big)
        &=
        \sum_i (y_i - x_i) \partial_{x_i} \Big(f(x)r\big(x,\rho_r*\eta(x)\big)\Big) 
        \\ &\qquad {}
        +
        \frac{1}{2} \sum_{ij} (y_i - x_i)(y_j - x_j)
            \partial_{x_i x_j} \Big(f(z)r\big(\smooth{r}\eta(z)\big)\Big) 
    \end{align*}
    where $z = t x + (1-t) y$ for some $0 \le t \le 1$. Condition~\ref{control through r} 
gives uniform bounds on the derivatives of $r(x, \rho_r*\eta(x))$ 
in this expression and so, provided $f$ also has uniformly bounded first
and second derivatives, we have a bound of the form
\begin{multline*}
\Big| f(y)r\big(y,\rho_r*\eta(y)\big) - f(x)r\big(x,\rho_r*\eta(x)\big) \Big|
\\
\leq
        K_1 \left|
            \theta \int_{\IR^d} \sum_i (y_i - x_i) q_\theta(x, dy)
        \right|
        +
        K_2 \left|
            \theta \int_{\IR^d} \sum_{ij} (y_i - x_i) (y_j - x_j)  q_\theta(x, dy)
        \right| 
\end{multline*}
for suitable constants $K_1$, $K_2$.
This is bounded uniformly because
$q_\theta(x, dy)$ is the density of a Gaussian with mean $\meanq(x)/\theta$
and covariance $\covq(x)/\theta$, and both $\meanq(x)$ and $\covq(x)$
are bounded uniformly in $x$.

Now suppose that instead
Condition~\ref{control through gamma} of Assumptions~\ref{def:model_setup}
is satisfied.
First note that,  
    \begin{align} \label{eqn:q_bound_split}
    &
        \left| \theta 
            \int_{\IR^n} 
                \left\{
                    f(y) r\big(y,\smooth{r}\eta(y)\big)
                    -
                    f(x) r\big(x, \smooth{r}\eta(x)\big)
                \right\}
            q_{\theta}(x,dy)
        \right|
    \\ &\qquad {} \leq
        \left| \theta 
            \int_{\IR^n} 
                \left\{
                    f(y) r\big(y, \smooth{r}\eta(y)\big)
                    -
                    f(x) r\big(x,\smooth{r}\eta(y)\big)
                \right\}
            q_{\theta}(x,dy)
        \right|
    \\ &\qquad \qquad {}  +
        \left| \theta 
            \int_{\IR^n} 
                \left\{
                    f(x) r\big(x,\smooth{r}\eta(y)\big)
                    -
                    f(x) r\big(x,\smooth{r}\eta(x)\big)
                \right\}
            q_{\theta}(x,dy) 
        \right| .
    \end{align}

 Writing
    $K_3 = \sup_{x,m} \max_i |\partial_{x_i} f(x)r(x, m)|$
    and 
    $K_4 = \sup_{x,m} \max_{i,j} |\partial_{x_i x_j} f(x)r(x, m)|$, the first term
is bounded exactly as above. For the second, 
    \begin{multline*}
        r(x, \smooth{r}\eta(y)) - r(x, \smooth{r}\eta(x))
        \\
=
        (\smooth{r}\eta(y) - \smooth{r}\eta(x)) r'(x,\smooth{r}\eta(x))
        + \frac{1}{2} (\smooth{r}\eta(y) - \smooth{r}\eta(x))^2 r''(x, \overline{m}),
    \end{multline*}
    where $\overline{m}= t'\smooth{r}\eta(x) +(1-t') \smooth{r}\eta(y)$ for some $0\leq t'\leq 1$, and
    we have used $r'$ and $r''$ to denote the first and second derivatives of $r(x,m)$
with respect to the second argument.
    So, writing $K_5=\|f\|_\infty\|r'\|_\infty$ and $K_6=\|f\|_\infty\|r''\|_\infty$, in
this case 
    the second term in~\eqref{eqn:q_bound_split} is bounded by
    \begin{align*}
       K_5
        \left|
            \theta \int_{\IR^d}
                (\smooth{r}\eta(y) - \smooth{r}\eta(x))
            q_\theta(x, dy)
        \right|
        +
       K_6
        \left|
            \theta \int_{\IR^d}
                (\smooth{r}\eta(y) - \smooth{r}\eta(x))^2
            q_\theta(x, dy)
        \right| .
    \end{align*}
Under Condition~\ref{control through gamma} of 
Assumptions~\ref{def:model_setup}, the right hand side is bounded by 
a constant times $\smooth{\gamma}\eta(x)+(\smooth{\gamma}\eta(x))^2$ and 
$m^2\gamma(m)$ is bounded. We have shown that $B_f$ is bounded, and so recalling that
$F$ is bounded above, we can now conclude that
under Assumptions~\ref{def:model_setup}
    $\gamma(x, \eta) B_f(x, \eta) + f(x) F(x, \eta) \le C_f $ for some $C_f$.

Taking expectations in~(\ref{eqn:eta_f_mgale_decomp}),
    \begin{align}
\label{bound on intfdeta}
        \IE\left[ \langle f, \eta^N_t \rangle \right]
        &\le
        \IE\left[ \langle f, \eta^N_0 \rangle \right]
        + C_f \int_0^t \IE\left[ \langle 1, \eta^N_s \rangle \right] dt .
    \end{align}
The bound~(\ref{eta_f_bound}) then follows by first applying Gronwall's
inequality in the case $f=1$, and then, after substitution of 
the resulting bound into the
expression above, a second 
application of Gronwall's inequality.
\end{proof}

With a bound on per-capita net growth rate in hand,
bounds on the expectation of the supremum of 
the total population size over a finite time interval also follow easily.

\begin{lemma}[Compact containment for the population process]
    \label{lem:eta_compact_containment}
    Under Assumptions~\ref{def:model_setup},
    if $\theta/N \to \alpha$ and $\eta^{N}_0 \to \eta_0$ weakly as $N \to \infty$,
    then for all $T>0$ and $N > 0$, there exists some constant $C_T$,
    such that
    \begin{align}
        \label{eqn:eta_mass_bound}
        \IE\left[
            \sup_{0 \le t \le T}
            \langle 1, \eta^{N}_t \rangle
        \right]
        \le C_T .
    \end{align}
    In particular, for any $\delta > 0$, there exists $K_{\delta}>0$ such that
    \begin{equation}
    \limsup_{N \to \infty}
        \IP \left\{ \sup_{s \in [0,T]}
            \langle 1 ,\eta^{N}_{s}\rangle
            > K_\delta \right\}
        \leq \frac{C_T}{K_{\delta}}
        < \delta.
    \end{equation}
\end{lemma}

\begin{proof}
    First note that by Lemma~\ref{lem:eta_f_bound},
    $\IE[\langle 1, \eta^N_t \rangle] \le \IE[\langle 1, \eta^N_0 \rangle] e^{Ct}$
    for some $C$.
    Now, let $M^*_t(f) = \sup_{0 \le s \le t} M_t(f)$,
    and as before $[M(f)]_t$ the quadratic variation of $M_t(f)$,
    so that by the Burkholder-Davis-Gundy inequality, there is a $C'$ so that
    \begin{align*}
        \IE\left[ M^*_t(1) \right]
        &\le
        C' \IE\left[ \sqrt{[M(1)]_t} \right]
        \\ &\le
        C'\left( 1 + \IE\left[ [M(1)]_t \right] \right)
        \\ &=
        C'\left( 1 + \frac{\theta}{N} \IE\left[
            \int_0^t
            \int_{\IR^d} \left\{
                \gamma(x, \eta^N_s)
                \int_{\IR^d} r(y, \eta^N_s) q_\theta(x, dy)
                + \mu_\theta(x, \eta^N_s)
            \right\} \eta_s^N(dx)
            dt
            \right] \right)
        \\ &=
        C'\left( 1 + \IE\left[
            \int_0^t
            \int_{\IR^d} \left\{
                \frac{2\theta}{N} \gamma(x, \eta^N_s) r(x, \eta^N_s)
		+ \frac{\gamma(x,\eta_s^N)}{N} B_1(x, \eta^N_s)-\frac{1}{N}F(x,\eta^N_s)
            \right\} \eta_s^N(dx)
            dt
            \right] \right) .
    \end{align*}

	We have not assumed that $F$ is bounded below, but to see that the 
	term involving $-F$ does not cause us problems, we rearrange
	equation~(\ref{eqn:eta_f_mgale_decomp}) 
	with $f=1$ to see that
\[
\IE\left[\int_0^t \Big\langle -\frac{1}{N}F(x, \eta^N_s), \eta^N_s(dx)
	\Big\rangle\right]
	=\IE[\langle 1, \eta^N_0\rangle] -\IE[\langle 1,\eta^N_t\rangle]
	+\IE\left[\int_0^t\Big\langle %int_{\IR^d}
\gamma(x,\eta_s^N)
	B_1(x, \eta_s^N),\eta_s^N(dx)\Big\rangle ds\right],
\]
which is bounded since $\gamma (x,\eta)$ and $B_1(x,\eta)$ are both bounded and 
$\langle 1,\eta_t\rangle\geq 0$. 
    Since $\theta/N \to \alpha < \infty$,
    combining constants and again using Gronwall's inequality, we obtain that
    \begin{align*}
        \IE\left[ M^*_t(1) \right]
        % &\le
        % C'\left( 1 + \IE\left[
        %     \int_0^t
        %     \int_{\IR^d} \left\{ C'' \right\} \eta_s^N(dx)
        %     dt
        %     \right] \right) \\
        &\le
        C' + C'' \IE[ \langle 1, \eta_0^N \rangle ] e^{tC} .
    \end{align*}
    Taking suprema and expectations on both sides of equation~\eqref{eqn:eta_f_mgale_decomp},
    then again using the fact that $\gamma(x, \eta) B_1(x, \eta) + F(x, \eta) \le C$,
    \begin{align*}
        \IE\left[\sup_{0 \le s \le t} \langle 1, \eta^N_s \rangle \right]
        &\le
        \IE[\langle 1, \eta^N_0 \rangle]
        + \IE\left[
            \sup_{0 \le s \le t}
            \int_0^t \int_{\IR^d} \left\{
                \gamma(x, \eta^N_s)
                B_1(x, \eta^N_s)
                + F(x, \eta^N_s)
            \right\} \eta^N_s(dx) ds
        \right]
        \\
 & + \IE[M^*_t(1)] 
        \\
        &\le
        \IE[\langle 1, \eta^N_0 \rangle]
        + C \IE\left[
            \int_0^t \sup_{0 \le s \le t} \langle 1, \eta^N_s \rangle ds
        \right]
        + C' + C'' \IE[ \langle 1, \eta_0^N \rangle ] e^{tC} .
    \end{align*}
    Once again applying Gronwall's inequality,
    \begin{align*}
        \IE\left[\sup_{0 \le s \le t} \langle 1, \eta^N_s \rangle \right]
        &\le
        C''' 
        \left(1+ \IE[\langle 1, \eta^N_0 \rangle]\right) e^{2tC} .
    \end{align*}
    For any $T$,
    the inequality on the right is bounded above by a constant $C(T)$ independent of $N$.
    As a result, for any $K > 0$,
    \begin{equation*}
    \limsup_{N \to \infty}
        \IP\left[ \sup_{0 \le s \le T} \langle 1, \eta^{N}_{s} \rangle \geq K \right]
        \leq
        \frac{C(T)}{K}.
    \end{equation*}
\end{proof}

\subsubsection{Tightness}

Our next task is to show tightness of 
$(\langle f,\eta_t^N\rangle)_{t\geq 0}$ for 
$f\in C_b^\infty(\overline{\IR}^d)$. 

\begin{lemma}[Tightness of $(\langle f, \eta^{N}_t \rangle )_{t>0}$]
    \label{lem:eta_projections_tightness}
	For each $f \in C^{\infty}_{b}(\overline{\IR}^d)$, 
the collection of processes
$(\langle f, \eta^{N}_t \rangle)_{t \geq 0}$
for $N = 1, 2, \ldots$ is tight
as a sequence of c\`adl\`ag, real-valued processes.
\end{lemma}
\begin{proof}
The Aldous-Rebolledo criterion (found in the form we use here in \cite{rebolledo:1980}),
applied to the semimartingale representation of $\langle f, \eta^N_t\rangle$
of equation~\eqref{eqn:eta_f_mgale_decomp}, tells us
that it suffices to show that for each $T>0$, for each fixed 
$0\leq t\leq T$, the sequence 
$\{\langle f, \eta^N_t \rangle\}_{N \ge 1}$ is tight, and that
for any sequence of stopping times $\tau_N$ bounded by $T$,
and for each $\nu > 0$, there exist $\delta > 0$ and $N_0 > 0$ such that 
\begin{gather}
        \label{eqn:eta_projections_goal1}
    \sup_{N > N_0}
    \sup_{t \in [0, \delta]}
    \IP\left\{\left|
            \int_\tau^{\tau + t}
            \int_{\IR^d}
            \left\{
                \gamma(x, \eta^N_s) B_f(x, \eta^N_s)
                + f(x) F(x, \eta^N_s)
            \right\} 
            \eta^N_s(dx)
            ds
        \right|> \nu \right\}
        < \nu ,
    \\ \text{and} \qquad
        \label{eqn:eta_projections_goal2}
    \sup_{N > N_0}
    \sup_{t \in [0, \delta]}
    \IP\left\{\big|
        [M^{N}(f)]_{\tau + t} 
            - [M^{N}(f)]_\tau \big|
        > \nu
    \right\}
    < \nu.
\end{gather}
Tightness of $\langle f, \eta^N_t\rangle$ for fixed $t$
follows from Lemma~\ref{lem:eta_f_bound} and Markov's inequality,
so we focus on the remaining conditions.

The proof of Lemma~\ref{lem:eta_f_bound}
provides a uniform bound on $\gamma B_f$, but we only know
that $F$ is bounded above. However, since, by 
assumption, for each fixed value of
$m$, $\sup_{k\leq m}|F(x,k)|$ is uniformly bounded as a function of $x$, 
noting that $\smooth{F}\eta\leq
\langle 1,\eta\rangle\|\rho_F\|_\infty$,
and using Lemma~\ref{lem:eta_compact_containment} 
to choose $N_0$ and $K$ such that
$$\IP\left[\sup_{0\leq s\leq T}\langle 1, \eta_s^N\rangle\geq K\right]<\nu,$$
we now choose $\delta_1$ so that
$$\sup\big\{
\sup_{x}|F(x,k)|
: k\leq K\|\rho_F\|_\infty \big\}\delta_1
<\nu/2,
\qquad
\sup_{x,\eta}\gamma(x,\eta)B_f(\,\eta)\delta_1<\nu/2,$$
and~(\ref{eqn:eta_projections_goal1}) is satisfied.

Similarly,
\begin{align*}
    &
 \big|   [M^{N}(f)]_{\tau + t} 
        - [M^{N}(f)]_\tau \big|
    \\ & \qquad =
  \Big|  \int_{\tau}^{\tau + t}
        \frac{\theta}{N}
        \int_{\IR^d}
        \left\{
            \gamma(x, \eta^N_s)
            \int_{\IR^d} f^2(y) r(y, \eta^N_s) q_\theta(x, dy)
            +
            \mu_\theta(x, \eta^N_s) f^2(x)
        \right\}
        \eta^N_s(dx)
    ds \Big|
    \\ & \qquad =
   \Big| \int_{\tau}^{\tau + t}
        \frac{\theta}{N}
        \int_{\IR^d}
        \left\{
            \gamma(x, \eta^N_s)
            \left(
                2 f^2(x) r(x, \eta^N_s)
                +
                B_{f^2}(x, \eta^N_s)
            \right)
            -
            f^2(x) \frac{F(x, \eta^N_s)}{ \theta}
        \right\}
        \eta^N_s(dx)
    ds\Big| ,
\end{align*}
and so using the fact that $\theta/N \to \alpha < \infty$,
an argument entirely analogous to 
that for \eqref{eqn:eta_projections_goal1}
yields a $\delta_2$ for which \eqref{eqn:eta_projections_goal2} is
satsified. Taking $\delta=\min\{\delta_1,\delta_2\}$, the result follows.

\end{proof}

% % % % % % % % %

We collect the results of the last two subsections into a proposition.
\begin{proposition}[Tightness of $(\eta^{N}_t)_{t \geq 0}$]
\label{tightness in one point compactification}
    The collection of measure-valued processes 
    $\{(\eta^{N}_t)_{t \geq 0}: N \geq 1\}$
    is tight in $\mathcal{D}_{[0,\infty)}(\cmeasures)$.
\end{proposition}

\begin{proof}
    Theorem 3.9.1~in \cite{ethier/kurtz:1986}
    says that if the collection of $E$-valued processes
    satisfies a compact containment condition
    (for any $\epsilon > 0$ and $T>0$, there is a compact set
    such that the processes stay within that set up to time $T$ with probability at least $1-\epsilon$),
    then the collection is relatively compact (which is equivalent to tightness
since we are working on a Polish space)
    if and only if
    $\{(f(\eta^{N}_t))_{t \geq 0}: N \geq 1\}$ is relatively compact
    for all $f$ in a dense subset of $C_b(E)$
    under the topology of uniform convergence in compact sets.

Since $\{ \nu: \langle 1, \nu \rangle \leq K \} $ is compact in $\cmeasures$,
    Lemma~\ref{lem:eta_compact_containment}
    gives compact containment.
    Lemma \ref{lem:eta_projections_tightness}
    shows that the real-valued processes $\langle f, \eta^{N}_t \rangle$
    are relatively compact for all 
$f \in \mathcal{C}^{\infty}_{b}(\overline{\IR}^d)$.
    Since by the Stone-Weierstrass theorem,
    the algebra of finite sums and products of terms of this form
    is dense in the space of bounded continuous functions on $\cmeasures$,
    we have relative compactness in $\mathcal{D}_{[0,\infty)}(\cmeasures)$.
\end{proof}


% % % % % % % % % % % % %
\subsubsection{Characterisation of the limit points}
    \label{sec:population_generators_proofs}

We wish to characterise the limit points of
$\{(\eta^{N}_t)_{t>0}\}_{N\geq 1}$ as solutions to a martingale problem 
with generator $\Pgen^{\infty}$ which we now identify. Most of the work
was done in Section~\ref{sec:heuristics}.

\begin{definition}
    \label{def: MP definition of limit}
For $G \in \mathcal{C}^{\infty}(\IR)$ with $\|G'''\|_\infty<\infty$,
and 
$f \in \mathcal{C}_{b}^\infty(\overline{\IR}^d)$, define the 
function $G_f$ by 
$G_f(\eta):=G (\langle f, \eta \rangle)$.
Let $\Pgen^\infty$ be the generator
given by
\begin{equation}
    \label{eq: Limit Generator Definition}
\begin{aligned}
\Pgen^{\infty} G_f(\eta):=& G'(\langle f, \eta \rangle)
                   %\times 
\left\{
                   \big\langle
                        \gamma(x, \smooth{\gamma} \eta(x))
                            \mathcal{B}\left(
                            f(\cdot) r(\cdot, \smooth{r} \eta(\cdot))
                            \right)(x)
                    +
                    f(x)
                        F(x, \smooth{F} \eta(x)),
                        \eta(dx)
                    \big\rangle
                   \right\}\\
                &+ \frac{\alpha}{2} G''(\langle f, \eta \rangle)
                  %\times 
\left\{
                  \big\langle
                    2\gamma\left( x, \eta \right)
                    r\left(x,\eta \right)
                    %+\mu\left(x,\eta \right)
                    f^2(x),
                    \eta (dx)
                    \big\rangle 
                  \right\}.
\end{aligned}    
\end{equation}
We say that $(\eta^{\infty}_t)_{t \geq 0}$ is a solution 
to the Martingale Problem $(\Pgen^{\infty}, \eta_0)$ if 
for all such test functions %$G \in \mathcal{C}^{2}(\IR)$
%and $f \in \mathcal{C}_{b}(\IR^d)$,
$$M_t:=G_f(\eta^{\infty}_t)-G_f(\eta_0)
-\int_{0}^{t}\Pgen^{\infty}G_f(\eta^{\infty}_s)ds$$
is a martingale. 
\end{definition}


\begin{lemma}[Characterisation of limit points]
    \label{lem:limit_mgale}
Suppose that $(\eta^{N}_0)_{N\geq 1}$ converges weakly to $\eta_0$ as 
$N\to\infty$.
Then any limit point of $\{(\eta^{N}_t)_{t \geq 0}\}_{N\geq 1}$ 
in $\mathcal{D}_{[0,\infty)}(\cmeasures)$ %(\mathcal{M}_f(\IR^d))$
is a solution to the martingale problem for 
$(\Pgen^\infty, \eta_0)$.
\end{lemma}
\begin{proof}
We use Theorem 4.8.2 in \cite{ethier/kurtz:1986}.
First observe that the set of functions
$\{G_f(\eta):= G(\langle f, \eta \rangle ),~
G \in \mathcal{C}^{\infty}(\IR), \|G'''\|_\infty<\infty,~
f \in \mathcal{C}_{b}^\infty(\overline{\IR}^d)\}$
is separating 
on $\mathcal{M}_F(\overline{\IR}^d)$.
Therefore, it suffices to show that for any $t>0$,
\begin{equation}
    \label{eq: Convergence Condition}
\lim_{N \to \infty}
\mathbb{E}\left[
\left(
G_f(\eta^{N}_{t+\tau})-G_f(\eta^{N}_t)
-\int_{t}^{t+\tau}\Pgen^{\infty}G_f(\eta^{N}_s)ds
\right)
\prod_{i=1}^{k} h_i(\eta^{N}_{t_i})
\right]=0
\end{equation}
for all $k\geq 0$, $0\leq t_1<t_2<...,t_k \leq t < t+\tau$,
and $h_1,...,h_k \in \mathcal{C}_{b}(\cmeasures)$.

Since $(\eta^N_t)_{t\geq 0}$ is Markov, the tower property gives that,
for each choice of $G_f$ and for each $N$,
\begin{equation}
    \label{eq: Prelimit MP Application}
%\lim_{N \to \infty}
\mathbb{E}\left[
\left(
G_f(\eta^{N}_{t+\tau})-G_f(\eta^{N}_t)
-\int_{t}^{t+\tau}\Pgen^{N}G_f(\eta^{N}_s)ds
\right)
\prod_{i=1}^{k}h_i(\eta^{N}_{t_i})
\right]=0
\end{equation}
for all $k\geq 0$, $0\leq t_1<t_2<...,t_k \leq t < t+\tau$,
and $h_1,...,h_k \in \mathcal{C}_{b}(\cmeasures)$.
%
Therefore, it suffices to show that 
\begin{equation}
    \label{eq: Convergence of Spatial Generator}
\lim_{N \to \infty}
\mathbb{E}\left[
\int_{t}^{t+\tau}
\left|
\Pgen^{N}G_f(\eta^{N}_s)
-\Pgen^{\infty}G_f(\eta^{N}_s)
\right|
ds
\prod_{i=1}^{k}h_i(\eta^{N}_{t_i})
\right]=0,
\end{equation}
and again using the tower property, since the functions $h_i$ are
bounded, this will follow if
\begin{equation}
\lim_{N \to \infty}
\mathbb{E}\left[\left.
\int_{t}^{t+\tau}
\left|
\Pgen^{N}G_f(\eta^{N}_s)
-\Pgen^{\infty}G_f(\eta^{N}_s)
\right|
ds\right| {\cal F}_t\right]=0
\end{equation}
(where $\{{\cal F}_t\}_{t\geq 0}$ is the natural $\sigma$-field).

We rewrite $\Pgen^NG_f(\eta^N_s)$ using a 
Taylor series expansion up to third order for
$G\big(\langle f,\eta\rangle\pm f(y)/N\big)$ around
$G(\langle f, \eta \rangle )$.
As in Section~\ref{sec:heuristics} (except that now we are more explicit about
the error term), we find
\begin{equation} 
    \label{eq: Pre-Limit Generator Expanded}
\begin{aligned}
\Pgen^{N} G_f(\eta):=& G'(\langle f, \eta \rangle)\int_{\IR^d}  
\theta\Big\langle\int_{\IR^d} \big\{f(y)\gamma(x, \eta) r(y,\eta) 
-f(x)\mu_\theta(x, \eta)\big\}q_{\theta}(x,dy), \eta(dx)\Big\rangle\\
&+\frac{1}{2}\frac{\theta}{N}G''(\langle f, \eta \rangle)\int_{\IR^d} 
 \Big\langle\gamma(x, \eta)\int_{\IR^d} f^2(y) r(y,\eta)q_{\theta}(x,dy)+f^2(x)
\mu_\theta(x, \eta), \eta(dx)\Big\rangle\\
&+\frac{1}{6}\frac{\theta}{N^2}\Big\langle \big\{
G'''(w) \gamma(x, \eta)\int_{\IR^d} f^3(y) r(y,\eta) 
q_{\theta}(x,dy)
-G'''(v)f^3(x)\mu_\theta(x, \eta)\big\}, \eta(dx)\Big\rangle
\end{aligned}    
\end{equation}
%\normalsize
for some $w,v \in [\langle f,\eta \rangle - \|f\|_\infty/N, 
\langle f,\eta \rangle + \|f\|_\infty/N]$.

Combining with equation~\eqref{limit of mean measure equation}, and the fact
that $\mu_\theta(x,\eta) \to r(x,\eta)\gamma(x,\eta)$ as $\theta\to\infty$, we have
pointwise convergence:
\begin{equation}
\lim_{N\to \infty} |\mathcal{P}^{N}G(\langle f, \eta \rangle) 
- \mathcal{P}^{\infty}G(\langle f, \eta \rangle)| = 0,   
\end{equation}
To conclude convergence of the expectation, 
we would like to apply the Dominated Convergence 
Theorem in~(\ref{eq: Convergence of Spatial Generator}).
Recall that $f$ and $G$ and their derivatives are bounded, $\gamma(x,\eta)$ is
bounded, and, in the notation of Lemma~\ref{lem:eta_f_bound}
$\gamma(x,\eta) B_f(x,\eta)$ is bounded. Moreover $\theta/N^2\to 0$ as $N\to\infty$. Thus we can 
dominate 
$\left|
\Pgen^{N}G_f(\eta^{N}_s)
-\Pgen^{\infty}G_f(\eta^{N}_s)
\right|$ by 
a constant multiple of
$\theta \big\langle |r(x,\eta^N_s)\gamma(x,\eta^N_s)-\mu_\theta(x,\eta^N_s)|,
\eta^N_s(dx)\big\rangle$, 
which we  
recognise as $\langle |F(x,\eta^N_s)|,\eta^N_s(dx)\rangle$.
Since $F$ is bounded above, there is a constant $K$ such that $|F|\leq K-F$ so
that exactly as in the proof of 
Lemma~\ref{lem:eta_compact_containment},
we can check that
\[
\IE\Big[\left.\int_t^{t+\tau}\big\langle |F(x,\eta^N_s)|,\eta^N_s(dx)\big\rangle ds
\right| {\cal F}_t\Big]<\infty,
\]
which concludes our proof.
\end{proof}

The last step in the proof of Theorem~\ref{thm:nonlocal_convergence}
is to check that
any limit point $(\eta_t)_{t\geq 0}$ of $\{(\eta_t)_{t\geq 0}\}_{N\geq 1}$
actually takes its values in $\measures$, that is ``no mass has escaped to infinity''.

\begin{lemma}
\label{no mass at infty}
Let $(\eta_t)_{t\geq 0}$ be a limit point of $\{(\eta_t)_{t\geq 0}\}_{N\geq 1}$. 
Given $\delta>0$, 
\[
\IP\big[\eta_t\big(\{\|x\|>R\}\big)>\delta\big]\to 0\qquad\mbox{ as }R\to\infty.
\]
\end{lemma}
\begin{proof}[Sketch]
To avoid the idea being buried in notation, we sketch the proof in the 
special case of $d=1$ and $\DG=\Delta$.
For any $f\in C_b^\infty(\IR)$,
\begin{multline}
\label{special case first moment equation}
\IE[\langle f,\eta_t\rangle] 
=\IE[\langle f,\eta_0\rangle]
\\
+\IE\Big[\int_0^t\Big\langle\gamma(x, \smooth{\gamma}\eta_s(x))
\Delta \big(f(\cdot)r(\cdot,\smooth{r}\eta_s(\cdot))\big)(x)
+F(x,\smooth{F}\eta_s(x)) f(x),
\eta_s(dx)\Big\rangle ds\Big]
\end{multline} 
We take a sequence of test functions $f_n$ increasing to
$\exp(\lambda x)$, with $\lambda>0$. Having passed
to the limit,
substituted in~(\ref{special case first moment equation}) and 
expanded the term involving the Laplacian,we obtain
\begin{multline}
\label{exponential moment}
\IE[\langle \exp(\lambda x),\eta_t\rangle]
=\IE[\langle \exp(\lambda x),\eta_0\rangle]
+\IE\Big[\int_0^t\Big\langle\gamma(x, \smooth{\gamma}\eta_s(x))
\exp(\lambda x)
\Big\{\lambda^2 r(x, \smooth{r}\eta_s(x))
\\
+2\lambda\partial_x\big(r(\cdot,\smooth{r}\eta_s(\cdot)\big)(x)
+\partial_{xx} r(\cdot,\smooth{r}\eta_s(\cdot)\big)(x)
+F(x,\smooth{F}\eta_s(x))\Big\} ,
\eta_s(dx)\Big\rangle ds\Big].
\end{multline} 
Evidently, in much the same way as in the proof of
Lemma~\ref{lem:eta_f_bound},
our conditions on $r$ and $F$ ensure
that the integrand on the right hand side 
of~(\ref{exponential moment})
is bounded by $C\langle\exp(\lambda x),\eta_s(dx)\rangle$ for some constant
$C$. Gronwall's inequality then implies that 
$\IE[\langle\exp(\lambda x),\eta_t(dx)\rangle]$ 
is uniformly bounded for $t\in [0,T]$, and an application of Markov's inequality
tells us that for any $\delta >0$,
$\IP[\eta_t([R,\infty])>\delta]\to 0$ as $R\to\infty$. A symmetric argument
yields the corresponding bound for $\IP[\eta_t([-\infty, -R])>\delta$ and the 
result is proved.
\end{proof}

% % % % % % % % % % % % %
\subsection{Convergence of some nonlocal equations to classical PDEs}

It is natural to conjecture that when the limit of the rescaled 
population process that we obtained in the previous section solves a nonlocal
PDE, then if we further scale the kernels 
$\rho_r$, $\rho_\gamma$, and $\rho_F$ by setting 
$\rho^\epsilon(\cdot)=\rho(\cdot/\epsilon)/\epsilon$, then as $\epsilon\to 0$, the 
corresponding solutions should converge to a limiting
population density that solves the corresponding ``classical'' PDE. We verify
this in two examples; the first in which the nonlocal equation is a reaction
diffusion equation with the ``nonlocality'' only appearing in the 
reaction term, and the second in which the nonlocal PDE is a special 
case of a nonlinear porous medium equation. These, in particular,
capture the examples that we explored in 
Section~\ref{ancestral lineages for travelling waves}.

\subsubsection{Reaction diffusion equation limits}
\label{two-step convergence to FKPP}

In this subsection we prove Proposition~\ref{prop:nonlocal_to_local}. {\em The conditions of the
proposition are in force throughout this subsection.} 
The proof rests on a Feynman-Kac representation.
We write $(Z_t)_{t\geq 0}$ for a diffusion with generator $\DG^*$ and denotes 
its transition density by $f_t(x,y)$. The first step is a regularity
result for this density.

\begin{lemma} \label{regularityForX1}
Fix $T>0$. There exists a constant $K= K(T)>0$ such that, for any 
$x, y, z \in \mathbb{R}^d$ and $t \in [0,T]$,
\begin{equation} \label{eq:boundDensityXt}
\int |f_t(x,z)-f_t(y,z)| dz \leq \frac{|x-y|}{\sqrt{t}} K.
\end{equation}
\end{lemma}
\begin{proof}
We first use the Intermediate Value Theorem to obtain the bound
\begin{align*}
\int |f_t(x,z)-f_t(y,z)| dz & \leq \int |x-y| |\nabla f_t(w,z)| dz
\end{align*}
where $\nabla$ acts on the first coordinate only and $w$ is in the line 
segment $[x,y]$ joining $x$ to $y$. 
Under our assumptions on $b$ and $C$, equation~(1.3) 
of~\cite{sheu:1991}, gives existence of constants 
$\lambda=\lambda(T)>0$ and $K$ such that,
\[ |\nabla f_t(w,z)|  \leq  \frac{K }{\sqrt{t}}p_{ \lambda  t}(w,z), \]
where $p_s(x,y)$ is the Brownian transition density.
Hence, 
\begin{align*}
\int |f_t(x,z)-f_t(y,z)| dz & \leq K 
\frac{|x-y|}{\sqrt{t}} \int p_{\lambda  t}(w,z)dz = 
K  \frac{|x-y|}{\sqrt{t}},
\end{align*}
which proves the result.
\end{proof}
\begin{lemma} \label{regularityForX2}
Fix $T>0$. Let $x,y \in \mathbb{R}^d$, $t \in [0,T]$. Let us denote by 
$(Z_t^y)_{t\geq 0}$ and $(Z_t^x)_{t\geq 0}$ independent copies of the 
diffusion $(Z_t)_{t\geq 0}$ starting 
from $y$ and $x$ respectively. There exists a constant $K=K(T)>0$ such that, 
\[ \IE[\|Z_t^y-Z_t^x\|] \leq K(\sqrt{t} + \|y-x\|). \]
\end{lemma}
\begin{proof}
First we write, 
\[ \IE[\|Z_t^y-Z_t^x\|]  = \int \int \|u-v\| f_t(y,u) f_t(x,v) du dv . \]
Under our regularity assumptions on $C$, $b$, using Equation~(1.2)
of~\cite{sheu:1991}, there exist constants $K$, $\lambda =\lambda (T)>0$ 
for which,
\[ f_t(y,u) \leq K  p_{\lambda  t}(y,u). \]
It then follows that,
\begin{align}
  \IE[\|Z_t^y-Z_t^x\|]  \leq \int \int \|u-v\| K^2 p_{\lambda  t}(y,u) 
p_{\lambda  t}(x,v) dv du = K^2 \IE[\|B_{\lambda  t}^y-B_{\lambda  t}^x\|], 
\label{eq:ExpDif1}
\end{align}
where $(B_t^y)_{t\geq 0}$ and $(B_t^x)_{t\geq 0}$ are 
independent Brownian motions starting at $y$ and $x$ respectively. 
Using the triangle inequality, and writing $(B^0_t)_{t\geq 0}$ for a 
Brownian motion started from the origin,
\begin{equation}
\IE[\|B_{\lambda  t}^y-B_{\lambda  t}^x\|]\leq \|y-x\|+\IE[\|B^0_{2\lambda t}\|] 
%&= \sqrt{ d \lambda t } \sqrt{2/\pi} e^{-(y-x)^2/(2d\lambda t )} + 
%(y-x) (1-2\Phi(-(y-x)/(\lambda  d t))) \nonumber \\ 
%& \leq \sqrt{2d \lambda  /\pi}  \sqrt{t} + 
\leq \|y-x\|+C\sqrt{t}. \label{eq:ExpDif2}
\end{equation}
Substituting (\ref{eq:ExpDif2}) in (\ref{eq:ExpDif1}) gives the result.
\end{proof}

We use the representations:
\begin{align}
\varphi_t(x) &= \IE_x\Big[\varphi_0(Z_t) 
+ \int_0^t \varphi_s(Z_s)F(\varphi_s(Z_s)) ds\Big], \label{FK:varphi} \\
\varphi^\epsilon_t(x) &= \IE_x\Big[\varphi_0(Z_t) 
+ \int_0^t \varphi^\epsilon_s(Z_s)F(\rho^\epsilon_F*\varphi^\epsilon_s(Z_s)) ds\Big],
\end{align}
from which
\begin{equation} 
\varphi_t(x) - \varphi^\epsilon_t(x) = 
\IE_x\left[ \int_0^t \Big(\varphi_s(Z_s)F\big(\varphi_s(Z_s)\big)
-\varphi^\epsilon_s(Z_s)F\big(\rho_\epsilon*\varphi^\epsilon_s(Z_s)\big)\Big) ds \right]. 
\label{FK:diferencesVarphis}
\end{equation}
The key to our proof of Proposition~\ref{prop:nonlocal_to_local}
will be to replace $F(\varphi_s(Z_s))$ by $F(\rho^\epsilon_F*\varphi_s(Z_s))$ 
in this expression. 
We achieve this through three lemmas.

First we need a uniform bound on $\varphi$ and $\varphi^\epsilon$.  
\begin{lemma} 
\label{BoundednessVarphis}
For any $T>0$ there exists $M = M(T, \Vert \varphi_0 \Vert) >0$ such that, 
for all $0 \leq t \leq T$:
\[ \max\{ \| \varphi_t(\cdot) \|_\infty, \| \varphi^\epsilon_t(\cdot) \|_\infty\} 
< M.
 \]
\end{lemma}
\begin{proof}
Using that $\varphi_0$ and $F$ are bounded above, from the
representation~(\ref{FK:varphi}), we have
\begin{align*}
\varphi_t(x) \leq \| \varphi_0 \|_\infty + 
K \IE\Big[\int_0^t \varphi_s(Z_s) ds\Big].
\end{align*}
In particular, since $\varphi_s$ is non-negative,
\[ \| \varphi_t(\cdot) \|_\infty \leq \| \varphi_0 \| 
+  K \int_0^t \| \varphi_s(\cdot) \|_\infty ds, \]
so, by Gronwall's inequality,
\[ \| \varphi_t(\cdot) \|_\infty \leq \| \varphi_0 \|_\infty \exp\left( K T \right). \]
Similarly,
$\| \varphi^\epsilon_t(\cdot) \|_{\infty} \leq \| \varphi_0 \|_\infty \exp\left( K T \right) $.
\end{proof}

We also need a continuity estimate for $\varphi$.
\begin{lemma} 
\label{ContinuityVarphi}
Let $T>0$. There exists a constant 
$K=K(T, \| \varphi_0\|_\infty)>0$ and 
$\delta_0=\delta_0(T, \| \varphi_0\|_\infty) >0$ such that 
for all $0 < \delta<\delta_0$ and $0 \leq t \leq T$,
\[ 
 \|x-y\|<\delta^3 \Rightarrow |\varphi_t(x)-\varphi_t(y)|< K\delta.
\]
\end{lemma}
\begin{proof}
First we need some notation. 
Fix $T>0$ and write $M$ for the corresponding constant from 
Proposition~\ref{BoundednessVarphis}. Let 
$\| F \|_{M} = \sup_{x \in [0,M]} |F(x)|$. We reserve $\widehat{K}$ for 
the constant on the right hand side of 
equation~(\ref{eq:boundDensityXt}). 
We write $K_{\varphi_0}$ for the Lipschitz constant of 
$\varphi_0$. Set %Having this we can set:
\[ 
\delta_0=\min\Big( \frac{1}{\| F \|_M^{2}},\frac{1}{M e\big(2\| F \|_{M}+ \widehat{K}\big)}, 
\frac{1}{K_{\varphi_0} + 2 \| F \|_M M},  1\Big).
\]
In what follows we take $0< \delta<\delta_0$. 

We first prove that the result holds if $t < \delta^2$. As before
% $f_t(x,y)$ be 
%the transition density of $(X_t)_{t \geq 0}$.  Let 
let $Z_t^x$ and $Z_t^y$ be 
independent copies of the diffusion $Z_t$ starting at $x$ and $y$ respectively. 
From our representation (\ref{FK:varphi}) and 
Lemma~\ref{BoundednessVarphis}, we can write:
\begin{align*}
|\varphi_t(x)- \varphi_t(y)| 
&\leq \big|\IE_x[\varphi_0(Z_t)]-\IE_y[\varphi_0(Z_t)]\big| 
+ 2 \| F \|_M M t 
\\ 
& \leq \IE[|\varphi_0(Z_t^x)-\varphi_0(Z_t^y)|] 
+ 2 \| F \|_M M t  \\ 
&\leq K_{\varphi_0}  \IE[\|Z_t^x-Z_t^y\|] 
+ 2 \| F \|_M M t \\ 
& \leq K_{\varphi_0}  (\sqrt{t} + \|y-x\|) 
+ 2 \| F \|_M M t \\ 
& \leq K_{\varphi_0} (\delta + \delta^3) 
+ 2 \| F \|_M M \delta^2 
\leq (K_{\varphi_0} +1)\delta,
\end{align*}
where we have used Lemma~\ref{regularityForX2} in the fourth inequality and the definition of $\delta_0$ in the last inequality.

Suppose now that $\delta^2<t$.
We will follow the pattern in Lemma~2.2 of~\cite{penington:2017}. 
First, note that by the Feynman-Kac formula we have an alternative 
representation for $\varphi_t(x)$: for any $t'<t$,
\[ 
\varphi_t(x) = \IE_x\big[ \varphi_{t-t'}(Z_{t'}) 
\exp\big( \int_0^{t'} F(\varphi_{t-s}(Z_s)) ds \big) \big]. 
\]
Therefore, setting $t'=\delta^2$, for all $z$
\[ 
e^{-\delta^2\| F \|_{M}}\IE_z\big[ \varphi_{t-\delta^2}(Z_{\delta^2})\big] 
\leq  
\varphi_t(z) 
\leq e^{\delta^2\| F \|_{M}} \IE_z\big[  \varphi_{t-\delta^2}(Z_{\delta^2})\big]. 
\]
We can then deduce that,
\begin{align}
\nonumber
\varphi_t(x)-\varphi_t(y) 
&\leq e^{\delta^2\| F \|_{M}}\IE_x\big[ \varphi_{t-\delta^2}(Z_{\delta^2})\big] 
- e^{-\delta^2\| F \|_{M}}\IE_y\big[ \varphi_{t-\delta^2}(Z_{\delta^2})\big] 
\\ 
\nonumber
& = e^{\delta^2\| F \|_{M}}
\Big(\IE_x\big[ \varphi_{t-\delta^2}(Z_{\delta^2})\big]
- \IE_y\big[ \varphi_{t-\delta^2}(Z_{\delta^2})\big]\Big) 
\\ 
\nonumber
&\qquad \qquad 
+ \big(e^{\delta^2\| F \|_{M}} - e^{-\delta^2\| F \|_{M}}  \big)
\IE_y\big[ \varphi_{t-\delta^2}(Z_{\delta^2})\big] 
\\
\label{difference}
&\leq e^{\delta^2\| F \|_{M}} 
\Big(\IE_x\big[ \varphi_{t-\delta^2}(Z_{\delta^2})\big]- 
\IE_y\big[ \varphi_{t-\delta^2}(Z_{\delta^2})\big]) 
+ M\big(e^{\delta^2\| F \|_{M}}-e^{-\delta^2\| F \|_{M}}\big),
\end{align}
where the last line follows from Lemma~\ref{BoundednessVarphis}. 

To bound the differences of the expected values in the last equation 
note that, by using again Lemma~\ref{BoundednessVarphis},
\begin{align*}
\IE_x\big[ &\varphi_{t-\delta^2}(Z_{\delta^2})\big]
- \IE_y[ \varphi_{t-\delta^2}(Z_{\delta^2})\big] & \\ 
& = \int \varphi_{t-\delta^2}(z) (f_{\delta^2}(x,z)-f_{\delta^2}(y,z) ) dz  \\ 
&\leq M \int \big|f_{\delta^2}(x,z)-f_{\delta^2}(y,z) \big| dz \\ 
&\leq M \widehat{K} \frac{\|x-y\|}{\delta}  
\leq M \widehat{K} \delta^2, 
\end{align*}
where in the first inequality we have used Lemma~\ref{regularityForX1}. 
Substituting in~(\ref{difference}),
\begin{align*}
\varphi_t(x)-\varphi_t(y)
&\leq e^{\delta^2 \| F \|_{M}} 
\left(M \widehat{K}\delta^2 +M - M e^{-2\delta^2\| F \|_{M}}  \right) \\ 
& \leq e^{\delta^2 \| F \|_{M}}\left(M \widehat{K}\delta^2 
+ 2 M \delta^2\| F \|_{M}  \right) \\ 
&\leq e \left( M \widehat{K}+2 M \| F \|_M  \right) \delta^2 \leq  \delta,
\end{align*}
where the last two inequalities follow from the definition of $\delta$. 
Interchanging $x$ and $y$ yields the same bound for $\varphi_t(y)-\varphi_t(x)$, and the result 
follows.
\end{proof}

We proceed to control the difference between $F(\varphi)$ and 
$F(\rho^\epsilon_F*\varphi)$. 
Note first that since $\rho_F\in L^1$, 
\[
I(\epsilon):=\int_{\{\|y\|>\epsilon^{3/4}\}}\rho^\epsilon_F(y)dy
=\int_{\{\|y\|>\epsilon^{-1/4}\}}\rho_F(y)dy
\to 0\qquad\mbox{ as }
\epsilon\to 0.
\]

%For simplicity, we define,
%\[ \widehat{\delta}(\epsilon) := 2e^{-\epsilon^{-1/3}/2}+\epsilon^{1/9}. \]
\begin{lemma} 
\label{ContinuityConvolution}
Let $T>0$. There exists a constant 
$C=C(T,\| \varphi_0 \|_\infty)>0$ such that, for all $0 \leq t \leq T$, 
for all $\epsilon$ small enough,
\begin{equation} 
\label{ContinuityConvolution1} 
\| \varphi_t(\cdot) - \rho^\epsilon_F*\varphi_t(\cdot) \|_\infty 
\leq C (I(\epsilon)+\epsilon^{1/4}). 
\end{equation}
Furthermore, there is a constant $\widetilde{C}(T, \| \varphi_0 \|_\infty) = \widetilde{C} >0 $ 
such that, for all $0 \leq t \leq T$,
\begin{equation} 
\label{ContinuityConvolution2} 
\| F(\varphi_t(\cdot)) - F(\rho^\epsilon_F*\varphi_t(\cdot)) \|_\infty 
\leq \widetilde{C}\big(I(\epsilon)+\epsilon^{1/4}\big). % \widehat{\delta}(\epsilon). 
\end{equation}
\end{lemma}
\begin{proof}
Let $\epsilon<\delta_0^4$, with $\delta_0$ from Lemma~\ref{ContinuityVarphi}. Then,
\begin{align*}
|\varphi_t(x) - \rho^\epsilon_F*\varphi_t(x)| & = 
\int_{\|x-y\| > \epsilon^{3/4} } \rho^\epsilon_F(x-y)|\varphi_t(y)-\varphi_t(x)| dy  \\ 
& + \int_{\|x-y\| \leq \epsilon^{3/4} } \rho^\epsilon_F(x-y)|\varphi_t(y)-\varphi_t(x)| dy \\ 
& \leq 2 M \int_{\|x-y\| > \epsilon^{3/4}  } \rho^\epsilon_F(x-y) dy 
+ \int_{\|x-y\| \leq \epsilon^{3/4}} \rho^\epsilon_F(x-y) K \epsilon^{1/4} dy \\
&\leq 2M I(\epsilon) + K \epsilon^{1/4},
\end{align*}
where we used 
the estimates of Lemma~\ref{BoundednessVarphis}
and Lemma~\ref{ContinuityVarphi}.
This proves (\ref{ContinuityConvolution1}). For (\ref{ContinuityConvolution2}), % note that, 
%by Proposition~\ref{BoundednessVarphis}, we have that $\varphi_t(x)$ and 
%$\rho_\epsilon*\varphi_t(x)$ are bounded by $M$. 
let $L_M$ be the (uniform) Lipschitz constant of $F$ on $[0,M]$. Then,
\begin{align*}
 \| F(\varphi_t(\cdot)) - F(\rho^\epsilon_F*\varphi_t(\cdot)) \|_\infty 
&\leq L_M \| \varphi_t(\cdot)) - (\rho^\epsilon_F*\varphi_t(\cdot)) \|_\infty \\ 
& \leq L_M ( 2 M I(\epsilon) + K\epsilon^{1/4}),
\end{align*}
which proves (\ref{ContinuityConvolution2}). 
\end{proof}

\begin{proof}[of Proposition \ref{prop:nonlocal_to_local}]
Let $\epsilon$ be small enough that Lemma~\ref{ContinuityConvolution} holds. 
We use the notation $\widehat{\delta}(\epsilon)$ for the quantity on the right hand side 
of~(\ref{ContinuityConvolution2}).
Then from the representation~(\ref{FK:diferencesVarphis}) 
and Lemma~\ref{ContinuityConvolution} we can write,
\begin{align*}
| &\varphi_t(x) - \varphi^\epsilon_t(x)|  \\ 
& \leq \IE_x\left[ \int_0^t\Big| \varphi_s(Z_s)
F(\rho^\epsilon_F*\varphi_s(Z_s))
-\varphi^\epsilon_s(Z_s)
F\big(\rho^\epsilon_F*\varphi^\epsilon_s(Z_s)\big)\Big| ds \right] 
+M t \widehat{\delta}(\epsilon)   \\ 
& \leq \IE_x\left[ \int_0^t \big|F(\rho^\epsilon_F*\varphi^\epsilon_s(Z_s))\big|\cdot
\big|\varphi^\epsilon_s(Z_s)-\varphi_s(Z_s)\big| ds  \right] \\ 
& \qquad +  \IE_x\left[\int_0^t |\varphi_s(Z_s)| 
\big|F(\rho^\epsilon_F*\varphi^\epsilon_s(Z_s))
-F(\rho^\epsilon_F*\varphi_s(Z_s))\big| ds \right] 
+ M t \widehat{\delta}(\epsilon)  \\ 
& \leq \| F \|_M \int_0^t \| \varphi^\epsilon_s(\cdot) 
- \varphi_s(\cdot) \|_\infty ds
  + M L_M \int_0^t \| \rho^\epsilon_F*\varphi^\epsilon_s(\cdot)
-\rho^\epsilon_F*\varphi_s(\cdot) \|_\infty ds 
+  M  t \widehat{\delta}(\epsilon)  \\ 
& \leq (\| F \|_M + M L_M) 
\int_0^t \| \varphi^\epsilon_s(\cdot) - \varphi_s(\cdot) \|_\infty ds 
+ M  t \widehat{\delta}(\epsilon),
\end{align*}
where the second inequality is the triangle inequality, 
and the third is Lemma~\ref{BoundednessVarphis}. 
An application of Gronwall's inequality then yields,
\begin{align*}
\| \varphi^\epsilon_t(\cdot)-\varphi_t(\cdot)\|_\infty 
&\leq M  t \widehat{\delta}(\epsilon)  
\exp(t (\| F \|_M + M L_M)) \\ 
& \leq  M  T \widehat{\delta}(\epsilon)  \exp(T(\| F \|_M + M L_M)),
\end{align*}
giving the result.
\end{proof}


\subsubsection{Porous Medium Equation}

In this subsection we prove Proposition~\ref{nonlocalPME to PME}.
To ease notation, we drop the subscript $\gamma$ on $\rho_\gamma$ and we present 
the proof in $d=1$ (although we retain the notation $\nabla$).
It should be clear that it extends almost without change
to higher dimensions. 

Recall that 
we are concerned with non-negative solutions to the equation~(\ref{mollified equation}):
\begin{equation*}
%\label{mollified equation}
	\partial_t\psi_t^\epsilon(x) =
	\Delta\left(\psi_t^\epsilon \, \rho^\epsilon *\psi_t^\epsilon\right)(x)
	+\psi_t^\epsilon(x)\left(1-\rho^\epsilon *\psi_t^\epsilon(x) %*\zeta_\epsilon
\right),
\end{equation*}
and 
we assume that $\rho=\zeta*\check{\zeta}$ with $\zeta$ a rapidly decreasing function. 
The example we have in mind is $\zeta$ (and therefore $\rho$) being the density of
a mean zero Gaussian random variable.  
We shall prove that under the 
assumptions of Proposition~\ref{nonlocalPME to PME},
as $\epsilon\to 0$, we have
convergence to the solution to the porous medium equation with
logistic growth:
\begin{equation}
\label{PMEold}
	\partial_t \psi_t(x)=
%\frac{\partial^2}{\partial x^2}
	\Delta\left(\psi_t^2\right)(x)
	+\psi_t(x)\left(1-\psi_t(x)\right).
\end{equation}

We work on the time interval $[0,T]$.
We will require a lower bound on $\int \psi_t^\epsilon(x)\log \psi_t^\epsilon(x) dx$ 
which we record as a lemma.
\begin{lemma}
\label{lower bound on ulogu}
Suppose that there exists $\lambda<1$ and $C<\infty$, both independent of
$\epsilon$, such that $\int \exp(\lambda |x|) \psi_0^\epsilon(x) dx<C$.
Then there exists
a constant $K<\infty$, independent of $\epsilon$, such that  
 $\int \psi_t^\epsilon(x)\log \psi_t^\epsilon (x)dx>-K$ for all $t\in [0,T]$. 
\end{lemma}
\begin{proof}

Consider 
\begin{multline}
\frac{d}{dt}\int \exp(\lambda x) \psi_t^\epsilon (x)dx
	=\int \exp(\lambda x)\Delta \big(\psi_t^\epsilon \, \rho^{\epsilon}*\psi_t^\epsilon\big)(x) dx
	\\
%*\zeta_\epsilon)\dif x
	+\int \exp(\lambda x)\psi_t^\epsilon(x)\big(1-\rho^{\epsilon}*\psi_t^\epsilon(x)\big) dx\\
	=\int (\lambda^2-1)\exp(\lambda x) \psi_t^\epsilon(x) \rho^{\epsilon}*
	\psi_t^\epsilon(x) dx 
%*\zeta_\epsilon)\dif x
	+\int \exp(\lambda x)\psi_t^\epsilon(x) dx\\
\leq
	\int \exp(\lambda x)\psi_t^\epsilon(x) dx,
\end{multline}
and so, by Gronwall's inequality, 
$\int \exp(\lambda x)\psi_t^\epsilon (x)dx$ is uniformly bounded on $[0,T]$.
In particular, combining with the Mean Value Theorem, we find 
\begin{equation*}
\int_x^{x+1}\psi_t^\epsilon(y)dy\leq C\exp(-\lambda x),
\end{equation*}
where the constant $C$ is independent of $x>1$.
A fortiori, 
\begin{equation}
\label{exp decay of u} 
\int_x^{x+1}\psi_t^\epsilon(y)\ind_{\psi_t^\epsilon(y)\leq 1}dy\leq C\exp(-\lambda x).
\end{equation}
Now the function $\psi\mapsto \ind_{0\leq \psi\leq 1}\psi|\log \psi|$ is concave, and so 
using Jensen's inequality and~(\ref{exp decay of u}),
\begin{equation*}
\int_x^{x+1}\psi_t^\epsilon(y)
|\log \psi_t^\epsilon (y)|\ind_{\psi_t^\epsilon(y)\leq 1}dy
\leq C'x \exp(-\lambda x).
\end{equation*}
Evidently a symmetric argument applies for $x<-1$.
Summing over $x$, we find
\begin{equation*}
\int \psi_t^\epsilon (x) \log \psi_t^\epsilon (x) dx
\geq -C''\sum_{x=1}^\infty x\exp(-\lambda x)>-K>-\infty,  
\end{equation*}
as required.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{nonlocalPME to PME}]

First observe that
\begin{multline*}
	\int \psi_t^\epsilon(x)\, \rho^{\epsilon}*\psi_t^\epsilon(x) dx 
=
\int\int\int \psi_t^\epsilon(x)\psi_t^\epsilon(x-y)\zeta^{\epsilon}(y-z)
\check{\zeta}^{\epsilon}(z) dz dy dx %\dif z\dif y\dif x
\\
=
\int\int\int \psi_t^\epsilon(\tilde{x}-\tilde{z})
\psi_t^\epsilon(\tilde{x}-\tilde{y})
\zeta^{\epsilon}(\tilde{y})
\zeta^{\epsilon}(\tilde{z}) d\tilde{z} d\tilde{y} d \tilde{x}
	=\int\left(\zeta^{\epsilon}*\psi_t^\epsilon(x)\right)^2 dx,
%*\rho_\epsilon\right(x))^2\dif x,
\end{multline*}
where we have set $\tilde{x}=x-z$, $\tilde{y}=y-z$, $\tilde{z}=-z$.

Now note that
\begin{eqnarray*}
\frac{d}{dt}\int \psi_t^\epsilon (x) dx
	&=&
\int \Delta \big(\psi_t^\epsilon \, \rho^{\epsilon}*\psi_t^\epsilon\big)(x) dx 
	+\int \psi_t^\epsilon(x)\big(1-\rho^{\epsilon}*\psi_t^\epsilon(x)\big) dx
\\	
&=&
 \int \psi_t^\epsilon (x) d x
	-\int \big(\zeta^{\epsilon}*\psi_t^\epsilon(x)\big)^2  dx.
\end{eqnarray*}
Thus $\int \psi_t^\epsilon(x) dx$ is uniformly bounded above in
$\epsilon$ and $t\in [0,T]$. Note that this also then gives a uniform
bound on the rate of change of $\int \psi_t^\epsilon (x) d x$, and since
we are working on $[0,T]$ this will be enough to give continuity in time of the
$L^1$ norm of the limit
when we pass to a convergent subsequence.

Now consider
\begin{eqnarray}
	\frac{d}{dt}\int \psi_t^\epsilon\log \psi_t^\epsilon dx &=&
	\int (1+\log \psi_t^\epsilon)\left[\Delta \big(\psi_t^\epsilon\,
	\rho^{\epsilon}*\psi_t^\epsilon\big)
	+\psi_t^\epsilon\big(1-\rho^{\epsilon}*\psi_t^\epsilon\big)\right]
dx
\nonumber
\\
&=&
	\int (1+\log \psi_t^\epsilon)\left[\nabla\Big(\psi_t^\epsilon\,
	\nabla(\rho^{\epsilon}*\psi_t^\epsilon)+
	\nabla \psi_t^\epsilon\, \rho^{\epsilon}*\psi_t^\epsilon\Big)
	+\psi_t^\epsilon\big(1-\rho^{\epsilon}*\psi_t^\epsilon\big)\right]  dx
\nonumber
	\\
&=&
\int \left[-\frac{\nabla \psi_t^\epsilon}{\psi_t^\epsilon}
\Big(\psi_t^\epsilon\, \nabla(\rho^{\epsilon}*\psi_t^\epsilon)
+\nabla \psi_t^\epsilon\, \rho^{\epsilon}*\psi_t^\epsilon\Big)+
(1+\log \psi_t^\epsilon)\psi_t^\epsilon(1-\rho^{\epsilon}*\psi_t^\epsilon)\right] d x
\nonumber
	\\
&=&
-\int \left(\nabla (\zeta^{\epsilon}*\psi_t^\epsilon)\right)^2  dx
-\int (\nabla \psi_t^\epsilon)^2\frac{\rho^{\epsilon}*\psi_t^\epsilon}{\psi_t^\epsilon}  dx
\nonumber
	\\
&&+\int
\left[\psi_t^\epsilon+\psi_t^\epsilon\log \psi_t^\epsilon
\big(1-\rho^{\epsilon}*\psi_t^\epsilon\big)
-\psi_t^\epsilon\, \rho^{\epsilon}*\psi_t^\epsilon\right] dx
\nonumber
	\\
&=&
-\int \left(\nabla (\zeta^{\epsilon}*\psi_t^\epsilon)\right)^2  dx
-\int (\nabla \psi_t^\epsilon)^2\frac{\rho^{\epsilon}*\psi_t^\epsilon}{\psi_t^\epsilon} dx
-\int (\zeta^{\epsilon}*\psi_t^\epsilon)^2  dx
\nonumber
	\\
&&+\int
\left[\psi_t^\epsilon+\psi_t^\epsilon\log \psi_t^\epsilon
\big(1-\rho^{\epsilon}*\psi_t^\epsilon\big)
%-u^\epsilon\cdot u^\epsilon*\zeta_\epsilon
\right] dx .
	\label{change in ulogu}
\end{eqnarray}
The first three terms are negative; and we already
saw that the $L^1$ norm
of $\psi_t^\epsilon$ is uniformly bounded.
%
Moreover, since $\psi_t^\epsilon\log \psi_t^\epsilon$ is uniformly bounded below,
$$-\int \psi_t^\epsilon \log \psi_t^\epsilon \, \rho^{\epsilon}*\psi_t^\epsilon d x
\leq C\int \rho^{\epsilon}*\psi_t^\epsilon d x=\int \psi_t^\epsilon dx.$$
From this and~(\ref{change in ulogu}),
we see immediately that $\int \psi_t^\epsilon \log \psi_t^\epsilon dx$
is uniformly bounded
above in $\epsilon$ and $t\in [0,T]$. 
Combining with Lemma~\ref{lower bound on ulogu},
we deduce that
we have a uniform bound on $\int \psi_s^\epsilon(x)|\log \psi_s^\epsilon(x)| d x$.
This in turn means that both
$\int_0^t \int(\zeta^{\epsilon}*\psi_s^\epsilon(x))^2  dx ds$ and
$\int_0^t \int \big(\nabla (\zeta^{\epsilon}*\psi_s^\epsilon(x))\big)^2  dx ds$
are uniformly bounded in
$\epsilon$ and $t\in [0,T]$.
Now observe that
\begin{equation}
\label{total equation}
\int\Delta\left((\rho^{\epsilon}*\psi_t^\epsilon)\psi_t^\epsilon\right)\phi  dx =
\int \nabla(\rho^{\epsilon}*\psi_t^\epsilon)\,
\psi_t^\epsilon\,\nabla\phi dx +\int \rho^{\epsilon}*\psi_t^\epsilon
\,\nabla \psi_t^\epsilon
\, \nabla\phi d x.
\end{equation}
For the first term
\begin{eqnarray}
\nonumber
\int \nabla(\rho^{\epsilon}*\psi_t^\epsilon)\, \psi_t^\epsilon\, \nabla\phi  dx
&=&
\int\int\int\nabla \psi_t^\epsilon(x-y)\zeta^{\epsilon}(y-z)
\check{\zeta}^{\epsilon}(z)
\psi_t^\epsilon(x)\nabla\phi(x) dz  dy  dx
\\
\nonumber
&=&
\int\int\int\nabla \psi_t^\epsilon(\tilde{x}-\tilde{y})
\zeta^{\epsilon}(\tilde{y})
\zeta^{\epsilon}(\tilde{z})\psi_t^\epsilon(\tilde{x}-\tilde{z})
\nabla\phi(\tilde{x}-\tilde{z}) d\tilde{z} d\tilde{y} d\tilde{x}
\\
\nonumber
&=&\int(\nabla \zeta^{\epsilon}*\psi_t^\epsilon)\,
\left(\zeta^{\epsilon}*(\psi_t^\epsilon\nabla\phi)\right)
d x\\
\nonumber
&=&
\frac{1}{2}\int\nabla ((\zeta^{\epsilon}*\psi_t^\epsilon)^2)
\,\nabla\phi  dx
\\
\label{error 1}
&&+
\int\nabla(\zeta^{\epsilon}*\psi_t^\epsilon)
\left[\zeta^{\epsilon}*(\psi_t^\epsilon\,\nabla\phi)-
\nabla\phi\,(\zeta^{\epsilon}*\psi_t^\epsilon)\right] dx,
\end{eqnarray}
where, as before, we have substituted $\tilde{x}=x-z$, $\tilde{y}=y-z$,
$\tilde{z}=-z$.

To control the term~(\ref{error 1})
we use the Intermediate Value Theorem to see that
\begin{multline*}
\left|\int\left[\psi_t^\epsilon(x-y)\nabla\phi(x-y)\zeta^{\epsilon}(y)-
\nabla\phi(x)\psi_t^\epsilon(x-y)\zeta^{\epsilon}(y)
\right] dy\right|
\\
\leq C\|\Delta\phi\|_\infty\int \psi_t^\epsilon (x-y)y\zeta^{\epsilon}(y) d y.
\end{multline*}
Since $\zeta\in\mathcal{S}(\IR)$, 
%Using the bound 
%$yp_{\epsilon^2/2}(y)\leq C\epsilon p_{\epsilon^2}(y)$ 
%(c.f.~(\ref{first use of trick}) above)
the integral in this expression is
$\mathcal{O}(\epsilon)$.

Similarly, for the second term in~(\ref{total equation}),
\begin{eqnarray}
\nonumber
\int (\rho^{\epsilon}*\psi_t^\epsilon)\, \nabla \psi_t^\epsilon\, \nabla\phi dx
&=&
\int\int\int \psi_t^\epsilon(x-y)\zeta^{\epsilon}(y-z)
\check{\zeta}^{\epsilon}(z)\nabla \psi_t^\epsilon(x)\nabla\phi(x) dz dy  dx
\\
\nonumber
&=&
\int\int\int \psi_t^\epsilon(\tilde{x}-\tilde{y})\zeta^{\epsilon}(\tilde{y})
\zeta^{\epsilon}(\tilde{z})\nabla \psi_t^\epsilon(\tilde{x}-\tilde{z})
\nabla\phi(\tilde{x}-\tilde{z}) d \tilde{z} d\tilde{y} d\tilde{x}
\\
\nonumber
	&=&\int(\zeta^{\epsilon}*\psi_t^\epsilon)\,
\left(\zeta^{\epsilon}*(\nabla \psi_t^\epsilon \, \nabla\phi)\right)  dx\\
\nonumber
&=&
\frac{1}{2}\int\nabla ((\zeta^{\epsilon}*\psi_t^\epsilon)^2)
\, \nabla\phi  dx
\\
\label{error 2}
&&+
\int (\zeta^{\epsilon}*\psi_t^\epsilon)
\left[\zeta^{\epsilon}*(\nabla \psi_t^\epsilon\, \nabla\phi)-
\nabla\phi\,(\nabla \zeta^{\epsilon}*\psi_t^\epsilon)\right] dx,
\end{eqnarray}
and~(\ref{error 2}) is controlled in the same way as~(\ref{error 1}):
using the Intermediate Value Theorem:
\begin{multline*}
\left|\int\left[\nabla \psi_t^\epsilon(x-y)\nabla\phi(x-y)\zeta^{\epsilon}(y)
-\nabla\phi(x)\nabla \psi_t^\epsilon(x-y)\zeta^{\epsilon}(y)
\right] dy\right|
\\
\leq C\|\Delta\phi\|_\infty\left|\int \nabla \psi_t^\epsilon(x-y)
y \zeta^{\epsilon}(y)
d y\right|,
\end{multline*}
which again is $\mathcal{O}(\epsilon)$.

We now have the ingredients that we need.
The calculations above yield both a uniform (in $\epsilon$) bound on
$\zeta^{\epsilon}*\psi_t^\epsilon$ in
$L^1\cap L^2\big([0,T]\times \IR\big)$, and
\begin{multline}
\int \psi_t^\epsilon(x)\phi(x) dx-\int \psi_0^\epsilon (x)\phi(x) dx
	=\int_0^t\int (\zeta^{\epsilon}*\psi_s^\epsilon(x))^2\Delta\phi(x) dx
\\
	+\int_0^t \int \zeta^{\epsilon}*\psi_s^\epsilon(x)
	\left(1- \zeta^{\epsilon}*\psi_s^\epsilon(x)\right)
\phi(x) dx
+\mathcal{O}(\epsilon)
\end{multline}
(for sufficiently regular $\phi$). Since
$\int \psi_t^\epsilon(x)\phi(x) d x-
\int \zeta^{\epsilon}*\psi_t^\epsilon (x)\phi(x) d x$ is order $\epsilon$, we see
that $\zeta^{\epsilon}*\psi^\epsilon\rightharpoonup \psi$ in $L^1$, where $\psi$ is the (unique)
solution
to equation~(\ref{PME})
and, so, therefore, does $\psi^\epsilon$. In fact, strong convergence, that is
$\int |\psi^\epsilon -\psi|\phi dx \to 0$, follows from the
uniform integrability of $\psi^\epsilon$ that we
deduce from the uniform control of
$\int \psi^\epsilon|\log \psi^\epsilon| d x$ that we proved above.
\end{proof}


% % % % % % % % % % % % %
\subsection{Simultaneous scaling with interaction distance}



In this section we prove Theorem~\ref{thm:local_convergence},
which simultaneously scales the parameters $\theta$ and $N$ and the interaction 
kernel $\rho_F$ in the special case in which $r\equiv 1\equiv\gamma$, $q_{\theta}(x,dy)$
is isotropic with zero mean, the kernel
$\rho_F$ is Gaussian, and the scaling 
limit is a reaction diffusion equation.

To simplify notation, in this section we shall write 
$$\rho_\epsilon*\eta(x)=\rho_F^\epsilon*\eta(x)
=\langle p_{\epsilon^2}(x,y),\eta(dy)\rangle,$$
where $p_t(x,y)$ denotes the heat semigroup.

The first part of the proof mirrors that of Theorem~\ref{thm:nonlocal_convergence}:
we establish sufficient bounds on the moments of $\rho_\epsilon*\eta_t(x)$ to
establish $C$-tightness and then apply standard results on 
convergence of Markov processes from~\cite{ethier/kurtz:1986}. Although
we prove the required bounds, we skip the details of this part. The challenge
comes in identifying the limit points. This is much more intricate than the case in 
which we do not scale the interaction kernel, as weak convergence will no longer
be sufficient to guarantee the form of the nonlinear terms in the limiting equation.
Identification of the limit will rest on regularity inherited from continuity 
estimates for a random walk with Gaussian jumps which we prove in 
Subsection~\ref{continuity for random walk}.
Various bounds on $\rho_\epsilon*\eta_t$ are then provided in 
Subsection~\ref{bounds on rhoepsilon},
and we identify the limit points in Subsection~\ref{limit in onestep case}.


\subsubsection{Continuity estimates for a Gaussian random walk}
\label{continuity for random walk}

Let us write $\mathcal{L}^\theta f(x) := \theta \int (f(y)-f(x))q_\theta(x,y) dy$ 
where $q_\theta$ is a Gaussian kernel of mean $0$ and variance $1/\theta$. 
We note that $\mathcal{L}^\theta$ is the generator of a continuous (time and 
space) random walk, which makes jumps of mean $0$ and variance $1/\theta$ at
rate $\theta$.

In what follows we write $\psi_t^{\epsilon, x}(y)$ for the solution of
\begin{equation}
    \partial_t \psi_t^{\epsilon,x}
    =
    \mathcal{L}^\theta \psi_t^{\epsilon, x},
    \label{AlmostHeatEquation}
\end{equation}
with initial condition 
$\psi_0^{\epsilon,x}(y) = \rho_\epsilon(y-x) =p_{\epsilon^2}(x,y)$.
%We have the following result for $\psi_t^{\epsilon,x}$.

\begin{lemma} \label{PsiBoundHS}
    Fix $t>0$,
    let $(N(s))_{s \ge 0}$ be a rate one Poisson process,
    and let $T(t) = N(\theta t) / \theta$.
    Then
    \[
        \psi^{\epsilon,x}_t(y)
        =
        \IE\left[ p_{\epsilon^2+T(t)}(x,y)\right],
    \]
    and, moreover, there is a $C$ independent of $\epsilon$ or $t$ such that
    \[
        \| \psi^{\epsilon,x}_t \|_\infty
        \leq
        \frac{C}{(\epsilon^2 + t)^{d/2}} .
    \]
\end{lemma}

\begin{proof}
    The first claim is immediate from the definition of the random walk 
with generator $\mathcal{L}^\theta$.

For the second claim, first define $\tau(t) = T(t)-t$.
Since if $\tau(t) \ge -(\epsilon^2 + t)/2$,
then $1/(\epsilon^2 + T(t)) \le 2/(\epsilon^2 + t)$,
while $\epsilon^2 + T(t) \ge \epsilon^2$ always,
then partitioning over
$\{ \tau(t) \ge -(\epsilon^2 + t)/2 \}$ and its complement,
\begin{align}
   \| \psi^{\epsilon,x} \|_\infty
    &=
        \IE\left[ 
            \frac{1}{\big(2 \pi (\epsilon^2+T(t))\big)^{d/2}}
        \right] \nonumber
    \\ & \le
        \frac{C}{(\epsilon^2 + t)^{d/2}}
        + 
        \frac{C}{\epsilon^d}
        \IP\left\{
            \tau(t) < - (\epsilon^2 + t) / 2
        \right\} \label{eqn:psi_infty_bound}
    \end{align}
    Now, observe that since $\IE[e^{-N(\theta t)}] = \exp(-\theta t (1 - e^{-1}))$,
    by Markov's inequality,
    \begin{align}
        \IP\left\{
            \tau(t) < - \frac{\epsilon^2 + t}{2}
        \right\}
        &=
        \IP\left\{
            e^{-N(\theta t)} > e^{-\theta (t - \epsilon^2) / 2}
        \right\}
\nonumber
\\
&\leq \frac{\IE[\exp\big(-N(\theta t)\big)]}{\exp\big(-\theta(t-\epsilon^2)/2\big)}
\nonumber
        \\&=
        \frac{
            \exp(- \theta t(1 - e^{-1}))
        }{
            \exp(- \theta (t - \epsilon^2) / 2)
        }
\nonumber
        \\&=
        \exp\left\{ - \delta \theta t - \frac{\theta \epsilon^2}{2} \right\} ,
\label{bound for negative tau}
    \end{align}
    where $\delta = 1/2 - e^{-1} > 0$.
    The second term in \eqref{eqn:psi_infty_bound} is therefore bounded by
    \[
        C \left(1 + \frac{t}{\epsilon^2}\right)^{d/2} e^{-\delta \theta t - \epsilon^2 \theta / 2}
        \frac{1}{(\epsilon^2 + t)^{d/2}} .
    \]
    Now observe that the derivative (with respect to $t$) of 
$e^{-\delta \theta t - \epsilon^2 \theta / 2} (1 + t/\epsilon^2)^{d/2}$ 
is
    \[
        \left( \frac{d}{2 \epsilon^2} - \left(1 + \frac{t}{\epsilon^2}\right) \delta \theta \right)
        \left( 1 + \frac{t}{\epsilon^2} \right)^{d/2 - 1} e^{-\delta \theta t - \epsilon^2 \theta / 2} ,
    \]
    which is negative if $\theta(\epsilon^2 + t) > d/2 \delta$,
so that
the second term in \eqref{eqn:psi_infty_bound} 
is bounded by $C / (\epsilon^2 + t)^{d/2}$
(for a different constant $C$).
Substituting this into \eqref{eqn:psi_infty_bound} yields the result.
\end{proof}

We are going to approximate $\psi_t^{\epsilon,x}(\cdot)$ by $p_{\epsilon^2+t}(x, \cdot)$,
and to control the error that this introduces we 
need to control $T(t)-t$. We achieve this through a large deviations bound.

\begin{lemma}
    \label{lem:poisson_ld}
    Fix $A > e^3 - 1$ (so that $\log(1 + A) > 3$).
    In the notation of Lemma~\ref{PsiBoundHS},
    \begin{align*}
        \limsup_{\theta \to \infty}
        \frac{1}{\theta} \log
        \IP\left\{
            T(t) - t > A(\epsilon^2 + t)
        \right\}
    \le
        - (\epsilon^2 + t) \frac{A}{2}
        - \log(1 + A)\left( \frac{t + A(\epsilon^2 + t)}{2} \right) .
    \end{align*}
\end{lemma}

\begin{proof}%[of Lemma~\ref{lem:poisson_ld}]
    We apply Cram\'er's Theorem.
    First note that
    \begin{align*}
        \IP\left\{
            T(t) - t > A(\epsilon^2 + t)
        \right\}
        =
        \IP\Big\{
            \frac{N(\theta t)}{ \theta}
            >
            t + A(\epsilon^2 + t)
        \Big\} .
    \end{align*}
For convenience, suppose $\theta$ is an integer,
and so we may write $N(\theta t) = Z_1(t) + \cdots + Z_\theta(t)$,
where the $Z_i(t)$ are i.i.d.~Poisson($t$) random variables.
    To apply Cram\'er's Theorem, we need the rate function for the 
Poisson($t$) distribution,
    which is
    \[
        \Lambda^*(x) = \sup_{\alpha > 0} \left[ \alpha x - \lambda(\alpha) \right],
    \]
    where $\lambda(\alpha) = \log \IE[\exp(\alpha Z_1(t))] = t(e^\alpha - 1)$.
    Now, the supremum of $\alpha x + t - te^\alpha$ is attained at $x = t e^\alpha$,
    and so
    \[
        \Lambda^*(x) = x \log(x/t) - x + t .
    \]
    Cram\'er's Theorem then gives
    \begin{align*}
        \limsup_{\theta \to \infty}
        \frac{1}{\theta} \log
        \IP\left\{
            \frac{N(\theta t)}{\theta}
            > A(\epsilon^2 + t)
        \right\}
    &\le
        - \Lambda^*(t + A(\epsilon^2 + t))
    \\ &=
        A(\epsilon^2 + t)
        -
        \big(t + A(\epsilon^2 + t)\big) \log\left( 1 + \frac{A(\epsilon^2 + t)}{t} \right) 
    \\ &\le
        A(\epsilon^2 + t)
        -
        \big(t + A(\epsilon^2 + t)\big) \log(1 + A)
    \\ &\le
        A(\epsilon^2 + t) \left( 1 - \frac{1}{2} \log(1 + A) \right)
        -
        \frac{1}{2} \big(t + A(\epsilon^2 + t)\big) \log(1 + A)  .
    \end{align*}
    Since $(1/2) \log(1 + A) > 3/2$, from which $1 - (1/2) \log(1 + A) < - 1/2$,
    the proof is complete.
\end{proof}

As advertised, we wish to control the difference between
$\psi_t^{\epsilon,x}(y)$
and $p_{\epsilon^2 + t}(x, y)$.

\begin{lemma}
    \label{Lemma:BoundPsiHS2}
    In the notation of Lemma~\ref{PsiBoundHS},
    and with $A$ as in Lemma~\ref{lem:poisson_ld},
    \begin{multline}
\label{heat kernel estimate}
        \left|
            \psi_t^{\epsilon, x}(y)
            -
            p_{\epsilon^2 + t}(x, y)
        \right|
        \\
\le
        \frac{C}{(\epsilon^2 \theta)^{d/2}}
        p_{2(A+1)(\epsilon^2+t)}(x, y)
        +
        C \exp(- \epsilon^2 \theta / 2)
        +
        \frac{C}{(\epsilon^2 + t)^{d/2}}
        \IP\left\{
            \tau(t) > A(\epsilon^2 + t)
        \right\} .
    \end{multline}
\end{lemma}

\begin{proof}%[of Lemma~\ref{Lemma:BoundPsiHS2}]
    Still using the notation of Lemma~\ref{PsiBoundHS},
the three terms on the right hand side of 
equation~(\ref{heat kernel estimate})
    correspond to bounds obtained by partitioning
    into three events according to the value of $\tau(t)$.
    Let $A_1 = \{ \tau(t) < - (\epsilon^2 + t)/2 \}$,
    $A_2 = \{ \tau(t) > A(\epsilon^2 + t) \}$,
    and $A_3$ the remaining event, $\{ - (\epsilon^2 + t)/2 \le \tau(t) \le A(\epsilon^2 + t) \}$.
    Then,
    \begin{align*}
        \left|
            \psi_t^{\epsilon, x}(y)
            -
            p_{\epsilon^2 + t}(x, y)
        \right|
        &=
        \left|
        \IE\left[
            p_{\epsilon^2 + t + \tau(t)}(x, y)
            -
            p_{\epsilon^2 + t}(x, y)
        \right]
        \right|
        \\&\le
        \IE\left[
            (1_{A_1} + 1_{A_2} + 1_{A_3})
            \left|
            p_{\epsilon^2 + t + \tau(t)}(x, y)
            -
            p_{\epsilon^2 + t}(x, y)
        \right|
        \right] .
    \end{align*}
    For the first term, note that if $a < b$ then
    \begin{align*}
        |p_a(x, y) - p_b(x, y)| 
        &=
        \frac{1}{(2\pi)^{d/2}}
        \left|
            \frac{1}{a^{d/2}}
            e^{-\|x - y\|^2 / 2a}
            -
            \frac{1}{b^{d/2}}
            e^{-\|x - y\|^2 / 2b}
        \right|
        \\ &=
        \frac{1}{(2\pi a^2)^{d/2}}
            e^{-\|x - y\|^2 / 2b}
        \left|
            e^{-\|x - y\|^2 \left(\frac{1}{2a} - \frac{1}{2b}\right)}
            -
            \left(\frac{a}{b}\right)^{d/2}
        \right|
        \\ &\le
        C \left(\frac{b}{a}\right)^{d/2}
        p_b(x, y) ,
    \end{align*}
    where the inequality follows because both terms under the absolute value 
are less than 1.
    % note the original had $p_{b/2}$ on the right.
Since, on the event $A_1$, $\tau(t)<0$, we can  
apply this with $a = \epsilon^2 + t + \tau(t)$ and $b = \epsilon^2 + t$,
and, using the bound~(\ref{bound for negative tau}),
    \begin{align*}
        \IE\left[
            1_{A_1} |p_a(x, y) - p_b(x, y)| 
        \right]
        & \le
            C \left(\frac{\epsilon^2 + t}{\epsilon^2}\right)^{d/2}
            p_{\epsilon^2 + t}(x, y) 
            \IP\left\{ \tau(t) < - \frac{\epsilon^2 + t}{2} \right\}
        \\ & \le
            C \frac{1}{\epsilon^d}
            \IP\left\{ \tau(t) < - \frac{\epsilon^2 + t}{2} \right\}
        \\ & \le
        C \exp\left(-\frac{\theta \epsilon^2}{2} \right) .
    \end{align*}

    For the third term, we will first collect some facts.
    Observe that on the event $A_3$,
    $\epsilon^2 + t + \tau(t)$ is between
    $(\epsilon^2 + t)/ 2$ and $(A + 1)(\epsilon^2 + t))$,
    and for any $s$ in this interval,
    \begin{align}
        p_{2s}(y)
        &\le \nonumber
        \left( \frac{
            2 (\epsilon^2 + t)(A + 1)
        }{
            \epsilon^2 + t
        } \right)^{d/2}
        p_{2(A+1)(\epsilon^2 + t)}(x, y)
        \\ &= \label{eqn:p_bounded_on_interval}
        (2(A+1))^{d/2}
        p_{2(A+1)(\epsilon^2 + t)}(x, y) .
    \end{align}
    Moreover, since $u e^{-u} \le e^{-1}$ for all $u \ge 0$,
    \begin{align}
        \frac{\|x - y\|^2}{s} p_s(x, y)
        &= \nonumber
        \frac{4}{(2 \pi s)^{d/2}}
        e^{- \frac{ \|x-y\|^2 }{ 4s }}
        \frac{\|x-y\|^2}{4s}
        e^{- \frac{ \|x-y\|^2 }{ 4s }}
        \\ &\le \label{eqn:p_deriv_term2}
        C p_{2s}(x, y) .
    \end{align}
    Now, by the Intermediate Value Theorem,
    \begin{align}
\label{deduction from IVT}
        \left|
            p_{\epsilon^2 + t + \tau(t)}(x, y)
            -
            p_{\epsilon^2 + t}(x, y)
        \right|
        &=
        \left| \tau(t) \right|
        \left| \frac{\partial p_s(x, y)}{\partial s} \right|
    \end{align}
    for some $s$ between $\epsilon^2 + t + \tau(t)$ and $\epsilon^2 + t$.
   Since 
    \begin{align*}
        \frac{\partial}{\partial s} p_s(x, y)
        &=
        \frac{\partial}{\partial s}
        \left(
            \frac{1}{(2 \pi s)^{d/2}}
            \exp\left( - \frac{\|x - y\|^2}{2 s} \right)
        \right)
        \\ &=
        - \frac{d}{2s} p_s(x, y) + \frac{\|x - y\|^2}{2 s^2} p_s(x, y),
    \end{align*}
    applying the inequality~\eqref{eqn:p_deriv_term2},
    using the fact that $p_s(x, y) \le 2^{d/2} p_{2s}(x,y)$,
    and then \eqref{eqn:p_bounded_on_interval},
    we have that for any $s \in ((\epsilon^2 + t)/ 2, (A + 1)(\epsilon^2 + t))$,
    \begin{align*}
        \left| \frac{\partial}{\partial s} p_s(x, y) \right|
        \le
        \frac{C}{s} p_{2s}(x, y) 
        \le
        \frac{C}{\epsilon^2 + t} p_{2(A + 1)(\epsilon^2 + t)}(x, y) .
    \end{align*}
    Therefore, recalling that $\IE[\tau(t)^2] = t / \theta$, substituting 
into~(\ref{deduction from IVT}),
    \begin{align*}
        \IE\left[
            1_{A_3}
            \left|
                p_{\epsilon^2 + t + \tau(t)}(x, y)
                -
                p_{\epsilon^2 + t}(x, y)
            \right|
        \right]
        &\le
            \frac{C}{\epsilon^2 + t}
                p_{2(A + 1)(\epsilon^2 + t)}(x, y)
            \IE\left[|\tau(t)|\right]
        \\ &\le
            \frac{C}{\epsilon^2 + t}
                p_{2(A + 1)(\epsilon^2 + t)}(x, y)
            \IE\left[\tau(t)^2\right]^{1/2}
        \\ &=
            \left( \frac{Ct}{\theta(\epsilon^2 + t)^2} \right)^{1/2}
                p_{2(A + 1)(\epsilon^2 + t)}(x, y)
        \\ &\le
            \frac{C}{\sqrt{\theta \epsilon^2}}
                p_{2(A + 1)(\epsilon^2 + t)}(x, y) ,
    \end{align*}
    where the last inequality follows from $2 \epsilon^2 t \le (\epsilon^2 + t)^2$.

    Finally, on the event $A_2 = \{\tau(t) > A(\epsilon^2 + t)\}$,
    we simply use
    \begin{align*}
        \left|
            p_{\epsilon^2 + t + \tau(t)}(x, y)
            -
            p_{\epsilon^2 + t}(x, y)
        \right|
        \le
        \frac{C}{(\epsilon^2 + t)^{d/2}} ,
    \end{align*}
    so that
    \begin{align*}
        \IE\left[
        1_{A_2}
        \left|
            p_{\epsilon^2 + t + \tau(t)}(x, y)
            -
            p_{\epsilon^2 + t}(x, y)
        \right|
        \right]
        \le
        \frac{C}{(\epsilon^2 + t)^{d/2}}
        \IP\left\{
            \tau(t) > A (\epsilon^2 + t)
        \right\} .
    \end{align*}
\end{proof}

The last result will be useful when combined with the next bound for the heat kernel.

\begin{lemma}
    \label{Lemma:ContinuityHS}
Let $s>0$, and $x, y, z\in \IR^d$. The following estimate holds:
\[
    |p_s(x,z) - p_s(y,z)|
    \leq
    \frac{C\|x-y\|}{\sqrt{s}} \left(p_{2s}(x,z) + p_{2s}(y,z)\right) ,
\]
where the constant $C$ does not depend on $x,y,z$ or $s$.
\end{lemma}

\begin{proof}

    Expanding the difference of two squares,
    \begin{align*}
        e^{- \frac{\|y-z\|^2}{2 s}}
        -
        e^{- \frac{\|x-z\|^2}{2 s}}
        &=
        \left(
            e^{-\frac{\|y-z\|^2}{4 s}}
            -
            e^{-\frac{\|x-z\|^2}{4 s}}
        \right)
        \left(
            e^{-\frac{\|y-z\|^2}{4 s}}
            +
            e^{-\frac{\|x-z\|^2}{4 s}}
        \right) .
    \end{align*}
Now, thinking of the first term in brackets as a function of a single variable on
the line segment $[y,z]$ connecting $y$ to $z$, we can apply the
Intermediate Value Theorem and take the modulus to bound this expression by 
    \[
        \|y-x\|
        \left( \frac{2\|w-z\|}{4s} \exp\left(-\frac{\|w-z\|^2}{4s}\right)\right)
        (4 \pi s)^{d/2}
        \left( p_{2s}(y,z)+p_{2s}(x,z) \right)
    \]
    for some $w\in [y,z]$.
    Using the fact that $x e^{-x^2}$ is uniformly bounded,
    we can bound the first bracket in the last equation by $C/\sqrt{s}$, and the result follows.
\end{proof}



\subsubsection{Moment bounds for $\rho_\epsilon*\eta$}
\label{bounds on rhoepsilon}


\begin{lemma}
\label{bounds on moments}
For each $T\in [0,\infty)$, and $k\in\IN$, there exist constants $C=C(k,T)$ and
$\widetilde{C}=\widetilde{C}(k,T)$, independent of $\epsilon$, such that for all 
$x\in\IR^d$ and all $u,t\in [0,T]$ with $u<t$,
\begin{equation}
\label{moment bound rhoepsilon}
\IE\Big[\left.\big(\rho_\epsilon*\eta_t(x)\big)^k\right| {\cal F}_u\Big]
\leq C\IE\Big[\langle\psi_{t-u}^{\epsilon,x}(z), \eta_u(dz)\rangle^k\Big]
+C\frac{\theta}{N\epsilon^d}
\IE\Big[\langle\psi_{t-u}^{\epsilon,x}(z), \eta_u(dz)\rangle\Big];
\end{equation} 
and
\begin{multline}
\label{flandoli trick for rhoepsilon}
\IE\Big[\left. \int_u^t\langle\psi_{t-s}^{\epsilon,x}(z),\eta_s(dz)\rangle^{k-1}
\big\langle\psi_{t-s}^{\epsilon,x}(z)|F(\rho_\epsilon*\eta_s(z))|,
\eta_s(dz)\big\rangle ds\right| {\cal F}_u\Big]
\\
\leq\widetilde{C}\IE\Big[\langle\psi_{t-u}^{\epsilon,x}(z), \eta_u(dz)\rangle^k\Big]
+\widetilde{C}\frac{\theta}{N\epsilon^d}
\IE\Big[\langle\psi_{t-u}^{\epsilon,x}(z), \eta_u(dz)\rangle\Big];
\end{multline}
where the function $\psi^{\epsilon,x}_t(\cdot)$ was
defined in~(\ref{AlmostHeatEquation}).
In particular, %since $\IE[\langle 1,\eta_0\rangle^k\]$ is finite, 
under the assumptions of Theorem~\ref{thm:local_convergence}, the
quantities on the right hand side of~(\ref{moment bound rhoepsilon})
and~(\ref{flandoli trick for rhoepsilon}) are both integrable 
with respect to Lebesgue measure.  
\end{lemma}
\begin{proof}
To simplify our expressions, we shall consider the case $u=0$, but
the proof goes through unchanged for other values of $u$.

The proof proceeds by induction. 

For any time-dependent function $\phi_t(x)$
with time derivative $\dot \phi_t(x) = \partial_t \phi_t(x)$,
\begin{align*}
%        \label{timeMGP}
    \langle \phi_0(x), \eta_t(dx) \rangle
    &= %\nonumber 
    \langle \phi_t(x), \eta_0(dx) \rangle
    +
    M_t(\phi_\cdot(\cdot))
    +
    \int_0^t \langle \mathcal{L}^\theta \phi_{t-s}(x) - \dot \phi_{t-s}(x), \eta_s(dx) \rangle ds
    \\ & {}
    +
    \int_{0}^t \langle \phi_{t-s}(x) F(x, \eta_s) , \eta_s(dx) \rangle ds ,
\end{align*}
where $M_t(\phi_\cdot(\cdot))$ is a martingale. 
Taking $\phi_t(\cdot)=\psi_t^{\epsilon,x}(\cdot)$,
\begin{eqnarray}
\nonumber
\rho_\epsilon*\eta_t(x)&=&\langle\psi^{\epsilon,x}_0(y),\eta_t(dy)\rangle
\\
&=&\langle\psi^{\epsilon,x}_t(y),\eta_0(dy)\rangle
+\int_0^t\langle\psi^{\epsilon,x}_{t-s}(y)F\big(\rho_\epsilon*\eta_s(y)\big), \eta_s(dy)\rangle ds
+M_t(x),
\label{expn rhoepsilon}
\end{eqnarray}
where $M_t(x)$ is a martingale. 
% DON'T DELETE THIS - I'LL NEED IT LATER
%\begin{equation}
%\label{qv M(x)}
%[M(x)]_t=\frac{\theta}{N}\int_0^t\Big\langle\int\psi^{\epsilon,x}_{t-s}(z)^2q_\theta(y,dz)
%+\Big(1-\frac{F(\rho_\epsilon*\eta(y)}{\theta}\Big)\psi^{\epsilon,x}_{t-s}(y)^2,\eta_s(dy)
%\Big\rangle.
%\end{equation}
Taking expectations in~(\ref{expn rhoepsilon}), using that $F$ is bounded above, and 
applying Gronwall's inequality, we obtain~(\ref{moment bound rhoepsilon})
in the case $k=1$. 
Moreover, rearranging~(\ref{expn rhoepsilon}) we find 
\begin{equation}
-\int_0^t\langle\psi^{\epsilon,x}_{t-s}(y)F(\rho_\epsilon*\eta_s(y), \eta_s(dy)\rangle ds
=\langle\psi^{\epsilon,x}_t(y),\eta_0(dy)\rangle
-\langle \psi^{\epsilon,x}_0(y),\eta_t(dy)\rangle +M_t(x),
\end{equation} 
and taking expectations again this yields 
$$\IE\Big[-\int_0^t\langle\psi^{\epsilon,x}_{t-s}(y)F(\rho_\epsilon*\eta_s(y), 
\eta_s(dy)\rangle ds\Big]
\leq \widehat{C}\IE\big[\langle\psi^{\epsilon, x}_t(y),\eta_0(dy)\rangle\big].$$
Since $F$ is bounded above, there exists a constant $K$ such that $|F|\leq K-F$ and so 
combined with the bound on $\IE[\langle\psi^{\epsilon, x}_0(y),\eta_t(dy)\rangle]$ just
obtained, this in turn yields 
$$\IE\Big[\int_0^t\langle\psi^{\epsilon,x}_{t-s}(y)|F(\rho_\epsilon*\eta_s(y)|, 
\eta_s(dy)\rangle ds\Big]\leq 
\widetilde{C}\IE\big[\langle\psi^{\epsilon, x}_t(y),\eta_0(dy)\rangle\big],$$
which is~(\ref{flandoli trick for rhoepsilon})
in the case $k=1$.

Now suppose that we have established~(\ref{moment bound rhoepsilon}) 
and~(\ref{flandoli trick for rhoepsilon}) for all exponents $j<k$.
First we calculate the generator $\Pgen^N$ of our scaled population process applied
to functions of the form $\langle f,\eta\rangle^k$.
Recalling that each jump of the process involves the birth or death or a single 
individual, and so increments $\langle f,\eta\rangle$ by $\pm f/N$ at the location of
that individual, we find 
\begin{multline}
\label{pgen applied to kth moment}
\Pgen^N\Big(\langle f,\eta\rangle^k\Big)
=
\Big\langle
\int\theta N\Big\{k\frac{f(y)}{N}\langle f,\eta\rangle^{k-1}+
\binom{k}{2}\frac{f(y)^2}{N^2}\langle f,\eta\rangle^{k-2}+\cdots
+\frac{f(y)^{k}}{N^{k}}\Big\}q_\theta(x,dy),\eta(dx)\Big\rangle
\\
+
\Big\langle\theta N\Big(1-\frac{F(\rho_\epsilon*\eta(x))}{\theta}\Big)
\Big\{-k\frac{f(x)}{N}\langle f,\eta\rangle^{k-1}+
\binom{k}{2}\frac{f(x)^2}{N^2}\langle f,\eta\rangle^{k-2}+\cdots
+\frac{(-1)^{k}f(x)^{k}}{N^{k}}\Big\},\eta(dx)\Big\rangle.
\end{multline}
Mimicking what we did above, we set $f(\cdot)=\psi^{\epsilon,x}_t(\cdot)$ and 
write
\begin{multline}
\label{kth moment for time varying function}
\IE\Big[\langle \psi^{\epsilon,x}_0,\eta_t\rangle^k\Big]
=\langle\psi^{\epsilon, x}_t(y),\eta_0(dy)\rangle^k
+\IE\Big[\int_0^t\Pgen^N\big(\langle\psi^{\epsilon,x}_{t-s}(y)\big),\eta_s(dy)\rangle ds
\\
-\int_0^t\langle k\dot{\psi}^{\epsilon,x}_{t-s}(y), \eta_s(dy)\rangle
\langle \psi^{\epsilon, x}_{t-s}(y),\eta_s(dy)\rangle^{k-1}ds\Big].
\end{multline}
Using that $\sup_s\|\psi^{\epsilon,x}_s(\cdot)\|_\infty=C/\epsilon^d$,
$N\epsilon^d>1$,
%$\langle\psi^{\epsilon, x}_t(y),\eta_0(dy)\rangle^{j}
%\leq C\langle\psi^{\epsilon, x}_{t}(y),\eta_0(dy)\rangle$, 
and our inductive
hypothesis, we find
\begin{multline*}
\IE\Big[
\Big\langle
\int_0^t\theta N\Big\{\sum_{j=2}^{k}\binom{k}{j}\frac{\psi_{t-s}^{\epsilon,x}(y)^j}{N^j}\langle \psi^{\epsilon,x}_{t-s},\eta_s\rangle^{k-j}
\Big(1+(-1)^j\Big(1-\frac{F(\rho_\epsilon*\eta_s(y))}{\theta}\Big)\Big),
\eta_s(dy)\Big\rangle ds\Big]
\\
\leq 
C\IE\Big[\Big\langle\int_0^t\sum_{j=2}^k\frac{\theta}{N\epsilon^d}
\Big(\frac{1}{(N\epsilon^d)^{j-2}}\Big)
\Big\langle
\Big(2+\frac{|F(\rho_\epsilon*\eta_s(y))|}{\theta}\Big),\eta_s(dy)\Big\rangle
\langle\psi^{\epsilon,x}_{t-s},\eta_s\rangle^{k-j}ds\Big]
\\
\leq
C'\frac{\theta}{N\epsilon^d}\sum_{j=1}^{k-1}
\IE\big[\langle\psi^{\epsilon, x}_t(y),\eta_0(dy)\rangle^j\big]
\leq
C''\frac{\theta}{N\epsilon^d}
\Big(\IE\big[\langle\psi^{\epsilon, x}_t(y),\eta_0(dy)\rangle^k\big]
+\IE\big[\langle\psi^{\epsilon, x}_t(y),\eta_0(dy)\rangle\big]\Big).
\end{multline*}
Combining this with~(\ref{pgen applied to kth moment})
and~(\ref{kth moment for time varying function}), using once again the fact that $F$
is bounded above, we find 
\begin{multline*}
\label{kth step of induction}
\IE\Big[\langle \psi^{\epsilon,x}_0,\eta_t\rangle^k\Big]
\leq
\IE\big[\langle\psi^{\epsilon, x}_t(y),\eta_0(dy)\rangle^k\big]
+\tilde{C}\IE\Big[\int_0^t
\langle\psi^{\epsilon, x}_{t-s}(y),\eta_s(dy)\rangle^k ds\Big]
\\
+C''\frac{\theta}{N\epsilon^d}
\Big(\IE\big[\langle\psi^{\epsilon, x}_t(y),\eta_0(dy)\rangle^k\big]+
\IE\big[\langle\psi^{\epsilon, x}_t(y),\eta_0(dy)\rangle\big]\Big)
%\langle\psi^{\epsilon, x}_t(y),\eta_0(dy)\rangle^j
,
\end{multline*}
and~(\ref{moment bound rhoepsilon}) 
follows from Gronwall's inequality. Rearranging exactly as in the case $k=1$, we 
recover~(\ref{flandoli trick for rhoepsilon}) 
and the inductive step is complete.
\end{proof}

We shall need the following consequence of the bounds that we obtained in 
Lemma~\ref{bounds on moments}:
\begin{corollary}
Under the assumptions of Lemma~\ref{bounds on moments},
for each $k\geq 1$,
\begin{equation}
\label{double integrals with respect to eta}
\IE\Big[\big\langle (\rho_\epsilon*\eta_t)^{k},\eta_t\big\rangle \Big]<C(k)<\infty.
\end{equation}
\end{corollary}
\begin{proof}[Sketch]
First observe that if $A\in (0,1)$, then 
\begin{equation}
\label{heat equation at smaller time}
p_{A\epsilon^2}(x,y)
=\frac{1}{A^{d/2}}p_{\epsilon^2}(x,y)\exp\big(-\frac{\|x-y\|^2}{2\epsilon^2}
\left(\frac{1}{A}-1\right)\big)\leq \frac{1}{A^{d/2}}p_{\epsilon^2}(x,y).
\end{equation}
Now consider
    \begin{align*}
\IE\big[\langle \rho_\epsilon*\eta_t(x),\eta_t(dx)\rangle\big]&=
\IE\big[\int\int p_{\epsilon^2}(x,z)\eta_t(dz)\eta_t(dx)\big]
\\        
&=
\IE\big[\int\int\int p_{\epsilon^2/2}(x,y) p_{\epsilon^2/2}(y,z)dy\eta_t(dz)\eta_t(dx)\big]
       % \langle p_{\epsilon^2/2} * p_{\epsilon^2/2} * \eta_t(x), \eta(dx) \rangle
        \\ &=
       \IE\Big[ \int \left( p_{\epsilon^2/2} * \eta_t(y) \right)^2 dy \Big]
\\&\leq C\int \IE\big[\big(\rho_\epsilon*\eta_t(x)\big)^2\big] dx,
    \end{align*}
where we used~(\ref{heat equation at smaller time}) in the last line.
Using Lemma~\ref{bounds on moments}
and our assumptions on $\eta_0$, this quantity is finite.

To illustrate the inductive step, now consider
\begin{multline}
\label{rewrite for inductive step}
\IE\big[\langle \rho_\epsilon*\eta_t(x)^2,\eta_t(dx)\rangle\big] 
=
\IE\Big[\int\int\int p_{\epsilon^2}(x,z_1) p_{\epsilon^2}(x,z_2)
\eta_t(dz_1)\eta_t(dz_2)\eta_t(dx)\Big]
\\
=
\IE\Big[\int\cdots\int p_{\epsilon^2/2}(x,y_1) 
p_{\epsilon^2/2}(x,y_2)p_{\epsilon^2/2}(y_1,z_1)
p_{\epsilon^2/2}(y_2,z_2)
\eta_t(dz_1)\eta_t(dz_2)dy_1dy_2\eta_t(dx)\Big].
\end{multline}
We use the identity
\[
p_{\epsilon^2/2}(x,y_1) p_{\epsilon^2/2}(x,y_2)
=p_{\epsilon^2}(y_1,y_2)p_{\epsilon^2/4}\Big(x, \frac{y_1+y_2}{2}\Big)
\]
to rewrite~(\ref{rewrite for inductive step}) as
\begin{align*}
&\IE\Big[\int p_{\epsilon^2/2}*\eta_t(y_1) p_{\epsilon^2/2}*\eta_t(y_2)
p_{\epsilon^2/4}*\eta_y\big(\frac{y_1+y_2}{2}\big)p_{\epsilon^2}(y_1,y_2)dy_1dy_2\Big]
\\
&\leq 
\IE\Big[\int\Big\{
\big(p_{\epsilon^2/2}*\eta_t(y_1)\big)^3+ \big(p_{\epsilon^2/2}*\eta_t(y_2)\big)^3+
\big(p_{\epsilon^2/4}*\eta_t\big(\frac{y_1+y_2}{2}\big)\big)^3
\Big\}p_{\epsilon^2}(y_1,y_2)dy_1dy_2\Big],
\end{align*}
where we have used that for any non-negative real numbers
$\beta_1$, $\beta_2$, $\beta_3$, 
$\beta_1\beta_2\beta_3\leq \beta_1^3+\beta_2^3+\beta_3^3$.
For the first two terms in the sum we integrate with respect to $y_2$ and $y_1$
respectively to reduce to an expression of the form considered in 
Lemma~\ref{bounds on moments}. For the final term, the change of variables
$z_1=y_1+y_2$, $z_2=y_1-y_2$ in the integral similarly 
allows us to integrate out the heat kernel, and we conclude that the result
holds for $k=2$.

We can proceed in the same way for larger values of $k$, using repeatedly that
\[
p_{t_1}(x,y_1)p_{t_2}(x,y_2)=p_{\frac{t_1t_2}{t_1+t_2}}
\Big(x,\frac{t_2y_1+t_1y_2}{t_1+t_2}\Big)p_{t_1+t_2}(y_1,y_2)
\]
to write 
\[
\prod_{j=1}^k p_{\tau}(x,x_j)
=\prod_{j=2}^{k}p_{\frac{j\tau}{j-1}}\big(x_j,X_{j-1}\big)
p_{\frac{\tau}{k}}(x, X_k)
\]
where
\[
X_1=x_1, \qquad X_j=\frac{j-1}{j}x_j+\frac{1}{j}X_{j-1}, \mbox{for }j\geq 2.
\]
This yields
\begin{multline*}
\big\langle\big(\rho_\epsilon*\eta_t(x)\big)^k,\eta_t(dx)\big\rangle
=\int\cdots\int\prod_{i=2}^kp_{\epsilon^2 i/(i-1)}(y_,X_i)
\prod_{i=1}^kp_{\epsilon^2/2}*\eta_t(y_i)p_{\epsilon^2/k}*\eta_t(X_k)
dy_1\ldots dy_k
\\
\leq
\int\cdots\int 
\prod_{i=2}^kp_{\epsilon^2 i/(i-1)}(y_i,X_i)
\Big\{\sum_{i=1}^k\big(p_{\epsilon^2/2}*\eta_t(y_i)\big)^{k+1}
+\big(p_{\epsilon^2/k}*\eta_t(X_k)\big)^{k+1}\Big\}dy_1\ldots dy_k,
\end{multline*}
and once again we can change variables in the integrals and 
use~(\ref{heat equation at smaller time}) to bound this by a constant 
multiple of $\int\IE\big[\big(\rho_\epsilon*\eta_t(x)\big)^{k+1}\big]dx$, and
the inductive step is complete.
\end{proof}

\begin{corollary}[$C$-tightness of $\{(\rho_\epsilon*\eta_t(x) dx)_{t\geq 0}\}$]
        \label{lem:density_tightness}
    Under the assumptions of Theorem~\ref{TeoremOneStepConvergence}
    the sequence of measure valued processes $\{ \rho*\eta_t(x) dx \}_{t \geq 0}$
    (taking values in $D([0,T], \measures)$ is $C$-tight.
\end{corollary}
\begin{proof}
Since the quadratic variation of the martingale $M_t(x)$ of 
equation~(\ref{expn rhoepsilon}) is
\begin{equation*}
%\label{qv M(x)}
[M(x)]_t=\frac{\theta}{N}\int_0^t\Big\langle\int\psi^{\epsilon,x}_{t-s}(z)^2q_\theta(y,dz)
+\Big(1-\frac{F(\rho_\epsilon*\eta(y)}{\theta}\Big)\psi^{\epsilon,x}_{t-s}(y)^2,\eta_s(dy)
\Big\rangle ds,
\end{equation*}
in exactly the same way as we proved that $\IE[\sup_{0\leq t\leq T} \langle 1,\eta_t\rangle]$ is 
bounded in Lemma~\ref{lem:eta_compact_containment}, 
using~(\ref{moment bound rhoepsilon}) 
and~(\ref{flandoli trick for rhoepsilon}) with $k=1$, 
we can deduce that $\IE[\sup_{0\leq t\leq T}M_t(x)]$ is bounded independently of
$N$, $\theta$ and $\epsilon$ (in fact that it tends to zero) and
by an application of Gronwall's inequality we deduce 
that $\IE[\sup_{0\leq t\leq T} \rho_\epsilon*\eta_t(x)]$ is
bounded. This proves compact containment.  

    To conclude $C$-tightness we show that, for $T>0$, and any
$f\in C_b^\infty (\IR^d)$ with bounded second derivatives, given $\nu>0$,
    there exists $\delta=\delta(f,T,\nu) >0$ so that for $\theta$ and $N$ large enough,
    \begin{align}
        \IP\Big[
            \sup_{s,t \in [0,T], |s-t| < \delta}
            \left|
                \langle \rho_{\epsilon} * f, \eta_s \rangle
                - \langle \rho_{\epsilon} * f, \eta_t \rangle
            \right|
            >
            \nu
        \Big]
        <
        \nu \label{conditionCtightness}
    \end{align}

First, observe that
\begin{multline}
\label{C-tightness estimate}
\left|
\langle \rho_{\epsilon} * f, \eta_t \rangle 
- \langle \rho_{\epsilon} * f, \eta_s \rangle            
\right|
\leq
\left|
\int_s^t\Big\langle\theta\int\big(\rho_\epsilon*f(y)-\rho_\epsilon*f(x)\big)
q_{\theta}(x,dy),\eta_u(dx)\Big\rangle du
\right|
\\
+\int_s^t\Big\langle |F\big(\rho_\epsilon*\eta_u(x)\big)|\rho_\epsilon*f(x), 
\eta_u(dx)\Big\rangle du +2\sup_{0\leq u\leq T}|\widehat{M}^N(f)_u|,
\end{multline}
where $\widehat{M}(f)$ is the martingale of~(\ref{eqn:eta_martingale})
with the test function $f$ replaced by $\rho_\epsilon*f$. In particular, 
just as for $M_t(x)$ above, we can deduce that its supremum tends to zero under
our scaling.

Again using the approach of Lemma~\ref{lem:eta_compact_containment}, we have that 
$\IE[\sup_{0\leq t\leq T} \langle 1,\eta_t\rangle]$ is bounded,
% and using that
%$|F|\leq K-F$ and rearranging the equation for $\langle 1,\eta_t\rangle>$ in the usual
%way to find an expression for 
%$-\int_s^t\langle F\big(\rho_\epsilon*\eta_u(x)\big), \eta_u(dx)\rangle du$, we can bound
and so combining with the corresponding estimate for $\rho_\epsilon*\eta_u$, 
by Markov's inequality, we can choose $m_\nu$, $\widetilde{m}_\nu$, so that 
$$\IP[\sup_{0\leq u\leq T}\langle<1,\eta> >\tilde{m}_\nu]<\nu/2$$
$$\IP[\sup_{0\leq u\leq T}\langle<1,\eta> >\tilde{m}_\nu]<\nu/2$$
UP TO HERE

 
By the Intermediate Value Theorem, using $T_t$ to denote the heat semigroup,
there exists $s\in (0,1/\theta)$ such that
\begin{multline*}
\left|
\theta\int\big(\rho_\epsilon*f(y)-\rho_\epsilon*f(x)\big)
q_{\theta}(x,dy) \right|
=
\left|\theta\Big(T_{\epsilon^2+1/\theta}f(x)-T_{\epsilon^2}f(x)\Big)\right|
\\
=
\left|\partial_sT_{\epsilon^2+s}f(x)\right|=\left|T_{\epsilon^2+s}\Delta f(x)\right|
\leq\|\Delta f\|_\infty.
\end{multline*}

    Now, we can use \eqref{MGPT_epsilon} to write this difference
    as an integral, then bound the terms as above to get
    \begin{align*}
        \left|
        \langle \rho_{\epsilon} * f, \eta_s \rangle
        -
        \langle \rho_{\epsilon} * f, \eta_t \rangle
        \right|
        \leq
        ( \| \Delta f \|_\infty + C_F \|f\|_\infty) |s - t|
        \sup_{0 \leq u \leq T } \langle 1, \eta_u \rangle
        +
        \sup_{u \leq T} |\widehat{M}_u| .
    \end{align*}
    Taking expectations we get,
    \begin{equation}
        \IE\left[ \left|
            \langle \rho_\epsilon * f, \eta_s \rangle
            -
            \langle \rho_\epsilon * f, \eta_t \rangle
        \right| \right]
        \leq
        \widehat{C} |s-t|
        +
        \widehat{C} \left( \frac{\theta}{N} T \right)^{1/2} .
    \end{equation}
    Using this and Markov's inequality give that \eqref{conditionCtightness}
    holds with $\delta = \alpha/(3 \widehat{C}|s-t|)$,
    for $N$, $\theta$ big enough so $\widehat{C}\sqrt{T\theta/N}$ is bounded by $\alpha/3$.
    (Note the last condition is possible by \eqref{ConditionsForConvergence}.)
    Thus we have concluded $C$-tightness. 
\end{proof}

\comment{
    Peter writes: the following Remark refers to the proof of Lemma~\ref{lem:SecondMomentMass},
    which was previously wrong and is now fairly different.
    However, I don't see how to prove the Remark using any similar method,
    because to bound $\int p_{\epsilon^2 / 2} * \eta_t(x)^2 dx$
    it seems like we need control on $\rho_\epsilon * \eta_t(x)$ uniformly in $x$
    (and, integrability), which I don't think we have?
    But, I'm probably being dense.
    See after this for an attempted rewrite.
}

\begin{remark} \label{remark:BoundedIntegral}
    Similar as we got equation (\ref{eq:boundedIntegralAgainstf}), from equation \ref{MGPT_epsilon} we can conclude there exists a $C < \infty$ such that
    \[
        \IE\left[\sup_{0 \leq t \leq T} \int p_{\epsilon^2/2} * \eta_t(x) dx \right] < C .
    \]
    By using this and the same pattern used to prove Lemma~\ref{lem:SecondMomentMass},
    we can then conclude there is a (different) $C < \infty$ such that
    \[
        \IE\left[  \sup_{0 \leq t \leq T} \int \left( p_{\epsilon^2/2} * \eta_t(x) \right)^2 dx \right]
        <
        C .
    \]
    Then, because
    \begin{align*}
        \langle p_{\epsilon^2} * \eta_t(x), \eta(dx) \rangle
        &=
        \langle p_{\epsilon^2/2} * p_{\epsilon^2/2} * \eta_t(x), \eta(dx) \rangle
        \\ &=
        \int_{\IR^d} \left( p_{\epsilon^2/2} * \eta_t(x) \right)^2 dx .
    \end{align*}
    we can conclude that
    $\IE[\langle \rho_\epsilon*\eta_t(x), \eta_t(dx) \rangle ]$ is uniformly bounded,
    which will be useful in the next section.
\end{remark}

\comment{Here's my start at rewriting that Remark;
    however, I'm stuck since $\theta(p_{\epsilon^2 + 1/\theta} - p_{\epsilon^2}) * \eta$
    isn't bounded by $p_{\epsilon^2} * \eta$ (uniformly in $\epsilon$, anyhow).
}

\begin{remark} \label{remark:BoundedIntegral2}
    Below, we will want to bound
    \[
        \langle \rho_\epsilon * \eta_t, \eta_t \rangle
        =
        \int_{\IR^d} \int_{\IR^d} p_{\epsilon^2}(x, y) \eta_t(dx) \eta_t(dy) .
    \]
    Writing $H(\eta) = \langle \rho_\epsilon * \eta, \eta \rangle$,
    note that since $p_{\epsilon^2}$ is symmetric,
    $H(\eta + \delta_z / N) = H(\eta) + 2 \rho_\epsilon * \eta(z) / N + p_{\epsilon^2}(z, z) / N^2$,
    so following Section~\ref{sec:heuristics},
    \begin{align*}
        \langle \rho_\epsilon * \eta_t, \eta_t \rangle
        &=
        \langle \rho_\epsilon * \eta_0, \eta_0 \rangle
        +
        2 \theta \int_0^t \left \langle \int_{\IR^d}
            \left(
               \rho_\epsilon * \eta_s(y)
               -
               \rho_\epsilon * \eta_s(x)
            \right)
            q_\theta(x, dy)
        , \eta_s(dy) \right \rangle ds
        \\ & \qquad {}
        +
        2 \int_0^t
        \langle
            \rho_\epsilon * \eta_s(x) F(x, \eta_s)
        , \eta_s(dx) \rangle
        ds
        +
        O\left( \frac{1}{\epsilon^d N^2} \right)
        + M_t ,
    \end{align*}
    where $M_t$ is a martingale with quadratic variation
    \begin{align*}
        [M]_t
        &=
        4 \frac{\theta}{N} \int_0^t
        \left \langle
            \int_{\IR^d}
                \rho_\epsilon * \eta_s(y)^2
            q_\theta(x, dy)
        , \eta_s(dx) \right\rangle
        ds
        \\ & \qquad {}
        +
        4 \int_0^t
        \left \langle
            \rho_\epsilon * \eta_s(x)^2 (1 + F(x, \eta_s) / \theta) 
        , \eta_s(dx) \right\rangle
        ds
        +
        O\left( \frac{1}{\epsilon^d N^2} \right) .
    \end{align*}
\end{remark}

\subsubsection{Identifying the limit}

\paragraph{Continuity estimate}

In this section we prove the following continuity estimate for $\rho_\epsilon * \eta_t(x)$.
\begin{proposition} \label{ContinuityEstimate}
Under the same conditions of Theorem~\ref{TeoremOneStepConvergence} we have that for all $x \in \IR^d$, $\delta>0$,
\[
\int |\rho_\epsilon*\eta_s(x) -\rho_\epsilon*\eta_s(y)| \rho_\delta(x-y) dy \]
is bounded by
\begin{align*}
\frac{\delta}{\sqrt{\epsilon^2+s}} \langle 1,\eta_0(dz) \rangle &+ \int_0^s \frac{\delta}{\sqrt{\epsilon^2+s-r}} (\langle 1, \eta_r(dz)\rangle + \langle \rho_\epsilon*\eta_r(z), \eta_r(dz)\rangle)dr \\ &+ |M_s(x)| + \int |M_s(y)| \rho_\delta(y-x)dy + \mathcal{O}\left( \frac{1}{\epsilon^2 \theta}\right)
\end{align*} 
\end{proposition}
where $M_s(x)$ is the martingale in (\ref{MartingaleProblemMolifier}).
\begin{proof}
Recall from our assumptions that $|F'(x)1_{x>0}|$ is uniformly bounded. Hence, we can always write,
\begin{align} \label{StrongBoundOnF}
|F(\rho_\epsilon*\eta_t(x))| \leq |F(0)| + C \rho_\epsilon*\eta_t(x)
\end{align}
So, by (\ref{MartingaleProblemMolifier}) we can write,
\begin{align}
& |\rho_\epsilon*\eta_t(x)-\rho_\epsilon*\eta_t(y)| \nonumber \\ &= |\langle \rho_\epsilon(y-z)-\rho_\epsilon(x-z),\eta_t(dz) \rangle | \nonumber\\ &\leq  |M_t(x)-M_t(y)|  +\langle \psi_t^{\epsilon,y}(z)-\psi_t^{\epsilon,x}(z), \eta_0(dz)\rangle \nonumber \\ &+ C\int_0^t \langle |\psi_{t-s}^{\epsilon,x}(z) - \psi_{t-s}^{\epsilon, y}(z) |, \eta_s(dz) \rangle  ds | \\ &+ C \int_0^t \langle |\psi_{t-s}^{\epsilon,x}(z) - \psi_{t-s}^{\epsilon, y}(z) | \rho_\epsilon*\eta(z), \eta_s(dz) \rangle  ds | \label{eq:bound1CE} \end{align} 
By using Lemma~\ref{Lemma:BoundPsiHS2} we can then write, 
\begin{align*}
 \int &|\rho_\epsilon*\eta_s(x) -\rho_\epsilon*\eta_s(y)| \rho_\delta(x-y) dy  \leq \int |p_{\epsilon^2+s}(y,z)-p_{\epsilon^2+s}(x,z)|\eta_0(dz) \rho_\delta(x-y)dy \\&+ C\int_0^s \langle p_{\epsilon^2+s-r}(y,z)-p_{\epsilon^2+s-r}(x,z)|, \eta_r(dz) \rangle \rho_\delta (x-y) dr  \\  & + C \int_0^s \langle |p_{\epsilon+s-r}(y,z) - p_{\epsilon+s-r}(x,z) | \rho_\epsilon*\eta_r(z), \eta_r(dz) \rangle  dr | + \int|M_s(y)-M_s(x)| \rho_\delta(y-x)dy  \\  & + \frac{C}{\epsilon^2 \theta} \int \langle \rho_\epsilon*\eta(z),\eta_s(dz)\rangle ds   +\mathcal{O}\left( \frac{1}{\epsilon^2 \theta} \right) 
\end{align*}
By Remark~\ref{remark:BoundedIntegral} the last integral term is of order $(\epsilon^2 \theta)^{-1}$. By using Lemma~\ref{Lemma:ContinuityHS} we get that the contribution of the first three terms is bounded by,
\begin{align*}
\int &\frac{|y-x|}{\sqrt{\epsilon^2+s}} \rho_{\delta}(y-x) dy \langle 1, \eta_0(dz)\rangle + \int \int_0^s  \langle \frac{|y-x|}{\sqrt{\sqrt{\epsilon^2+s-r}}}, \eta_r(dz) \rangle \rho_\delta(x-y) dy dr \\ & +  \int \int_0^s \langle \frac{|y-x| \rho_\epsilon*\eta(z)}{\sqrt{\epsilon^2+s-r}}, \eta_r(dz) \rangle \rho_\delta(y-x) dy dr \\ & \leq \frac{\delta}{\sqrt{\epsilon^2+s}} \langle 1, \eta_0(dz) \rangle + \int_0^s \frac{\delta}{\sqrt{\epsilon^2+s-r}} (\langle 1, \eta_r(dz) \rangle + \langle \rho_\epsilon*\eta_r(z), \eta_r(dz) \rangle) dr.
\end{align*}
To finish the proof just notice that,
\begin{align*}
\int |M_s(y)-M_s(x)| \rho_\delta(y-x)dy \leq |M_s(x)| + \int |M_s(y)| \rho_\delta(y-x)dy.
\end{align*}
\end{proof}

\paragraph{Identifying the limit of $\langle T_{\epsilon^2} f(y), \eta_t (dy) \rangle$.}
We now prove the identification of the limit. We recall that, the main difficulty, is the non-linear term of the martingale problem. To deal with them, our first step would prove that an approximation of the form:
\begin{align*}
\langle T_{\epsilon^2} f(y) F(\rho_\epsilon *\eta_s(dy)), \eta_s(dy) \rangle \sim \langle T_{\epsilon^2} f(y) F(\rho_\epsilon * \eta_s(dy)), \rho_\epsilon * \eta_s(y) dy \rangle
\end{align*}
This is the content of the next result.
\begin{proposition} \label{CuadraticApprox}
Under the condition of Theorem~\ref{TeoremOneStepConvergence} there is a constant $C>0$ such that
\begin{equation*}
\int_0^t \IE[|\langle T_{\epsilon^2} f(y) F(\rho_\epsilon *\eta_s(dy)), \eta_s(dy) \rangle - \langle T_{\epsilon^2} f(y) F(\rho_\epsilon * \eta_s(dy)), \rho_\epsilon * \eta_s(y) dy \rangle|]
\end{equation*}
is bounded by,
\begin{equation*}
\mathcal{O}\left( \frac{1}{\epsilon^2 \theta} + \epsilon^2 + \epsilon + \epsilon^{1/3}  + \left( \frac{\theta}{N \epsilon^d} \right)^{1/2}\right)
\end{equation*}
\end{proposition}
\begin{proof}
We first note that,
\begin{align*}
    &\langle T_{\epsilon^2} f(y) F(\rho_\epsilon *\eta_s(dy)), \eta_s(dy) \rangle - \langle T_{\epsilon^2} f(y) F(\rho_\epsilon * \eta_s(dy)), \rho_\epsilon * \eta_s(y) dy \rangle \\ & = \langle \int T_{\epsilon^2} f(y) F(\rho_\epsilon * \eta_s(y))\rho_\epsilon(y-w) dy, \eta_s(dw)\rangle - \langle T_{\epsilon^2} f(w) F(\rho_\epsilon*\eta_s(w)), \eta_s(dw) \rangle \\ &= \langle \int \{ T_{\epsilon^2} f(y) F(\rho_\epsilon*\eta_s(y))-T_{\epsilon^2}f(w) F(\rho_\epsilon*\eta_s(w))\} \rho_\epsilon(w-y)dy, \eta_s(dw) \rangle.
\end{align*}
Let us denote $I$ the integral against $dy$ in the last expresion, that is 
\[ I :=  \int \{ T_{\epsilon^2} f(y) F(\rho_\epsilon*\eta_s(y))-T_{\epsilon^2}f(w) F(\rho_\epsilon*\eta_s(w))\} \rho_\epsilon(w-y)dy \]
And note that $|I|$ is bounded by
\begin{align}
& \int \left\{ |F(\rho_\epsilon*\eta_s(y))-F(\rho_\epsilon*\eta_s(w))|T_{\epsilon^2}f(y)+F(\rho_\epsilon*\eta_s(w))|T_{\epsilon^2}f(y)-T_{\epsilon^2}f(w))|\right\} \rho_{\epsilon}(w-y)dy \nonumber \\ &\leq \int C |\rho_\epsilon*\eta_s(y)-\rho_\epsilon*\eta_s(w)| \rho_\epsilon(w-y)dy + C \epsilon^2 \Vert \Delta f \Vert_2 \label{FirstBoundCT}
\end{align}
where in the last inequality we have used $F(x)1_{x>0}$ and $|F'(x)1_{x>0}|$ are both bounded above. We want to use Proposition~\ref{ContinuityEstimate} in the last equation, but the terms that we would obtain would be too hard to control unless we first control the total mass and their noise. Let us define,
\begin{align*}
A_{t} &:= \left\{ \sup_{0 \leq s \leq t} \langle 1, \eta_s(dz) \rangle > \epsilon^{-2/3} \right\}. \end{align*}
Note that, by Markov's inequality and Lemma~\ref{Lemma:UniformlyBoundedMass} the event $A_{t}$ has probability of order $\epsilon^{2/3}$. Hence, writing $I = I(1_{A^c_{t}} + 1_{A_{t}})$ and proceeding as before we can use Proposition~\ref{ContinuityEstimate} in (\ref{FirstBoundCT}) to get,
\begin{align}
&\int_0^t \IE[|\langle T_{\epsilon^2} f(y) F(\rho_\epsilon *\eta_s(dy)), \eta_s(dy) \rangle - \langle T_{\epsilon^2} f(y) F(\rho_\epsilon * \eta_s(dy)), \rho_\epsilon * \eta_s(y) dy \rangle|] \nonumber \\ &\leq   \int_0^t \IE[ \frac{\epsilon}{\sqrt{\epsilon^2 + s}} \langle 1,\eta_0(dz) \rangle \langle 1, \eta_s(dz)\rangle ] ds +  \int_0^t \IE[  \int_0^s \frac{\epsilon}{\sqrt{\epsilon^2 + s - r}} \langle 1, \eta_r (dz) \rangle dr \langle 1, \eta_s(dz) \rangle  ] ds \nonumber\\ & + \int_0^t \IE[   1_{A^c_{t}} \int_0^s \frac{\epsilon}{\sqrt{\epsilon^2 + s - r}} \langle \rho_\epsilon*\eta_r(z), \eta_r(dz) \rangle dr \langle 1, \eta_s(dz) \rangle  ] ds + \int_0^t \IE[\langle |M_s(x)|,\eta_s(dx) \rangle] ds \nonumber \\ &+ \int_0^t \int \IE[|M_s(y)|\rho_\epsilon*\eta_s(y)] dy ds  +\mathcal{O}\left( \frac{1}{\epsilon^2 \theta} + \epsilon^2 + \epsilon^{1/3} \right)  \label{ThingsToBound}
\end{align}
Note we have bounded $1_{A^c_{t}}$ by one in all the terms expect the third one, which is the one that will require it. For the first two term we can use our hypothesis on the initial condition and that $2 ab \leq a^2+b^2$ to get that their contribution is bounded by,
\begin{align*}
\int_0^t C \frac{\epsilon}{\sqrt{\epsilon^2+s}}\IE[\langle 1, \eta_s(dz) \rangle] ds + \int_0^t \int_0^s \frac{\epsilon}{\sqrt{\epsilon^2+s-r}}\IE[\langle 1, \eta_r(dz)\rangle^2 + \langle 1, \eta_s(dz)\rangle^2] ds,
\end{align*}
which, due to Lemma~\ref{lem:SecondMomentMass}, is of order $\epsilon$. To bound the third integral term we use the event $A_{t}$ to get it is bounded by,
\begin{align*}
\int_0^t \int_0^s \frac{\epsilon^{1/3}}{\sqrt{\epsilon^2+s-r}} \IE[ \langle \rho*\eta_r(z), \eta(dz) \rangle] dr ds = \mathcal{O}(\epsilon^{1/3})
\end{align*}
where we have used Remark~\ref{remark:BoundedIntegral}. 
We now proceed to control the terms with martingales in (\ref{ThingsToBound}). For the first one note that, by using Cauchy-Schwartz in $L^2(\mathbb{P})$ and then in $L^2(\IR^d)$,
\begin{align}
\int_0^t \int \IE[|M_s(y)|\rho_\epsilon*\eta_s(y)] dy ds & \leq \int_0^t \int \IE[|M^2_s(y)]^{1/2} \IE[\rho_\epsilon * \eta_s(y)^2]^{1/2} dy dy \nonumber \\ & \leq \int_0^t \left( \int \IE[|M_s(y)^2|] dy \int \IE[\rho_\epsilon * \eta_s(y)^2] dy \right)^{1/2} ds  \nonumber \\ &= \mathcal{O}\left( \left( \frac{\theta}{N \epsilon^d} \right)^{1/2} \right)
\label{TwiceCS} \end{align}
where in the last inequality we used Proposition~\ref{IntegrabilityOfM} and Remark~\ref{remark:BoundedIntegral}. For the second term with a Martingale in (\ref{ThingsToBound}) we need to proceed with more care. For this we first rewrite explicitly the dependency on $\epsilon$ of $M_t(x)$ by using $M_t^{\varepsilon^2}(x)$. We can then write from (\ref{MartingaleProblemMolifier}) and Lemma~\ref{Lemma:BoundPsiHS2}) that,
\begin{align*}
 M_t^{\epsilon^2}(x) &= \langle p_{\epsilon^2}(x,y), \eta_t(dx) \rangle - \langle p_{t + \epsilon^2}(x,y), \eta_0(dx) \rangle \\ &- \int_0^t \langle F(\rho_\epsilon*\eta_s(y)) p_{t-s+\epsilon^2}(x,y), \eta_s(dy) \rangle ds + \mathcal{O}\left(\frac{1}{\epsilon^2 \theta} \right) \end{align*}
 It follows then that $M_s^{\epsilon^2}= p_{\epsilon^2/2}*M_s^{\epsilon^2/2} + \mathcal{O}\left(1/\epsilon^2 \theta\right)$. Using this we can then bound,
 \begin{align*}
\int_0^t &\IE[ \langle |M_s^{\epsilon^2}(x)|, \eta_s(dx) \rangle ] ds \\ &= \int_0^t \IE[ \langle | p_{\epsilon^2/2}*M_s^{\epsilon^2/2}(x)|, \eta_s(dx) \rangle] ds \\ & \leq  \int_0^t \IE[ \langle p_{\epsilon^2/2}*| M_s^{\epsilon^2/2}(x)|, \eta_s(dx) \rangle] ds \\ &= \int_0^t \IE[ \langle \int p_{\epsilon^2/2}(x,y) |M_s^{\epsilon^2/2}(y)|dy, \eta_s(dx) \rangle] ds \\ &= \int_0^t \IE[ \int |M_s^{\epsilon^2/2}(y)| p_{\epsilon^2/2}*\eta_s(y) dy ] ds \\ &= \int_0^t \int \IE[ M_s^{\epsilon^2/2}(y)| p_{\epsilon^2/2} * \eta_s(dy)] dy ds 
 \end{align*}
 An analogues computation as the one performed to obatin (\ref{TwiceCS}) gives the last term is $\mathcal{O}\left( \left( \frac{\theta}{N \epsilon^d} \right)^{1/2} \right)$.
 \end{proof}
Now, to finish the characterisation of the limit, it remain to show that:
\[ \int f(x) \rho_\epsilon * \eta_t(x) F(\rho_\epsilon * \eta_t(x)) dx \rightarrow \int f(x) u_t(x) F(u_t(x)) dx \]
where $u_t(x)dx$ is the limit of $\rho_\epsilon*\eta_t(x)dx$. As we mention as the start of this note this does now follow from weak convergence of $\rho_\epsilon*\eta(x)dx$. For proving this, we recall that $F(x)$ is a polynomial, so we just need to prove said convergence of powers of $(\rho_\epsilon*\eta_t(x))$. To illustrate how to proceed, we will prove the convergence,
\begin{equation} \label{eq:QuadConvergence} \int f(x) \rho_\epsilon * \eta_t(x) \rho_\epsilon * \eta_t(x)^2 dx \rightarrow \int f(x) u_t(x) u_t(x)^2 dx \end{equation}
with the understanding that converge of higher powers will follows in a similar fashion with rougher notation. To achieve (\ref{eq:QuadConvergence}) we proceed by proving an approximation of the form,
\begin{equation}
\label{aprox2}
\langle f, (\eta_s * \rho_\epsilon)^2 \rangle \sim \int \int f(z) (\rho_\epsilon * \eta_s)(z) \rho_\delta (z-y) (\rho_\epsilon * \eta_s)(y) dz dy.
\end{equation}
Before proceeding with a proof of (\ref{aprox2}), we argue why this is enough to conclude Theorem~\ref{TeoremOneStepConvergence}. Note the right hand side of (\ref{aprox2}) can written as
\[ \int_{\IR^{d} \otimes \IR^d} f(z) \rho_\delta(z-y) (\rho_\epsilon * \eta_s(dz) \otimes (\rho_\epsilon * \eta_s(dy)). \]
Weak convergence of $\rho_\epsilon * \eta$ (plus continuity of the mapping $(z,y) \rightarrow f(z) \rho_\delta(z-y)$) gives that the last term converges in $L^1$ to
\[ \int_{\IR^{d} \otimes \IR^d} f(z) \rho_\delta(z-y) (u_s(dz) \otimes (u_s(dy)). \]
Finally, by a direct analogue of (\ref{aprox2}) we can say that
\begin{equation} \int_{\IR^{d} \otimes \IR^d} f(z) \rho_\delta(z-y) (u_s(dz) \otimes (u_s(dy)) \sim \langle f(x), u_s^2(x) dx \rangle. \label{aprox3} \end{equation}
which is what we seek. We proceed now to prove (\ref{aprox2}), with the understanding that (\ref{aprox3}) follows in a similar way and give even better error terms.
\begin{theorem}
Under the conditions of Theorem~\ref{TeoremOneStepConvergence}, we have that
\begin{align*}
\int_0^t \mathbb{E}&\left[\left| \langle f(y), (\eta_s * \rho_\epsilon(y))^2) dy\rangle - \int \int f(y) (\rho_\epsilon * \eta_s)(z) \rho_\delta (z-y) (\rho_\epsilon * \eta_s)(y) dz dy. \right|\right] ds
\end{align*}
is bounded by,
\[  \mathcal{O}\left(\delta^{1/3} + \delta +  \frac{1}{\epsilon^2 \theta} + \left( \frac{\theta}{N \epsilon^2}\right)^{1/2} \right)  \]
in particular, under (\ref{ConditionsForConvergence}), the last term goes to $0$ uniformly in $\theta,\epsilon,N$ and $\delta$ (when $\delta$, $\epsilon$ go to $0$ and $N,\theta$ to infinity).
\end{theorem}
\begin{proof}
First note,
\begin{align}
&\int_0^t \mathbb{E}\left[| \langle f(y), (\eta_s * \rho_\epsilon(y))^2) dy \rangle - \int \int f(y) (\rho_\epsilon * \eta_s)(z) \rho_\delta (z-y) (\rho_\epsilon * \eta_s)(y) dz dy. |\right] ds \nonumber \\ &\leq  \Vert f \Vert_\infty \int_{S_f}  \int_0^t  \mathbb{E}\left[ \int \left\{ \left|(\rho_\epsilon * \eta_s)(y)-(\rho_\epsilon * \eta_s)(z) \right|   \rho_\delta(z-y) dz \right\} (\rho_\epsilon *\eta_s)(y) \right] ds  dy \label{b1}
\end{align}
We want then to bound (\ref{b1}) by using Proposition~\ref{ContinuityEstimate}. For this consider the event,
\[ B_{s,y} := \{ \rho_\epsilon * \eta_s(y) > \delta^{-2/3} \}. \]
Note that by Markov's inequality and Lemma~\ref{FirstBoundLocalDensity} the event $B_{s,y}$ has probability of order $\delta^{2/3}$ (and independent of $s$ and $y$). Thus, proceeding as how get (\ref{ThingsToBound}), the integral against $ds$ in the right hand side of (\ref{b1}) is bounded by
\begin{align}
&\int_0^t \IE[ \frac{\delta}{\sqrt{\epsilon^2 + s}} \langle 1,\eta_0(dz) \rangle (\rho_\epsilon *\eta_s)(y) ] ds +  \int_0^t \IE[  \int_0^s \frac{\delta}{\sqrt{\epsilon^2 + s - r}} \langle 1, \eta_r (dz) \rangle dr(\rho_\epsilon *\eta_s)(y)  ] ds \nonumber\\ & + \int_0^t \IE[   1_{B^c_{s,\delta}} \int_0^s \frac{\delta}{\sqrt{\epsilon^2 + s - r}} \langle \rho_\epsilon*\eta_r(z), \eta_r(dz) \rangle dr (\rho_\epsilon *\eta_s)(y) ] ds + \int_0^t \IE[\langle |M_s(x)|,\eta_s(dx) \rangle] ds \nonumber \\ & + \int_0^t  \IE[|M_s(y)|\rho_{\epsilon+\delta}*\eta_s(y)] ds  +\mathcal{O}\left( \frac{1}{\epsilon^2 \theta} + \delta^{1/3} \right)  \label{ThingsToBound2}
\end{align}
From this point onward we can proceed as how we bounded (\ref{ThingsToBound}) in Proposition~\ref{CuadraticApprox} to obtain that the right hand side of (\ref{b1}) is bounded by,
\[  \mathcal{O}\left(\delta^{1/3} + \delta+ \frac{1}{\epsilon^2 \theta} + \left( \frac{\theta}{N \epsilon^2}\right)^{1/2} \right)  \]
Concluding the result.
\end{proof}

\paragraph{Conclusion}
Under the condition (\ref{ConditionsForConvergence}) we have that all the errors in our approximations go to $0$. It is then we can deduce that quadratic (and analogously, higher order) terms in the martingale problem for $\rho_\epsilon*\eta_t(x)dx$, (\ref{MGPT_epsilon}), converges to the corresponding term of the weak limit. Any linear term converge trivially by weak convergence. By a Taylor's expansion we can also conclude that,
\begin{align*}
\int_0^t \langle \mathcal{L}^\theta T_{\epsilon^2} f, \eta_s(dx)\rangle ds &= \int_0^t \langle \frac{1}{2} \Delta T_{\epsilon^2} f(x), \eta_s(dx) \rangle + \mathcal{O}(\theta^{-1}) 
\\ &= \int_0^t \langle \frac{1}{2} \Delta f(x), \rho_\epsilon* \eta_s(x) dx \rangle ds + \mathcal{O}(\theta^{-1} + \epsilon).
\end{align*}
Thus, by weak convergence we can deduce
\[ \int_0^t \langle \mathcal{L}^\theta T_{\epsilon^2} f, \eta_s(dx)\rangle ds \rightarrow \int_0^t \langle \frac{1}{2} \Delta f(x), u_s(x) dx \rangle ds \]
Putting all the convergences together, this identifies the limit of $\rho_\epsilon*\eta_s(x)dx$ as we have claimed in Theorem~\ref{TeoremOneStepConvergence}.


\todo[inline]{END OneStepConvergence}



% % % % % % % % % % % % %
\subsection{Tightness of the Lines of Descent}
    \label{sec:lookdown_convergence}

Now we prove the main theorem on convergence of the lookdown process, 
Theorem~\ref{thm:lookdown_convergence}.
This follows a similar pattern as the the proofs of convergence for the population process
in Section~\ref{sec:population_density_proof},
although we will need additional estimates to ensure that
trajectories of individual lines of descent converge,
not just the population process.


\begin{proof}[Proof of Theorem~\ref{thm:lookdown_convergence}]
    As in Section~\ref{sec:population_density_proof},
    the theorem will follow from tightness,
    ensuring convergence along subsequences,
    and characterization of the limit.
    This time, the processes $\lp^N$ take values in $\lpmeasures$,
    the space of locally finite measures on space $\times$ levels.
    (They will in fact be point measures, including the limit,
    but that is a consequence of this theorem.)
    Again, tightness will follow from a compact containment condition,
    tightness of one-dimensional distributions,
    and an application of \citet{EK} Theorem~3.9.1.
    We show that the martingale properties are preserved in the limit
    in Section~\ref{sec:lookdown_generator_proofs}.
    \comment{TODO: write and refer to lemma?}

    Lines of descent can escape to infinite level in finite time,
    and so we endow $\lpmeasures$ with the vague topology ``in the level coordinate'',
    i.e., the topology induced by test functions on $\overline{\IR^d} \times [0,\infty)$
    of the form $f(x) g(u)$, where $f \in C_b(\overline{\IR^d})$ is bounded and continuous
    and $g \in C_c([0,\infty))$ is continuous with compact support.
    In this topology, compact containment follows from
    tightness of the spatial and level motions of a single line of descent
    (Lemmas~\ref{lem: tightness of individual spatial trajectory}
    and~\ref{lem:tightness of levels}, respectively),
    and
    tightness of $\lp^N(\IR^d \times [0, u_0])$ for each $u_0$
    (i.e., the number of individuals with level below $u_0$).

    Finally, we must show that the limiting lookdown process $\lp$
    projects to the limiting process $\eta$, i.e., a solution of the martingale
    problem in Theorem~\ref{thm:nonlocal_convergence}.
    Let $N_k \to \infty$ be a sequence along which $\lp^{N_k}$ converges.
    However, by Theorem~\ref{thm:nonlocal_convergence},
    there is a subsequence $N_{k(j)}$ along which the projected population processes
    $\eta^{N_{k(j)}}$ converge, and the limit solves the martingale problem.
    This implies that any limit of $\lp^N$ projects to a population process $\eta$
    solving the martingale problem of Theorem~\ref{thm:nonlocal_convergence}.

\comment{MAKE THIS BELOW A LEMMA?}

% BEGIN OLD STUFF
To prove that the lookdown representation is tight, we generalise the vague
    topology to the setting of measure-valued processes. For a detailed
    description of the vague topology, refer to Appendix \ref{sec: Empirical
    Measure Topology}.

We will lay out the steps we follow to prove tightness. First of all, we will
    show that compact containment condition is satisfied if we can control the
    total mass of particles in our system living in a finite region in space
    with a level lower than some $u_{*}$. Since a control on total mass has
    already been shown in Lemma \ref{lem:eta_compact_containment}, we have
    immediately established compact containment condition for our lookdown
    processes. 

Then, we apply Theorem A 2.4 in \cite{kallenberg1997foundations} (Theorem
    \ref{teo: Vague Relative Compactness through Projection}) and use a
    corollary of Stone-Weierstrass Theorem (Lemma \ref{lem: Dense set for test
    functions on empirical measures}) to show that our lookdown representation
    $\{\xi^N_t: N \in \mathbb{N}\}$ is tight as a $\mathcal{M}(\IR^d
    \times [0, \infty])$-valued stochastic process in the vague topology as
    long as $\{F_g(\xi^N_t): N \in \mathbb{N}\}$ is tight as real-valued
    stochastic processes for a huge class of $F_g$. Explicitly, $F_g(\xi):=
    \prod_{(x_i,u_i)\in \xi}g(x_i,u_i)$, where $g(x,u)$ satisfies the property
    that $g(x,u) = 1 $ for all $u \geq u_{*}$ for some $u_{*}>0$.
% END OLD STUFF

    The process we consider is similar to the state-dependent branching processes
    of \citet{KR}, so one might expect that the proofs there would carry over with little change.
    However, there is an important difference:
    Recall that the level $U_a(t)$ of a line of descent evolves as
    $$
    \dot u 
    =
    c(x, \eta)
    u^2
    - b(x, \eta)
    u,
    $$
    where
    \begin{align*}
        c(x, \eta) &= \frac{\theta}{N} \gamma(x, \eta) \int_{\IR^d} r(y, \eta) q(x, dy) \\
        \text{and}\qquad
        b(x, \eta)
        &=
        \theta \gamma(x, \eta) \int_{\IR^d} \left(r(y, \eta) - r(x, \eta)\right) q(x, dy)
        + F(x, \eta) 
    \end{align*}
    (see equations \eqref{eqn:level_generator} and \eqref{eqn:b_limit}),
    where $c(x, \eta) \ge 0$ and $b(x, \eta)$ may take either sign.
    In \citet{kurtz/rodrigues:2011}, $b$ was bounded above and $c$ was bounded away from zero,
    so they noted that if $U_a(t) \ge b/c$
    lines of descent would only move upwards from that time onwards.
    This allows the processes to be jointly and simultaneously constructed
    for all values of $N$, with a pointwise embedding
    of $(\xi^N_t)_{t \ge 0}$ within $(\xi^{M})_{t \ge 0}$ for $b/c < N < M$.
    In other words, individuals with levels above $N > b/c$ at time $t_0$
    do not affect $(\lp^N_t)_{t \ge t_0}$,
    thus allowing a comparison of the number of lines of descent below level $u_0$
    to a branching process.
    Although we have provided a
    joint construction of $\lp^N$ for all $N$ in Section~\ref{sec: individual lines of descent},
    it does not have this monotonicity:
    for one thing, $b$ and $c$ depend on the population process $\eta$
    and so all individuals can affect all other ones (even those with lower levels).
    Furthermore, in the deterministic case $\theta/N$ and hence $c$ converge to zero,
    and so lines of descent with arbitrarily high level may drift back downwards.
    Indeed, this must be the case if the population persists,
    since in the deterministic case there is no branching.
    In our case $b$ and $c$ depend on the state of the population,
    so we have no such monotonicity.

    To get around this,
    first note that both $b$ and $c$ are uniformly bounded above 
    by Lemma~\ref{lem:gamma_bound} and since $\gamma$, $r$, $F$ \comment{(???)}, and $\theta/N$ are.
    Also note that for any $\delta > 0$ we can find a $K$ such that
    $$
    \IP\left\{
        \sup_{0 \le t \le T} \sup_x \smooth{F} \eta_t(x) < K
        \right\} < \delta,
    $$
    uniformly in $N$ and over a compact set of initial conditions.
    Since $F$ is uniformly continuous, this implies that $\sup_{0 \le t \le T} \sup_x F(x, \eta_t)$
    is similarly tight,
    as well as the rate of downward drift of the level of a line of descent,
    i.e., there is a $\bar b$ such that $b(x, \eta_t) \le \bar b$
    on a set of probability at least $1-\delta$ as well.
    Therefore, if $u_1 \ge u_0 + \bar b T$, then
    $$
    \IP\left\{
        \inf_{0 \le t \le T} U_a(t) \le u_0
        \;\big|\;
        U_a(0) > u_1
    \right\}
    \le \delta ,
    $$
    and so with high probability any lines of descent that had level below $u_0$
    at some point $0 \le t \le T$
    are descended from lines of descent with initial level below $u_1$.
    Since the number of lines of descent with level below $u_1$
    converges to a Poisson with finite mean,
    we need only show that the number of offpring lines of descent
    produced by this initially finite number is also tight.
    However, this follows immediately
    since the rate of production of new lines of descent with level below $u_1$
    of a line of descent at location $x$ and level $u$ is
    $2 (u_1 - u) N^{-1} \theta \gamma(x, \eta) \int_{\IR^d} r(y, \eta) q(x, dy)$
    and therefore uniformly bounded (because we may take $u_1 \le N$).


\end{proof}

% % % % % % % % % % % % %
\subsubsection{New stuff}

\comment{TODO: reference this elsewhere, if we keep it?}
We record for future reference
the Aldous-Rebolledo criteria for tightness of a sequence real-valued processes:

\begin{theorem}[\citet{rebolledo:1980}]
    \label{thm:aldous_rebolledo}
    Let $\{Y^(n)\}_{n \ge 1}$ be a sequence of real-valued processes
    with c\`adl\`ag paths.
    Suppose that the following conditions are satisfied.
    \begin{enumerate}
        \item For each fixed $t$, $\{Y_t^(n)\}_{n \ge 1}$  is tight.
        \item Given a sequence of stopping times $\tau_n$, bounded by $T$,
            for each $\epsilon > 0$ there exists $\delta > 0$ and $n_0$ such that
            \begin{align*}
                \sup_{n \ge n_0}
                \sup_{\theta \in [0, \delta]}
                \IP\left\{ \left|
                    Y^{(n)}_{\tau_n + \theta} - Y^{(n)}_{\tau_n}
                \right| > \epsilon
                \right\}
                \le \epsilon .
            \end{align*}
    \end{enumerate}
    Then the sequence $\{Y^(n)\}_{n \ge 1}$ is tight.
\end{theorem}

\begin{lemma}
    \label{lem:test_fn_tightness}
    Let $f$ be a bounded, continuous real-valued function on $\IR^d \times [0, \infty)$
    with uniformly bounded first and second derivatives
    for which there exists a $u_0$ such that if $u > u_0$ then $f(x, u) = 0$.
    Then, the sequence of real-valued processes $(\langle f, \xi_t^N \rangle)_{t \ge 0}$
    are tight in the space of c\`adl\`ag processes.
\end{lemma}

\begin{proof}[of Lemma \ref{lem:test_fn_tightness}]
    First, a reminder of the description of the process
    \comment{(TODO: figure out if the notation here for $a$ and $b$ should be used above;
        if so, move it up; if not, replace it here.)}
    Recall when the state of the process is $\eta$,
    the level of a line of descent at spatial location $x$ and level $u$
    changes following
    $$
        \dot u = a^N(x, \eta) u^2 - b^N(x, \eta) u,
    $$
    where
    $$
        a^N(x, \eta)
        =
        \frac{\theta}{N} \gamma(x, \eta) \int_{\IR^d} r(y, \eta) q(x, dy) ,
    $$
    and
    $$
        b^N(x, \eta)
        =
        \theta \gamma(x, \eta) \int_{\IR^d} (r(y, \eta) - r(x, \eta)) q(x, dy) + F(x, \eta) .
    $$
    Furthermore, the line of descent
    gives birth to lines at higher levels at rate $2 (N - u) a^N(x, \eta)$,
    that each such new line chooses a level uniformly from $[u, N]$,
    a spatial location $y$ from the kernel
    $$
        q^m(x, dy, \eta)
        =
        r(y, \eta) q(x, dy) / \int_{\IR^d} r(y, \eta) q(x, dy),
    $$
    and the two lines swap spatial locations with probability 1/2.

    It is evident from the description of the process
    (or, by differentiating in Definition~\ref{defn:lookdown_mgale})
    that
    \begin{align*}
    \begin{split}
    M^f_s
        &=
        \langle f, \xi_t^N \rangle
        -
        \langle f, \xi_0^N \rangle
    \\ & \qquad \qquad {}
        -
        \int_0^t
        \big\langle
            a^N(x, \eta^N_t)
            \int_u^N \int_{\IR^d}
            \left(
                f(y, u_1) + f(x, u_1) + f(y, u) - f(x, u)
            \right)
            du_1 q^m(x, dy, \eta^N_s)
    \\ & \qquad \qquad \qquad \qquad {}
            +
            \left(
                a^N(x, \eta^N_s) u^2
                - b^N(x, \eta^N_s) u
            \right)
            \frac{d}{du} f(x, u)
        ,
        \xi_s^N(dx, du) \big \rangle
        ds
    \end{split}
    \end{align*}
    is a martingale.
    Now we would like to bound the terms in the integral,
    to arrive at a bound using Gronwall's inequality, as before.

    First, note that there exists a constant $C_a$
    such that $a^N(x, \eta) \le C_a$
    (since $\gamma$ and $r$ are bounded above, and $\theta/N \to \alpha \ge 0$),
    and for any $\epsilon > 0$ and $T$ there is a $C_b$ such that for all $N$,
    $$
        \IP\left\{
            \sup_{0 \le s \le T} \sup_x |b^N(x, \eta_s^N)| > C_b
        \right\}
        < \epsilon 
    $$
    \comment{reference where we show this above}.
    Now, since $f$ vanishes for $u \ge u_0$,
    on the event that $\sup_x |b^N(x, \eta_s^N)| \le C_b$ then
    if $N \ge u_0$,
    \begin{align*}
    &
        \int_{\IR^d} \int_0^N
            \left(
                a^N(x, \eta^N_s) u^2
                - b^N(x, \eta^N_s) u
            \right)
            \frac{d}{du} f(x, u)
        \xi_s^N(dx, du)
    \\  & \qquad
    \le
        \int_{\IR^d} \int_0^{u_0}
            \left(
                C_a u_0^2
                + C_b u_0
            \right)
            \left\| \frac{d}{du} f \right\|_\infty
        \xi_s^N(dx, du) .
    \\  & \qquad
    \le
        C_0 \xi_s^N(\IR^d \times [0, u_0]) .
    \end{align*}

    Next, observe that for any $\eta$,
    $$
    \int_{\IR^d} \int_u^N (f(y, u_1) + f(x, u_1)) du_1 q^m(x, dy, \eta)
    \le 2 u_0 \|f\|_\infty .
    $$
    Furthermore,
    recall that we have assumed \comment{(check where we do this)}
    that $q^m$ is uniformly elliptic,
    and so since $f$ has bounded first and second derivatives,
    there is a $C_1$ depending on $f$ such that for all $\eta$,
    $\theta \int_{\IR^d} (f(y, u) - f(x, u)) q^m(x, dy, \eta) \le C_1$.
    Therefore, using boundedness of $\theta/N$,
    \begin{align*}
        &
        \int_{\IR^d} \int_u^N (f(y, u) - f(x, u)) du_1 q^m(x, dy, \eta^N_s)
        \\ &\qquad
        = (N - u) \int_{\IR^d} (f(y, u) - f(x, u)) q^m(x, dy, \eta^N_s)
        \\ &\qquad
        = \frac{N}{\theta} \left(1 - \frac{u}{N}\right) \theta \int_{\IR^d}
            (f(y, u) - f(x, u)) q^m(x, dy, \eta^N_s)
        \\ &\qquad
        \le C_1 \frac{N}{\theta} \ind_{u \le u_0} .
    \end{align*}

    Putting this together,
    \begin{align*}
        \IE\left[
            \langle f, \xi_t^N \rangle
        \right]
        \le
    \end{align*}


\end{proof}



% % % % % % % % % % % % %
\subsubsection{Tightness of the Lines of Descent}
    \label{sec:lookdown_tightness_proofs}

In this section,
we will prove tightness
of the spatial and level trajectory
of a line of descent
(constructed in \S \ref{sec: individual lines of descent})
as a forward-in-time process.

To prove tightness of the 
spatial and level locations
of a typical line of descent,
we will show that the
jumps in
spatial and level locations
of the line of descent
in finite time
can be controlled.

Recall that the lines of descents
track chains of descendant
individuals forwards through time following the \textit{levels}.

\subsubsection{Tightness of Spatial Trajectories}
Before proving tightness of our spatial trajectories, 
note that for a fixed label
$a=(a_0,a_1,..,a_k)$,
the spatial evolution of 
$X^N_{a}(t)$ for $t \geq \tau^N_{a}$
is determined
purely by the last term 
in Equation \eqref{eq: space_across_lineage}.

As a result, the spatial jumps are 
driven by 
an underlying Poisson point process. 
It is therefore sufficient
to prove that square of 
these increments are bounded
in small time scales.

\begin{lemma} \label{lem: tightness of individual spatial trajectory}
Assume that the jump kernel $q_\theta^\mathfrak{m}(x, \eta, dy)$ of the form
$$
    q_\theta^\mathfrak{m}(x,\eta,  dy)
    :=
    \frac{
        r(y, \smooth{r} \eta(y)) q_\theta(x, dy)
    }{
        \int r(x, \smooth{r} \eta(z)) q_\theta(x, dz)
    } 
$$
satisfies the inequalities
\begin{align}
   \sup_{\theta}\mathbb{E}\left[\limsup_{x, \eta} \theta \left| \int_{\IR^d} (y-x) q^{\mathfrak{m}}_{ \theta}(x,\eta,dy) \right|\right] \leq & \quad \overline{\alpha},\\
   \sup_{\theta}\mathbb{E}\left[\limsup_{x, \eta} \theta \int_{\IR^d} |y-x|^2 q^{\mathfrak{m}}_{ \theta}(x,\eta,dy)\right]  \leq & \quad \overline{\beta}.
\end{align}
Furthermore, assume that the birth rate satisfies 
$$\sup_{x, \eta}\gamma_{\theta}(x, \varrho_{\gamma}*\eta(x)) < C_G,$$ 
then for fixed $a=(a_0,a_1,..,a_k)$,
$\{(X^{N}_{a}(t))_{\tau^{N}_a\leq t < \sigma^{N}_{a}}: N \in \mathbb{N},  \}$ is relatively compact in 
$D_{\IR^d}[0,\infty )$ where $\IR^d$ is equipped with the standard Euclidean metric. 
\end{lemma}
\begin{proof}
We write $S_{t,r}:=(t,t+r  ]\times [0,\infty )\times \IR^d\times 
\{0,1\}$ to be the region for the Poisson measure driving $(X^{N}_{a}(t))_{t \geq \tau_a}$ between time $t$ and $(t+r)$ and write $\tilde\Pi_{a}(ds,dv,dz,d\kappa):=\Pi_{a}(ds,dv,dz, d\kappa) - ds\times dv\times dz \times (\frac{1}{2}\delta_{0}+\frac{1}{2}\delta_{1})d\kappa$ to be the compensated Poisson point process of $\Pi_{a}$. 
For $\tau_a \leq t < t+r < \sigma_a$, we can compute
\small
\begin{align*}
&\mathbb{E}[|X^{N}_{a}(t+r )-X^{N}_{a}(t)|^2|{\cal F}_t^{
\eta}]\\
=& \mathbb{E}\left[\left|
\int_{S_{t,r}}
(1-\kappa)y(z)
\ind_{ \{
        U_{a}(\tau_{a}+s)
        + \ell(X_{a}(\tau_{a}+s),y(z),\eta_{a}(\tau_{a}+s)) v < N
        \}
      } 
\Pi_{a}(dsdvdzd\kappa)
\right|^2
\Bigg|{\cal F}_t^{\eta}\right]\\
\leq&
 2\mathbb{E}\left[
 \left|
 \int_{S_{t,r}}
 {\bf 1}_{\{v < 2  \theta \gamma_{\theta}(x, \eta) r_{\theta}(x + y, \eta) \}}{\bf 1}_{\kappa =1}
y(z)
\tilde\Pi_{a}(ds,dv,dy,d\kappa )
\right|^2
\Bigg|{\cal F}_t^{\eta}
\right]\\
&\quad +2\mathbb{E}\left[
\left|
\int_{t}^{t+r}\int_{\{v < 2\theta \gamma_{\theta}(x, \eta) r_{\theta}(x + y, \eta) \}}
\frac{1}{2}\left| \int_{\IR^d} (y-X^{N}_{\tau}(s-)) q^{\mathfrak{m}}_{\theta}(X^{N}_{\tau}(s-),\eta^{N}(s-),dy) 
\right|
dvds \right|^2
\Bigg|{\cal F}_t^{\eta}
\right]\\
&\leq 
2\mathbb{E}\left[
\int_{t}^{(t+r)   }
(1+C_G) \theta 
\int_{{\IR}^d}|y-X^{N}_{
a}(s)|^2q^{\mathfrak{m}}_{\theta}(X^{N}_{a}(s),\eta^{N}(s),dy)ds
\Bigg|{\cal F}_t^{\eta}\right]\\
&\quad +2\mathbb{E}\left[
\left|
\int_{t}^{(t+r)}
(1+C_G)\theta 
\left|
\int_{{\IR}^d}
(y-X^{N}_{a}(s))q^{\mathfrak{m}}_{\theta}(X^{N}_{a}(s),
\eta^{N}(s),dy)
\right|
ds
\right|^2
\Bigg|{\cal F}_t^{\eta}\right]\\
\leq& 2r \overline{\beta}(1+C_G)+2(r \overline{\alpha}(1+C_G))^2.
\end{align*}
\normalsize

The first inequality uses of the inequality $(A+B)^2 \leq 2A^2+2B^2$ as well as the fact that $(N-U_{a})N^{-1} \leq 1$.
The second inequality holds 
because birth events occur
at rate $2\theta \gamma_{\theta}(x, \eta) r_{\theta}(x + y, \eta)$
which is bounded above by $2\theta (1+C_G)$. The third inequality holds due to the assumptions on the dispersal kernel.

Finally, since $\eta_0$ has compact support, we can take $t=\tau_u$ and apply Markov inequality to get that for every $\delta>0$, there exists $N_{\delta}>0$ such that
\begin{equation}\label{eq: Compact Containment Condition for individual spatial motion}
\sup_{N>N_{\delta}}\sup_{T>0}\mathbb{P}\left[|X^{N}_{a}(\tau_{a}+T)-X^{N}_{a}(\tau_{a})|>N_{\delta}\right]\leq   \frac{2r \overline{\beta}(1+C_G)+2(r \overline{\alpha}(1+C_G))^2}{N_{\delta}^2}  < \delta,
\end{equation}
giving us compact containment condition. Relative compactness thus follows by Theorem 3.8.6.
of \cite{ethier/kurtz:1986}.
\end{proof}
\begin{remark}
Note that if we assume $r \equiv 1$, then 
$q^{\mathfrak{m}}_{\theta}(x,\eta,dy)=q_{\theta}(x,dy).$
In particular, if we let $q_{\theta}$ be the standard Gaussian normal, we have the inequalities
\begin{align}
   \sup_{\theta}\limsup_{x, \eta} \theta \left| \int_{\IR^d} (y-x) q^{\mathfrak{m}}_{ \theta}(x,\eta,dy) \right| =& \sup_{\theta}\limsup_{x} \theta \left| \int_{\IR^d} (y-x) q_{\theta}(x,dy) \right| =  0,\\
   \sup_{\theta}\limsup_{x, \eta} \theta \int_{\IR^d} |y-x|^2 q^{\mathfrak{m}}_{ \theta}(x,\eta,dy)  =& \sup_{\theta}\limsup_{x} \theta \int_{\IR^d} |y-x|^2 q_{\theta}(x,dy) =  1,
\end{align}
as the last term can be expressed as $\theta \mathbb{E}_{x}[(B_{1/\theta}-x)^2]=1$.

As a result, we have tightness for all populations with unbiased isotropic dispersal.

Furthermore, the assumption on $\gamma$ is natural as we typically assume an uniform upper bound on the total fecundity rate, i.e.
$\sup_{u \geq 0} u\gamma(x,u) < C$.
\end{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Tightness of Level Processes}
We have now proved that the spatial trajectories of a line of descent during its lifetime is relatively compact as $N \to \infty$. However, we will still need to show that the level process is also well controlled, i.e. a line of descent with an initial level smaller than $K << N$ will have a level larger than $N$ in very small time-scale with very low probability. This will then prevent a line of descent from dying instantaneously in the scaling limit.

To make this precise, we must work in the vague topology. Proving tightness of the level process is slightly trickier as the levels of many particles will drift towards infinity in finite time. These particles however will not contribute to the metric associated with the vague topology and thus we will retain relative compactness. The main argument relies on the observation that particles with levels lower than some threshold $\overline{U}_{N, \delta}$  have regulated level increments of order $\epsilon$ over any small time interval of length $r$ given large enough $N$. As a result, we can evaluate our level process against test functions $(g_n)_{n=1,..,N}$ with cut off smaller than $\overline{U}_{N, \delta}$. 

\begin{lemma} \label{lem:tightness of levels} For fixed $a\in \mathcal{U}$, let $U^N_{a}(\tau_a)=U_0$, i.e. the level of the line of descent $a$ at birth is $U_0>0$.
On top of the assumptions laid out in Lemma \ref{lem: tightness of individual spatial trajectory}, assume that 
$$
    \sup_{\theta}\mathbb{E}\left[ \sup_{x, \eta} b_\theta(x, \eta) \right]
    :=
    \sup_{\theta}
    \theta 
    \mathbb{E}\left[ \sup_{x, \eta} \left(
    \gamma_\theta(x,\eta) \int_{\IR^d} r_\theta(y, \eta) q_\theta(x, dy)
    -
    \mu_\theta(x,\eta)
    \right)\right] 
    \leq C_G,
$$
and there exists some $C_F \geq 0$ such that
\begin{align} 
\begin{split}
b_{\theta}(x, \eta)
&=
    \theta \gamma_{\theta}(x, \eta) \int_{\IR^d} \left( r_{\theta}(y, \eta) - r_{\theta}(x, \eta) \right) q_{\theta}(x, dy)
    + F_{\theta}(x, \eta) \\
& \geq - C_F \varrho_F*\eta(x)
\end{split}
\end{align}
for all $x \in \IR^d$ and  $\eta \in \mathcal{M}_{F}(\IR^d)$.

Then
$\{(U^{N}_{a}(t))_{t \geq \tau_a}, N >0\}$ is relatively compact in 
$\mathcal{D}_{[0,\infty)}[0,\infty )$, where $[0,\infty)$ is equipped with the vague topology.
\end{lemma}



\begin{proof} Recall that for a fixed line of descent, the level process is determined only by the integral equation of the form 
\begin{align*}
&U^{N}_{a}(t+r)-U^{N}_{a}(t)\\
=&\int_{t}^{t+r}
\left(
        \frac{\theta}{N} \gamma_{\theta}(X^N_{a}(s),\eta^N_s)
        \int_{\IR^d} r_{\theta}(z,\eta^N_s) q_{\theta}(X^N_a(s),dz) U^N_a(s)^2
        -
        b_{\theta}(X^N_a(s),\eta^N_s) U^N_a(s)
    \right)
    ds.
\end{align*}

To control the level process, we consider two scenarios, one where the local density of the system between $[0,r]$ is never too big, i.e.
$$h(r)=\sup_{x \in \IR^d}\sup_{s\in [0,r]}\left\{\varrho_F*\eta^N_s(x) \right\} \leq H_{\epsilon}(r),$$
and otherwise. 

When the local density is smaller than $H_{\epsilon}$, we will show that the level grows steadily. Consequently, the level grows very fast and hit some level $\overline{U}_{K_{\epsilon}}$ very quickly only when the local density is larger than $H_{\epsilon}$. In this case, the effective increment in levels observed under the vague topology is bounded above by $\overline{U}_{K_{\epsilon}}$. We will first make this statement precise. 
We will also show that the probability of such event is very small.
As a result, the level process is tight.

We will first make our convergence statement precise under the vague topology. 
We will then prove tightness in the high density scenario,
and finally
prove tightness in the scenario where population density is well controlled
with a comparison principle.  

\textbf{Introducing a cutoff range:} 
By the characterisation of vague topology on $[0,\infty)$,
it suffices to prove that for all functions $g\in C_{c}([0,\infty), \mathbb{R}])$ (continuous functions with compact support), 
the real-valued process
$\{(g(U^{N}_{a}(t)))_{t \geq \tau_a}, N >0\}$ is relatively compact in 
$\mathcal{D}_{\mathbb{R}}[0,\infty)$,
where $\mathbb{R}$ is equipped with the Euclidean metric.

It suffices to show that for any $\epsilon > 0$ and for any line of descent $a \in \mathcal{U}$, there exists small enough  $r>0$ such that $\mathbb{E}[|g(U^{N}_{a}(t+r))-g(U^{N}_{a}(t))||{\cal F}_t^{
\eta}]< \epsilon$. 

Pick $U_{g,\varepsilon}>0$ large enough such that $\text{supp}(g)\subset [0,U_{g,\varepsilon}]$.
Since we picked $g$ to be continuous and differentiable with compact support, by Mean Value Theorem we have that 
$$|g(U^{N}_{a}(t+r ))-g(U^{N}_{a}(t))|<C'_{g}(|U^{N}_{a}(t+r)-U^{N}_{a}(t)|\wedge U_{g,\varepsilon}),$$
for some constant $C'_g$. Note that it suffices to consider $|U^{N}_{a}(t+r)-U^{N}_{a}(t)|\wedge U_{g,\varepsilon}$ because $g_n(u)=g_n(U_{g,\varepsilon})=0$ for all $u > U_{g,\varepsilon}$.

Therefore, it suffices to show that for all $\epsilon > 0$, there exists $r>0$ small enough and $N_0$ large enough such that 
\begin{equation}
\mathbb{E}[C'_{g}(|U^{N}_{a}(t+r)-U^{N}_{a}(t)|\wedge U_{g,\varepsilon}) ~|~{\cal F}_t^{
\eta} ] < \epsilon /2    
\end{equation}
uniformly over $N \geq N_0$.

\textbf{Controlling high density events:}
Recall that in Lemma \ref{lem:eta_compact_containment}, we have established compact containment for the total mass process. Therefore for any $\epsilon>0$ and $r>0$, there exists some large 
\begin{equation}
    \label{eq: Mass Bound Constant}
H_{\epsilon}(r):=\frac{\epsilon}{2||\varrho_F||_{\infty}C(r) }>0    
\end{equation}
such that 
the event 
$$A_{H_{\epsilon}}:= \left\{ \omega \in \Omega: \sup_{s\in [0,r]}\left\{\varrho_F*\eta^N_s(X^N_a(t+s)) \right\}(\omega) \geq H_{\epsilon}(r) \right\}$$
satisfies
\begin{equation}
\limsup_{N \to \infty}\mathbb{P}\left[A_{H_{\epsilon}}\right]
\leq  \frac{||\varrho_F||_{\infty}\mathbb{E}[\sup_{s\in [t,t+r]}\langle 1,  \eta^N_s \rangle ]}{H_{\epsilon}(r)}
\leq  \frac{||\varrho_F||_{\infty}C(t+r)}{H_{\epsilon}(r)}
< \frac{\epsilon}{2 U_{g,\varepsilon}C'_g},
\end{equation}
where $(C(t))_{t \geq 0}$ is the growth function that bounds the expected value of the running supremum of the total mass in Equation \eqref{eqn:eta_mass_bound}.

As a result, for any $s < r$,
\begin{equation}
\mathbb{E}\left[|g(U^{N}_{a}(t+r ))-g(U^{N}_{a}(t))| \mathbbm{1}_{A_{H_{\epsilon}}} ~|~{\cal F}_t^{
\eta} \right] 
\leq  \mathbb{E}\left[C'_{g} U_{g,\varepsilon}\mathbbm{1}_{A_{H_{\epsilon}}} \right] \leq  C'_{g} U_{g,\varepsilon}\mathbb{P}\left[A_{H_{\epsilon}} \right] < \varepsilon / 2.
\end{equation}

\textbf{Controlling bounded density events:}
We now consider $A_{H_{\epsilon}}^{c}$.
To control the downward drift of our levels from the quadratic evolution,
we neglect all positive drift and obtain
\begin{equation}
\begin{aligned}
\mathbb{E}[U^{N}_{a}(t+r)-U^{N}_{a}(t)] \geq   &   - \mathbb{E}\left[\int_{t}^{(t+r)  } b_{\theta}(X^N_a(s),\eta^N_s) U^N_a(s)ds\right]\\
\geq& - rC_GU_{g,\varepsilon}.
\end{aligned}    
\end{equation}

We can also control the upward drift by comparing our process $(U^N_{a}(\tau_a+r))_{r \geq 0}$ with another process $(W^N_a(r))_{r \geq 0}$, where 
\begin{align*}
&W^{N}_{a}(r)-W^{N}_{a}(0)\\
=&\int_{0}^{r}
\left(
        \frac{\theta}{N} C_G W^N_a(s)^2
        +
        C_F H_{\epsilon}(r) W^N_a(s)
    \right)
    ds,
\end{align*}
with $H_{\epsilon}(r) > \sup_{s\in [0,r]}\left\{\varrho_F*\eta^N_s(X^N_a(s)) \right\}$,
and $W^N_a(0)=U^N_a(\tau_a) = U_0 \leq U_{g,\varepsilon}$.

This is because $\gamma_{\theta} \leq C_G$, $r_{\theta} \leq 1$, and $-b_{\theta}(x, \eta) \leq  C_F \varrho_F * \eta(x) < -C_F H_{\epsilon}(r)$. Therefore, the growth rate of the differential equation governing $(U^N_a(t+s))_{s \geq 0}$ is smaller than that of $(W^N_a(s))_{s \geq 0}$ at all time instances.

Note that $(W^N_a(s))_{s \geq 0}$ can be solved explicitly and 
\begin{equation}
W^N_a(s) = \frac{C_F H_{\epsilon}(r) U_0 \exp(C_F H_{\epsilon}(r) s)}{C_F H_{\epsilon}(r)+C_G  N^{-1}\theta U_0(1- \exp(C_F H_{\epsilon}(r) s))}\leq U_{g,\varepsilon}    
\end{equation}
for 
\begin{equation}
    \label{eq: Bound on Level Hitting Time}
s \leq  \frac{1}{C_F H_{\epsilon}(r)} \log\left\{
1+
\frac{C_F H_{\epsilon}(r)
(U_{g,\varepsilon}
-U_0)
}
{C_F H_{\epsilon}(r)U_0 +C_G  N^{-1}\theta U_{g,\varepsilon}    U_0}
\right\}.
\end{equation}
As $\theta / N $ converges in the limit, we may take some constant $\bar{\alpha}$ such that for large enough $N$,
the level process $(U^N_{a}(s))_{s \geq 0}$ will not hit  $U_{g,\varepsilon}$ as long as
\begin{equation}
s \leq  \frac{1}{C_F H_{\epsilon}(r)} \log\left\{
1+
\frac{C_F H_{\epsilon}(r)
(U_{g,\varepsilon}
-U_0)
}
{C_F H_{\epsilon}(r)U_0 +C_G  \bar{\alpha} \overline{U}_{K_{\epsilon},\epsilon}    U_0}
\right\}:=T(U_0,\overline{U}_{K_{\epsilon},\epsilon},r).
\end{equation}

Note that $T$ only depends on the initial level $U_0$, the upper limit $U_{g,\varepsilon}$ and the bound on total mass $C(r)$ at time $r$, and is independent to $N$. 
In particular, 
\begin{equation}
\label{eq: Compact Containment for Levels}
\limsup_{N \to \infty}
\mathbb{P}\left[
\sup_{t \in [0, T(U_0,\overline{U}_{K_{\epsilon},\epsilon},r)]}
U^N_{a}(\tau_a + t)
\geq \overline{U}_{K_{\epsilon},\epsilon} 
\right]  
\leq \mathbb{P}[A_{H_{\epsilon}}]
< \frac{\epsilon}{2 U_{g,\varepsilon}C'_g},
\end{equation}
so we have therefore proven compact containment condition for our level process up to time $T(U_0,\overline{U}_{K_{\epsilon},\epsilon},r)$.

As a result, for all time $ 0 < t , t+ r \leq T(U_0,\overline{U}_{K_{\epsilon},\epsilon},r),$

\begin{align*}
W^N_a(t+r)-W^N_a(t) 
\leq &\int_{t}^{t+r} 
        \left( \frac{\theta}{N} C_G U_{g,\varepsilon}^2
        +
        C_F H_{\epsilon}(r) U_{g,\varepsilon}
        \right)
        ds\\
=&\left( \frac{\theta}{N} C_G U_{g,\varepsilon}^2
        +
        C_F H_{\epsilon}(r) U_{g,\varepsilon}
        \right) r \\
\leq&\left( \bar{\alpha} C_G U_{g,\varepsilon}^2
        +
        C_F H_{\epsilon}(r) U_{g,\varepsilon}
        \right) r
\end{align*}

Combining the results together, we have that for any $r > 0$ small enough such that 
\begin{equation}
r < \frac{\epsilon  }{4C'_g}\min\left\{\frac{1}{C_G U_{g,\varepsilon}}, \frac{1}{\bar{\alpha} C_G U_{g,\varepsilon}^2
        +
        C_F H_{\epsilon}(r) U_{g,\varepsilon}}\right\}    
\end{equation}
holds, then we have for all $t \leq T(U_0,\overline{U}_{K_{\epsilon},\epsilon},r)$,

\begin{equation}
\begin{aligned}
\mathbb{E}\left[|g(U^{N}_{a}(t+r ))-g(U^{N}_{a}(t))| \mathbbm{1}_{A_{H_{\epsilon}}^c} ~|~{\cal F}_t^{
\eta} \right] 
& \leq  \mathbb{E}\left[C'_{g} |U^{N}_{a}(t+r )-U^{N}_{a}(t)|\mathbbm{1}_{A_{H_{\epsilon}}^c} \right] \\
&\leq r C'_g \max\{ \bar{\alpha} C_G U_{g,\varepsilon}^2
        +
        C_F H_{\epsilon}(r) U_{g,\varepsilon}
        , C_G U_{g, \varepsilon}\} \\
        & < \varepsilon.
\end{aligned}
\end{equation}



\textbf{Conclusion:} Combining all of the above results, for any $\epsilon > 0$, we pick arbitrary $r>0$ and
generate $T(U_0,\overline{U}_{K_{\epsilon},\epsilon},r)$ by 
Equation \eqref{eq: Bound on Level Hitting Time} and Equation \eqref{eq: Mass Bound Constant}.

We have proven that for all $t \in [0, T(U_0,\overline{U}_{K_{\epsilon},\epsilon},r)]$,
\begin{equation}
\begin{aligned}
& \mathbb{E}[|g(U^{N}_{a}(t+r ))-g(U^{N}_{a}(t))|~|~{\cal F}_t^{
\eta}]\\
\leq & \mathbb{E}\left[|g(U^{N}_{a}(t+r ))-g(U^{N}_{a}(t))| \mathbbm{1}_{A_{H_{\epsilon}}} ~|~{\cal F}_t^{
\eta}\right]+ \mathbb{E}\left[|g(U^{N}_{a}(t+r ))-g(U^{N}_{a}(t))| \mathbbm{1}_{A_{H_{\epsilon}}^c} ~|~{\cal F}_t^{
\eta}\right] \\
< & \epsilon + \epsilon = 2\epsilon
\end{aligned}    
\end{equation}
for  
\begin{equation*}
r < \frac{\epsilon  }{4C'_g}\min\left\{\frac{1}{C_G U_{g,\varepsilon}}, \frac{1}{\bar{\alpha} C_G U_{g,\varepsilon}^2
        +
        C_F H_{\epsilon}(r) U_{g,\varepsilon}}\right\}.    
\end{equation*}

We have also proven Compact Containment Condition in Equation \eqref{eq: Compact Containment for Levels}.

Therefore we have proved tightness of our level processes by \cite{EK} Theorem 3.8.6.
\end{proof}

\subsection{Tightness of the Lookdown Representation}
\begin{lemma}
The lookdown representation satisfies the compact containment condition, 
Equation \eqref{eq: vague compact containment condition}, 
for all $\varepsilon > 0$ and fixed time $T > 0$, there exists $N,K> 0$ such that 
\begin{equation}
\liminf_{n \to \infty}\mathbb{P}\{ \xi^n_t \in \Gamma_{N,K} \text{ for all } t\in [0,T]\} > 1-\varepsilon,
\end{equation}
where $\Gamma_{N,K}$ is the compact set in Lemma \ref{lem: vague compact sets}
\end{lemma}

\begin{proof}
Note that
\begin{equation}
\begin{aligned}
     &1-\mathbb{P}\{ \xi^n_t \in \Gamma_{N,K} \text{ for all } t\in [0,T]\} \\
     = & \mathbb{P}\left\{ \sup_{t \in[0,T]}\frac{1}{N}\langle 1_{\overline{\mathbb{R}^d}\times [0,N]}, \xi^n_t \rangle \geq K \right\} \\
 \leq & \frac{\mathbb{E}\left\{ \sup_{t \in[0,T]}\frac{1}{N}\langle 1_{\overline{\mathbb{R}^d}\times [0,N]}, \xi^n_t \rangle\right\}}{K}.
\end{aligned}
\end{equation}
Recall that conditional on 
$$ \frac{1}{n}\langle 1_{[0,n]}, \xi^n_t \rangle = \eta^n_t,$$
the level of each individual is independently and uniformly distributed between $[0,n]$.
Therefore, for $n \geq N$,
\begin{equation}
\frac{\mathbb{E}\left\{ \sup_{t \in[0,T]}\frac{1}{N}\langle 1_{\overline{\mathbb{R}^d}\times [0,N]}, \xi^n_t \rangle\right\}}{K}
= \frac{\mathbb{E}\left\{ \sup_{t\in [0,T]}\langle 1, \eta^n_t \rangle \right\}}{K} < \varepsilon,
\end{equation}
for large enough $K>0$. 
The last inequality is given by Lemma \ref{lem:eta_compact_containment}.
\end{proof}

It now suffices for us to show that the processes $\{(F_g(\xi^n_t))_{t \geq 0}: n \geq 1\}$ is relatively compact for all test functions $F_g$.
To do so, we apply Aldous criterion for real-valued processes.

\subsubsection{Tightness of $(F_g(\xi_t))_{t \geq 0}$}

\begin{definition}
We define $\mathfrak{G}_{\leq 1}$ to be the set of functions $g \in \mathcal{C}(\IR^d \times [0, \infty) \to \IR$ such that 
$g(\cdot, u) \in \mathcal{C}_{0}(\IR^d)$ for all $u \geq 0$, and there exists some $u_{*}>0$ such that $g(x,u)=1$ for all $u \geq u_{*}$. Furthermore, for $g \in \mathfrak{G}_{\leq 1}$, define $F_g(\xi):= \exp(-\langle \log g, \cdot \rangle )$.
\end{definition}

To prove tightness of $(\xi_t)_{t \geq 0}$ in the vague topology, it suffices to prove tightness of $(F_g(\xi_t))_{t \geq 0}$ as a real-valued stochastic process. For a proof of this statement refer to Appendix \ref{sec: Topologies on Lookdown}.

\begin{lemma}
For all $g \in \mathfrak{G}_{\leq 1}$, the processes $\{(F_g(\xi^N_t))_{t \geq 0}: N \in \mathbb{N}\}$ is tight as a real-valued stochastic process.
\end{lemma}
\begin{proof}
Once again, we intend to apply the Aldous Criterion. 
To do so, we need to prove that for all $\epsilon > 0$,
there exists $n_0 > 0$ and $\delta > 0$ such that 
\begin{equation}
\sup_{n \geq n_0}\sup_{\theta \in [0, \delta]}\mathbb{P}\{|F_g(\xi^n_{\tau+\theta})-F_g(\xi^n_{\tau})| > \epsilon\} < \epsilon
\end{equation} 
holds. 
It suffices to show that for sufficiently small $\delta > 0$,
$\limsup_{n \to \infty}\mathbb{E}[|F_g(\xi^n_{\tau_n + s})-F_g(\xi^n_{\tau_n})|^2] < \epsilon$
for all $\theta \in [0, \delta]$.

First, note that only events happening on particles with levels smaller than $u_g$ at time $\tau_n$ will contribute to the difference $|F_g(\xi^n_{\tau_n + s})-F_g(\xi^n_{\tau_n})|$.

To calculate this difference, we couple $(\xi^n_{\tau_n + s})_{s \in [0,\delta]}$ with the process $(\hat{\xi}^n_{\tau_n + s})_{s \in [0,\delta]}$ which has the same level dynamics as $(\xi^n_{\tau_n + s})_{s \in [0,\delta]}$ but with no birth. 
With this coupling, 
we have 
$\mathbb{E}[|F_g(\xi^n_{\tau_n + s})-F_g(\xi^n_{\tau_n})|^2] \leq 2\mathbb{E}[|F_g(\xi^n_{\tau_n + s})-F_g(\hat{\xi}^n_{\tau_n + s})|^2]+ 2\mathbb{E}[|F_g(\hat{\xi}^n_{\tau_n + s})-F_g(\xi^n_{\tau_n})|^2]$

We first try to control $\mathbb{E}[|F_g(\hat{\xi}^n_{\tau_n + s})-F_g(\xi^n_{\tau_n})|^2]$. 
This difference is simply influenced by the level dynamics. Therefore,
\begin{equation}
\begin{aligned}
 &\mathbb{E}[|F_g(\hat{\xi}^n_{\tau_n + s})-F_g(\xi^n_{\tau_n})|^2]\\
 \leq &
 \mathbb{E}\left[ \int_{0}^{\delta} \sum_{(x,u)\in\xi^n_{\tau_n+s}}\,
    \left|
        \frac{\theta}{n} \gamma(x,\eta) \int_{\IR^d} r(y, \eta) q_\theta(x, dy) u^2 -b_{\theta}(x,\eta)u
    \right|^2 \partial_u g(x,u)^2 ds\right]\\
 \leq & ||g'||_{\infty}
 \mathbb{E}\left[ \int_{0}^{\delta} \sum_{(x,u)\in\xi^n_{\tau_n+s}}\,
    \left| \frac{\theta}{n}  \gamma(x,\eta)  u_g^2 + (2  \gamma(x,\eta) + F(x,\eta))u_g
    \right|^2  ds\right] + O(1/\theta)\\
\leq & ||g'||_{\infty}u_g
 \mathbb{E}_{\eta}\left[ \int_{\tau_n}^{\tau_n+\delta} \int_{\mathbb{R}^d}
    \left| \frac{\theta}{n}  \gamma(x,\eta^n_{s})  u_g^2 + (2  \gamma(x,\eta^n_{s}) + F(x,\eta^n_{s}))u_g
    \right|^2 \eta^N_s(dx) ds\right] + O(1/\theta),
\end{aligned}
\end{equation}
so this term can be controlled with Lemma \ref{lem:tightness_for_F}.

We now control 
$\mathbb{E}[|F_g(\xi^n_{\tau_n + s})-F_g(\hat{\xi}^n_{\tau_n + s})|^2]$.
Note that this difference is purely contributed by the number of birth with both the parent and child having levels below $u_g$,
and the simple inequality 
$\mathbb{E}[|F_g(\xi^n_{\tau_n + s})-F_g(\hat{\xi}^n_{\tau_n + s})|^2] \leq 4 \mathbb{P}\{|F_g(\xi^n_{\tau_n + s})-F_g(\hat{\xi}^n_{\tau_n + s})|>0\}$
holds. 

Therefore it suffices to prove that within a small interval of length $\delta > 0$,
the probability of such birth events happening is small.

This term can be controlled by the uniform boundedness of $\gamma(x,m)$.
For the birth event to contribute to the difference, the parent must have a level below $u_g$, and give birth to a child at level $u + \ell < u_g$.
Therefore, the total rate of relevant birth events satisfies
\begin{equation}
\sum_{(x,u) \in \xi^n_{\tau_n}([0,u_g] \times \overline{\mathbb{R}^d})}  \gamma(x,\eta^n_{\tau_n}) \mathbbm{1}_{u+ \ell < u_g }
\leq C_{\gamma} \xi^n_{\tau_n}([0,u_g] \times \overline{\mathbb{R}^d}.
\end{equation}
Therefore, with an application of Markov inequality, 
we have
\begin{equation}
\begin{aligned}
\mathbb{P}\{|F_g(\xi^n_{\tau_n + s})-F_g(\hat{\xi}^n_{\tau_n + s})|>0\}
\leq & \mathbb{P}[Poi(C_{\gamma} \xi^n_{\tau_n}([0,u_g] \times \overline{\mathbb{R}^d})s) \geq  1]\\
\leq & \mathbb{E}[Poi(C_{\gamma} \xi^n_{\tau_n}([0,u_g] \times \overline{\mathbb{R}^d})s)]\\
\leq & \mathbb{E}[Poi(C_{\gamma} \xi^n_{\tau_n}([0,u_g] \times \overline{\mathbb{R}^d})s)]\\
\leq & C_{\gamma}  \mathbb{E}[\xi^n_{\tau_n}([0,u_g] \times \overline{\mathbb{R}^d})]s\\
\leq & C_{\gamma}  u_g\mathbb{E}[\eta^n_{\tau_n}(\overline{\mathbb{R}^d})]s\\
<&  \epsilon.
\end{aligned}
\end{equation}

Combining all inequalities above, we have established Aldous Criterion and thus, tightness for the lookdown representation. 
\end{proof}

% % % % % % % % % % % % %
\subsubsection{Convergence of generators for the lookdown process}
    \label{sec:lookdown_generator_proofs}


\comment{
    I did this informal writeup of convergence of the lookdown generator
    without looking at the previous version
    mostly to double-check what we have there
    (which is probably better).
}

In Definition~\ref{defn:lookdown_mgale} we saw that the generator
of $(\lp_t)_{t \ge 0}$ is the sum of two parts,
equations \eqref{eqn:birth_generator} and \eqref{eqn:level_generator}.
Assume that there is a $u_*$ such that $g(x, u) = 1$ for all $u \ge u_*$.
We consider these two in turn.

First, the contribution from births is
\begin{align*}
\begin{split}
f(\lp)
&\mapsto
    f(\lp)
    \sum_{(x, u) \in \lp}
    2 \gamma(x, \eta)
    \bigg\{
        \frac{1}{2 N}
        \int_u^N
        g(x, u_1) du_1
        \frac{
            \theta \int_{\IR^d} (g(y, u) - g(x, u)) r(y, \eta) q(x, dy)
        }{
            g(x, u)
        }
    \\ & \qquad \qquad \qquad {}
        + \frac{\theta}{N}
        \int_u^N \int_{\IR^d}
        \left( \frac{g(y, u_1) + g(x, u_1)}{2} - 1 \right)
        r(y, \eta) q(x, dy)
    \bigg\}
    .
    \end{split}
\end{align*}
We have that
\begin{align*}
    &\theta \int_{\IR^d} (g(y, u) - g(x, u)) r(y, \eta) q(x, dy) \\
    &\qquad =
    \theta \int_{\IR^d} (r(y, \eta) g(y, u) - r(x, \eta) g(x, u)) q(x, dy)
    - \theta \int_{\IR^d} g(x, u) (r(y, \eta) - r(x, \eta)) q(x, dy) \\
    &\qquad \to
    \DG(gr)(x) - g(x) \DG r(x) \qquad \text{as }N \to \infty.
\end{align*}
i.e., to
$\DG\left(g(\cdot, u) r(\cdot, \smooth{r}\eta(\cdot))\right)(x)
- g(\cdot, u) \DG\left(r(\cdot, \smooth{r}\eta(\cdot))\right)(x)$.
Furthermore, by our assumption on $g$,
$\int_u^N g(x, u_1) du_1 / N \to 1$ for all $x$ and $u$,
so the first term here converges to
\begin{align*}
    \gamma(x, \eta)
        \frac{
            \DG\left( g(\cdot, u) r(\cdot, \smooth{r}\eta(\cdot))\right)(x)
            -
            g(\cdot, x) \DG\left( r(\cdot, \smooth{r}\eta(\cdot))\right)(x)
        }{
            g(x, u)
        } .
\end{align*}
This corresponds to each particle moving according to the motion induced by
$g \mapsto \gamma \DG(gr) - g \DG r$.
\comment{TODO: explain the $r$-tilted $\DG$ motion somewhere earlier, introducing notation for it.}
As for the second term,
since $\int g(y, u_1) q(x, dy) \to g(x, u_1)$
and $\theta/N \to \alpha$,
it converges to
\begin{align*}
    2 \alpha
    \gamma(x, \eta)
    r(x, \eta) 
    \int_u^\infty
    \left( g(x, u_1) - 1 \right)
    du_1
\end{align*}
This corresponds to births at higher levels at rate $2 \alpha \gamma r$.


The remaining term is \eqref{eqn:level_generator}:
\begin{align*}
    f(\lp)
    \mapsto
    f(\lp)
    \sum_{(x, u) \in \lp}
    \left(
    \theta
        N^{-1} \gamma(x,\eta) \int_{\IR^d} r(y, \eta) q(x, dy) u^2
        -
        b(x, \eta)u
    \right)
    \frac{\partial_u g(x,u)}{g(x,u)} .
\end{align*}
The first part only uses the fact that $\theta/N \to \alpha$,
while as noted in \eqref{eqn:b_limit}, $b$ converges to $\gamma \DG r + F$,
so in total this term converges to
\begin{align*}
    f(\lp)
    \mapsto
    f(\lp)
    \sum_{(x, u) \in \lp}
    \left(
    \alpha
        \gamma(x,\eta) \int_{\IR^d} r(y, \eta) q(x, dy) u^2
        -
        \left\{
            \gamma(x, \eta) \DG r(x, \eta) + F(x, \eta)
        \right\} u
    \right)
    \frac{\partial_u g(x,u)}{g(x,u)} .
\end{align*}
In other words, the level of a particle at $x$ evolves according to
\begin{align*}
    \dot u
    =
    \alpha
        \gamma(x,\eta) \int_{\IR^d} r(y, \eta) q(x, dy) u^2
        -
        \left\{
            \gamma(x, \eta) \DG r(x, \eta) + F(x, \eta)
        \right\} u .
\end{align*}



% % % % % % % % % % % % %
\subsection{Motion of ancestral lineages}

In this section we prove Theorem~\ref{thm:lineages}.
The argument follows directly from the discussion in Section \ref{sec:limiting_lines_of_descent}.

\begin{proof}[of Theorem~\ref{thm:lineages}]

\comment{(TODO: Use notation for the motion of a lineage introduced in statement of theorem.)}

Here
\comment{(and in the statement of the theorem)}
we take the high-density, deterministic limit
(so, $\theta, N \to \infty$ and $\theta/N \to 0$)
and so suppose that the limiting process has density $\varphi_t(x)$
at location $x$ and time $t$.
Let $Y$ denote the spatial motion followed by a single line of descent.
Above equation~\eqref{eqn:limiting_generator},
we showed that $Y$ is a diffusion with generator at time $s$
$$
    \mathcal{L}^Y_s g(x) = \gamma ( \DG(rg)(x) - g \DG r(x) ) .
$$
The diffusion is time-inhomogeneous if the density is not constant with time
(since recall that the coefficients $\gamma$ and $r$ depend on the density).
Formally, the intensity of levels at $y$ at time $t$
that are descended from individuals that were at $x$ at time $s$
(with $s < t$) is
\begin{equation} \label{eqn:formal_intensity}
    \varphi_s(x) \IE_{s,x} \left[
        \exp\left(
            \int_s^t (F + \gamma \DG r)(Y_u) du
        \right)
        \ind_{Y_t = y}
    \right]
    dy ,
\end{equation}
where the subscript $s, x$ in the expectation indicates that $Y_s = x$.
To see why this should be true, 
suppose that an ancestor has level $v$. Conditional on its 
spatial motion $\{Y_u\}_{s\leq u\leq t}$, its level at time $t$ will
be $v \exp(-\int_s^t(F+\gamma\Delta r)(Y_u)du)$. This will be less than a given level 
$N$ if $v < N \exp(\int_s^t(F+\gamma\Delta r)(Y_u)du)$. 
The intensity of levels at $y$ that are descended from individuals at
$x$ can therefore be obtained as the limit as $N\to\infty$ of 
$1/N$ times the number of levels at $x$ at time zero with
$u<N \exp(\int_s^t(F+\gamma\Delta r)(Y_u)du)$ and for which
the corresponding individual is at $y$ at time $t$, which is 
precisely the quantity in~\eqref{eqn:formal_intensity}. 

By our construction in Section~\ref{sec:limiting_lines_of_descent},
when we integrate~\eqref{eqn:formal_intensity}
with respect to $x$ we recover $\varphi_t(y)dy$. 
Consider an individual sampled at location $y$ at time $t$,
and write $p(t,s,y,x)$ for the probability density
that their ancestor at time $s$ was at $x$.
As a consequence of~\eqref{eqn:formal_intensity},
still formally,
\begin{equation}
\label{eqn:ptsyx}
    p(t,s,y,x)
    =
    \frac{\varphi_s(x)}{\varphi_t(y)}
    \IE_{s,x}\left[
        \exp\left( \int_s^t (F + \gamma\DG r)(Y_u) du \right)
        \ind_{Y_t=y}
    \right]
\qquad \text{ for } s < t.
\end{equation}

To make~(\ref{eqn:ptsyx}) meaningful, we multiply by suitable test functions
$f$ and $g$ and integrate.
\begin{align*}
&\int \int f(y) \varphi_t(y) p(t,s,y,x) g(x) dy dx \\
&\qquad =
\int g(x) \varphi_s(x)
\IE_{x,s}\left[
    \exp\left(
        \int_s^t(F+\gamma\DG r)(Y_u)du
    \right)f(Y_t)
\right] dx .
\end{align*}

Writing $\hat{T}_{t,s}$ for the time-inhomogeneous semigroup
corresponding to the motion of ancestral lineages backwards in time
(that is, $\hat{T}_{t,s} f(y) = \int p(t,s,x,y) f(x) dy$),
we can write this as 
$$
    \int f(y)\varphi_t(y)\hat{T}_{t,s}g(y)dy
    =
    \int g(x) \varphi_s(x)
        \IE_{s,x} \left[
            \exp\left(
                \int_s^t(F+\gamma\DG r)(Y_u)du
            \right)f(Y_t)
        \right]dx.
$$
Differentiating with respect to $t$ at $t=s$, and writing 
$\Lgen_s$ for the generator of $\hat{T}_{t,s}$ at time $s$, we find
\begin{align*}
&
    \int f(y) \left\{
        \varphi_s(y) \Lgen_s g(y)
        + \left(
            r(y) \DG^*(\gamma \varphi_s)(y) + \varphi_s(y) F(y)
        \right) g(y)
    \right\} dy \\
&\qquad =
    \int g(x) \varphi_s(x) \left(
        {\mathcal L}^Y_s f(x) + (F(x)+\gamma(x) \DG r(x)) f(x)
    \right) dx \\
&\qquad =
    \int f(x) \left(
        ({\mathcal L}^Y_s)^* (\varphi_s g)(x) + (F(x) + \gamma(x) \DG r(x)) \varphi_s(x) g(x)
    \right) dx ,
\end{align*}
where $({\mathcal L}^Y_s)^*$ is the adjoint of ${\mathcal L}^Y_s$
and we have used that $\varphi_s$ solves $\dot \varphi = r \DG^*(\gamma \varphi) + F \varphi$.
Since $f$ was arbitrary,
\begin{eqnarray*}
\Lgen_s g
    &=&
    \frac{1}{\varphi_s} \left[
        ({\mathcal L}^Y_s)^* (\varphi_s g)
        + \gamma \varphi_s g \DG(r)
        - r g \DG^*(\gamma \varphi_s)
    \right] .
\end{eqnarray*}
(Note that the $\phi_s F g$ terms have cancelled.)
Since the adjoint of ${\mathcal L}^Y_s$ is
\begin{align*}
    ({\mathcal L}^Y_s)^* f
    &=
    r \DG^* (\gamma f) - \gamma f \DG r ,
\end{align*}
we can rewrite the generator of a lineage as 
\begin{eqnarray*}
\Lgen_s g
    &=&
    \frac{r}{\varphi_s} \left[
        \DG^* (\gamma \varphi_s g)
        - g \DG^*(\gamma \varphi_s)
    \right] .
\end{eqnarray*}
This is equation \eqref{eqn:lineage_generator}.


To simplify to equation \eqref{eqn:lineage_generator2},
this, first define
$
    \DD f(x) = \sum_{ij} \covq_{ij} \partial_{ij} f(x),
$
% and note that $\DD$ satisfies the following identity:
% \begin{align}
%     \DD(fg)
%     &=
%     f \DD g
%     + 2 (\covq \grad f) \cdot \grad g
%     + g \DD f .
% \end{align}
and so the adjoint of $\DD$ is
$$
    \DD^* f(x)
    =
    \sum_{ij} \partial_{ij} (\covq_{ij} f(x)) .
$$
Note that $\DD^*$ satisfies the following identity:
\begin{align*}
    \DD^*(fg)
%     &=
%     \sum_{ij} \left\{
%         g \partial_{ij} (\covq_{ij} f)
%         + 2 \partial_{i} (\covq_{ij} f) \partial_j(g)
%         + \covq_{ij} f \partial_{ij} g
%     \right\} \\
    &=
    \sum_{ij} \left\{
        g \partial_{ij} (\covq_{ij} f)
        + 2 f \partial_{i} (\covq_{ij}) \partial_j(g)
        + 2 \covq_{ij} \partial_{i} (f) \partial_j(g)
        + \covq_{ij} f \partial_{ij} g
    \right\} \\
    &=
    g \DD^* f
    + 2 f \vec{c} \cdot \grad g
    + 2 (C \grad f) \cdot \grad g
    + f \DD g ,
\end{align*}
where $\vec{c}_j = \sum_i \partial_j C_{ij}$.
So, with $f = \gamma \varphi_s$,
\begin{eqnarray*}
\Lgen_s g
    &=&
    \frac{r}{\varphi_s} \left[
        \DD^*(\gamma \varphi_s g) - \grad \cdot (\gamma \varphi_s g \meanq)
        - g \DD^*(\gamma \varphi_s) + g \grad \cdot (\gamma \varphi_s \meanq)
    \right] \\
    &=&
    \frac{r}{\varphi_s} \left[
        \gamma \varphi_s \DD g
        + 2 \gamma \varphi_s \vec{c} \cdot \grad g
        + 2 (C \grad (\gamma \varphi_s)) \cdot \grad g
        - \gamma \varphi_s \meanq \cdot \grad g
    \right] \\
    &=&
    r \gamma \left[
        \DD g
        + 2 \vec{c} \cdot \grad g
        + 2 (C \grad \log(\gamma \varphi_s)) \cdot \grad g
        - \meanq \cdot \grad g
    \right] ,
\end{eqnarray*}
which is equation \eqref{eqn:lineage_generator}.

\end{proof}


\begin{proof}[of Corollary \ref{cor:lineages_simple}]

In this section, we need only
simplify equation \eqref{eqn:lineage_generator}
in the case where $\covq = \sigma^2 I$.

    \comment{TODO}

\end{proof}

\begin{proof}[of Corollary~\ref{cor:wavefront}
    \comment{
        TODO make non-redundant with proof of Theorem \ref{thm:lineages}.
    }

Let $Y$ denote the diffusion followed by following a single descendent line in this frame; 
i.e.~the diffusion with generator ${\cal L}^Y\phi=\gamma\Delta(r\phi)-\gamma\phi\Delta r -\mathfrak{c}\nabla\phi $. Note in particular here we take $\mathcal{B}=\Delta$. We start by expressing the evolution of a singles ancestry line in terms of the descendent line $Y$.

Formally, the intensity of levels at $y$ at time $t$ that are descended from 
individuals that were at $x$ at time $0$ is
\begin{equation}
\label{formal intensity}
w(x)\IE_x\left[\exp\left(\int_0^t (F+\gamma \Delta r)(Y_s)ds\right)\ind_{Y_t=y}\right]dy,
\end{equation}
where the subscript $x$ in the expectation indicates that $Y_0=x$.
To see why this should be true, 
suppose that an ancestor has level $u$. Conditional on its 
spatial motion $\{Y_s\}_{0\leq s\leq t}$, its level at time $t$ will
be $u\exp(-\int_0^t(F+\gamma\Delta r)(Y_s)ds)$. This will be less than a given level 
$N$ if $u<N \exp(\int_0^t(F+\gamma\Delta r)(Y_s)ds)$. 
The intensity of levels at $y$ that are descended from individuals at
$x$ can therefore be obtained as the limit as $N\to\infty$ of 
$1/N$ times the number of levels at $x$ at time zero with
$u<N \exp(\int_0^t(F+\gamma\Delta r)(Y_s)ds)$ and for which
the corresponding individual is at $y$ at time $t$, which is 
precisely the quantity in~(\ref{formal intensity}). 

It is easy to check that when we integrate~(\ref{formal intensity})
with respect to $x$ we recover $w(y)dy$. 
As a consequence of~(\ref{formal intensity}), writing $p(t,y,x)$ for
the probability density that the ancestor $t$ units in 
the past of an individual sampled 
at $y$ was at $x$, then, still formally,
\begin{equation}
\label{ptyx}
p(t,y,x)=\frac{w(x)}{w(y)}\IE_x\left[\exp\left(\int_0^t (F+\gamma\Delta r)(Y_s)ds\right)
\ind_{Y_t=y}\right].
\end{equation}

To make~(\ref{ptyx}) meaningful, we multiply by suitable test functions and 
integrate.
$$\int\int \psi(y)w(y)p(t,y,x)\phi(x)dydx=\int\phi(x)w(x)
\IE_x\left[\exp\left(\int_0^t(F+\gamma\Delta r)(Y_s)ds\right)\psi(Y_t)\right]dydx.$$

Writing $\hat{T}_t$ for the semigroup corresponding to the motion of 
ancestral lineages backwards in time (that is the semigroup corresponding
to $p(t,x,y)$), we can write this as 
$$\int \psi(y)w(y)\hat{T}_t\phi(y)dy=\int\phi(x)w(x)
\IE_x\left[\exp\left(\int_0^t(F+\gamma\Delta r)(Y_s)ds\right)\psi(Y_t)\right]dx.$$
Differentiating with respect to $t$ at $t=0$, and writing 
$\hat{\mathcal L}$ for the generator of $\hat{T}_t$, we find
\begin{eqnarray*}
\int \psi(y)w(y)\widehat{\mathcal L}\phi(y)dy&=&
\int\phi(x)w(x)\left({\mathcal L}^Y\psi(x)+\psi(x)(F+\gamma\Delta r)\psi(x)\right)dx\\
&=&\int\phi(x)w(x)\left(\gamma\Delta(r\psi)-\gamma\psi\Delta r -\mathfrak{c}\nabla\psi 
+(F+\gamma\Delta r)(\psi)\right)dx\\
&=&\int\phi(x)w(x)\left(\gamma\Delta(r\psi)-\mathfrak{c}\nabla\psi+F(\psi)\right)dx\\
&=&
\int\psi(x)\left[r\Delta(\gamma \phi w)+F\phi w+\mathfrak{c}\nabla(\phi w)\right]dx.
\end{eqnarray*}
Since $\psi$ was arbitrary,
\begin{eqnarray*}
\hat{\mathcal L}\phi&=&\frac{1}{w}\left[ r\Delta (\gamma\phi w)+F\phi w
+\mathfrak{c}\nabla(\phi w)\right]\\
&=&\frac{\phi}{w}\left[r\Delta(\gamma w)+Fw+\mathfrak{c}\nabla w\right]
+\frac{1}{w}\left[r\gamma w\Delta \phi +2\nabla (\gamma w)
\nabla\phi+\mathfrak{c}w\nabla\phi\right]\\
&=&r\gamma\left(\Delta\phi+2\frac{\nabla(\gamma w)}{\gamma w}\nabla\phi\right)
+\mathfrak{c}\nabla\phi,
\end{eqnarray*}
where we have used that $w$ solves~(\ref{profile}).

\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Results on Lookdown Representations}
    \label{sec: Markov Mapping Theorem Application}
\subsection{Generators for the spatial processes}
Before defining the generator of the lookdown representations, 
we recall the generator of the spatial processes.

We first define our pre-limit
$(\eta^{N}(t))_{t>0}$ as a solution
to a Martingale Problem
with generator $\Pgen^{N}$.


\begin{definition}
    \label{def2: MP definition of pre-limit}
Let $F \in \mathcal{C}^{2}(\IR)$ and $f \in \mathcal{C}_{0}(\IR^d)$.
We define $\Pgen^N$ to be a generator
acting on test functions of the form
$F_f(\eta):=F (\langle f, \eta \rangle)$
such that 
\begin{equation}
\begin{aligned}
\Pgen^{N} F_f(\eta):=& \int_{\IR^d} \theta\gamma(x, \eta) \left\{\int_{\IR^d} \left\{F(\langle f, \eta \rangle + f(y)/N )-F(\langle f, \eta \rangle )\right\}r(y, \eta)q_{\theta}(x,dy)\right\}N\eta(dx)\\
&+\int_{\IR^d} \theta \mu(x, \eta) \left\{F(\langle f, \eta \rangle - f(x)/N )-F(\langle f, \eta \rangle )\right\}N\eta(dx).
\end{aligned}    
\end{equation}
In this case, our pre-limit model $(\eta^{N}(t))_{t \geq 0}$ is the solution to the Martingale Problem $(\Pgen^{N}, \eta_0)$, i.e. 
for all $F \in \mathcal{C}^{2}(\IR)$
and $f \in \mathcal{C}_{0}(\IR^d)$,
$$M_t:=F_f(\eta^{N}_t)-F_f(\eta_0)
-\int_{0}^{t}\Pgen^{N}F_f(\eta^{N}_s)ds$$
is a martingale, and the initial condition is distributed as a Poisson measure with mean measure $\eta_0$.
\end{definition}

\begin{remark}
Taking $F(x)=x$ and $F(x)=x^2$, we can recover Equation \eqref{eqn:limiting_mgale_problem} and \eqref{eqn:limiting_mgale_variation} for the limit.
Similarly, we can recover
Definition \ref{defn:mgale_construction},
an equivalent characterisation
of our pre-limit model.
\end{remark}

We now write the candidate generator for the limiting object.

\begin{definition}
    \label{def2: MP definition of limit}
Let $F \in \mathcal{C}^{2}(\IR)$ and $f \in \mathcal{C}_{0}(\IR^d)$.
We define $\Pgen$ to be a generator
acting on test functions of the form
$F_f(\eta):=F (\langle f, \eta \rangle)$
such that 
\begin{equation}
    \label{eq: Limit Generator Definition}
\begin{aligned}
\Pgen^{\infty} F_f(\eta):=& F'(\langle f, \eta \rangle)
                   \times \left\{
                   \big\langle
                        \gamma(x, \smooth{\gamma} \eta(x))
                            \mathcal{B}\left(
                            f(\cdot) r(\cdot, \smooth{r} \eta(\cdot))
                            \right)(x)
                    +
                    f(x)
                        F(x, \smooth{F} \eta(x)),
                        \eta(dx)
                    \big\rangle
                   \right\}\\
                &+ \alpha F''(\langle f, \eta \rangle)
                  \times \left\{
                  \big\langle
                    \gamma \left( x, \eta \right)
                    r\left(x,\eta \right)
                    f^2(x),
                    \eta (dx)
                    \big\rangle 
                  \right\}.
\end{aligned}    
\end{equation}
We define $(\eta^{\infty}(t))_{t \geq 0}$ to be the solution to the Martingale Problem $(\Pgen^{\infty}, \eta_0)$, i.e. 
for all $F \in \mathcal{C}^{2}(\IR)$
and $f \in \mathcal{C}_{0}(\IR^d)$,
$$M_t:=F_f(\eta^{\infty}_t)-F_f(\eta_0)
-\int_{0}^{t}\Pgen^{\infty}F_f(\eta^{\infty}_s)ds$$
is a martingale, with initial condition distributed as a Poisson measure with mean measure $\eta_0$.
\end{definition}




\subsubsection{Convergence of the Generators for the Spatial Models}

Here, just as a sanity check, 
we make sure that the limiting generator is indeed the limit of the generators.

\begin{lemma}[Identifying the limit as a solution to the martingale problem]
    \label{lem:limit_mgale}
As $N \to \infty$,
$(\eta^{N}_t)_{t \geq 0}$ converges in $\mathcal{D}_{[0,\infty)}(\mathcal{M}_f(\IR^d))$
to $(\eta^{N}(t))_{t \geq 0}$.
\end{lemma}
\begin{proof}
We use Theorem 4.8.10 in \cite{EK}.
First of all, the set of functions
$\{F_f(\eta):= F(\langle f, \eta \rangle ),~
F \in \mathcal{C}^{2}(\IR), ~
f \in \mathcal{C}_{0}(\IR^d)\}$
is a dense and convergence-determining 
subset of functions in $\mathcal{M}_f(\IR^d)$.
Therefore, it suffices to show that for any $t>0$,
\begin{equation}
    \label{eq: Convergence Condition}
\lim_{N \to \infty}
\mathbb{E}\left[
\left(
F_f(\eta^{N}_{t+\tau})-F_f(\eta^{N}_t)
-\int_{t}^{t+\tau}\Pgen^{\infty}F_f(\eta^{N}_s)ds
\right)
\prod_{i=1}^{k}\langle h_i,\eta^{N}_{t_i} \rangle
\right]=0
\end{equation}
for all $k\geq 0$, $0\leq t_1<t_2<...,t_k \leq t < t+\tau$,
and $h_1,...,h_k \in \mathcal{C}_{0}(\IR^d)$.

To prove this,
we make use of the Martingale Problem characterisation
of $(\eta^{N}_t)_{t \geq 0}$.
From Definition \ref{def: MP definition of pre-limit},
we can apply Tower Law
and obtain that
\begin{equation}
    \label{eq: Prelimit MP Application}
\lim_{N \to \infty}
\mathbb{E}\left[
\left(
F_f(\eta^{N}_{t+\tau})-F_f(\eta^{N}_t)
-\int_{t}^{t+\tau}\Pgen^{N}F_f(\eta^{N}_s)ds
\right)
\prod_{i=1}^{k}\langle h_i,\eta^{N}_{t_i} \rangle
\right]=0
\end{equation}
for all $k\geq 0$, $0\leq t_1<t_2<...,t_k \leq t < t+\tau$,
and $h_1,...,h_k \in \mathcal{C}_{0}(\IR^d)$.

Therefore, it suffices to show that 
\begin{equation}
    \label{eq: Convergence of Spatial Generator}
\lim_{N \to \infty}
\mathbb{E}\left[
\int_{t}^{t+\tau}
\left|
\Pgen^{N}F_f(\eta^{N}_s)
-\Pgen^{\infty}F_f(\eta^{N}_s)
\right|
ds
\times 
\prod_{i=1}^{k}\langle h_i,\eta^{N}_{t_i} \rangle
\right]=0
\end{equation}
We apply Taylor Expansion
around the term $F(\langle f, \eta \rangle )$
up to third order.
This gives
\begin{equation}
    \label{eq: Taylor Expansion calculations i}
\begin{aligned}
&F(\langle f, \eta \rangle + f(y)/N )-F(\langle f, \eta \rangle )\\
=& F'(\langle f, \eta \rangle ) \frac{f(y)}{N}
+\frac{1}{2}F''(\langle f, \eta \rangle ) \frac{f^2(y)}{N^2}
+\frac{1}{6}F'''(w ) \frac{f^3(y)}{N^3}
\end{aligned}
\end{equation}
for some $w \in [\langle f, \eta \rangle, \langle f, \eta \rangle+ ||f||/N]$.
Similarly,
\begin{equation}
    \label{eq: Taylor Expansion calculations ii}
\begin{aligned}
&F(\langle f, \eta \rangle - f(x)/N )-F(\langle f, \eta \rangle )\\
=& - F'(\langle f, \eta \rangle ) \frac{f(x)}{N}
+\frac{1}{2}F''(\langle f, \eta \rangle ) \frac{f^2(x)}{N^2}
-\frac{1}{6}F'''(v ) \frac{f^3(x)}{N^3}
\end{aligned}
\end{equation}
for some $v \in [\langle f, \eta \rangle - ||f||/N, \langle f, \eta \rangle ]$.

We may now rewrite $\mathcal{P}^NF(\langle f, \eta \rangle)$ as 
\small
\begin{equation} 
    \label{eq: Pre-Limit Generator Expanded}
\begin{aligned}
\Pgen^{N} F_f(\eta):=& F'(\langle f, \eta \rangle)\int_{\IR^d}  \theta\left\{\int_{\IR^d} \left\{f(y)\gamma(x, \eta) r(y,\eta) -f(x)\mu(x, \eta)\right\}q_{\theta}(x,dy)\right\}\eta(dx)\\
&+\frac{1}{2}\frac{\theta}{N}F''(\langle f, \eta \rangle)\int_{\IR^d} 
 \left\{\gamma(x, \eta)\int_{\IR^d} \left\{f^2(y) r(y,\eta) \right\}q_{\theta}(x,dy)+f^2(x)\mu(x, \eta)\right\}\eta(dx)\\
&+\frac{1}{6}\frac{\theta}{N^2}\int_{\IR^d}\left\{ 
F'''(w) \left\{\gamma(x, \eta)\int_{\IR^d} f^3(y) r(y,\eta) q_{\theta}(x,dy)\right\}-F'''(v)f^3(x)\mu(x, \eta)\right\}\eta(dx)
\end{aligned}    
\end{equation}
\normalsize
for some $w,v \in [\langle f,\eta \rangle - ||f||/N, \langle f,\eta \rangle + ||f||/N]$.

As $N\to \infty$, 
$\theta/N \to \alpha$, 
$\theta/N^2 \to 0$,
and 
$$\int_{\IR^d} f^m(y) r(y,\eta) q_{\theta}(x,dy) \to f^m(x)r(x, \eta)$$
for $m=2,3$,
the third term in Equation \eqref{eq: Pre-Limit Generator Expanded} vanishes
while the second term converges to
that of Equation \eqref{eq: Limit Generator Definition}.
Furthermore,
$\mu(x,\eta) = r(x,\eta)\gamma(x,\eta)-\frac{1}{\theta}F(x,\eta) \to r(x,\eta)\gamma(x,\eta),$
so $f^2(x)\mu(x,\eta) \to r(x,\eta)\gamma(x,\eta) f^2(x)$.





Finally, by Equation \eqref{mean measure},
\eqref{eqn:rewritten mean measure}, \eqref{eqn:near_critical} and
\eqref{limit of mean measure equation},
we have 
\begin{multline}
 \theta\left\{\int_{\IR^d} \left\{f(y)\gamma(x, \eta) r(y,\eta) -f(x)\mu(x, \eta)\right\}q_{\theta}(x,dy)\right\}
 \\
\to \gamma(x, \eta)                           \mathcal{B}\left(
                            f(\cdot) r(\cdot, \eta)
                            \right)(x)
                    +
                    f(x)
                        F(x, \eta),
\end{multline}
so comparing terms we have
\begin{equation}
\lim_{N\to \infty} |\mathcal{P}^{N}F\langle f, \eta \rangle - \mathcal{P}^{\infty}F\langle f, \eta \rangle| \to 0,   
\end{equation}
and so by Dominated Convergence Theorem,
we establish Equation 
\eqref{eq: Convergence of Spatial Generator}
which concludes our proof.
\end{proof}



\subsection{Generator of Lookdown Representations}

We now define the generator of the lookdown representations.
\begin{definition}[Martingale Problem Characterisation]
    \label{defn:lookdown_mgale}
For given positive values of $N$ and $\theta$
and $\lp_0 \in \mathcal{M}_F(\IR^d \times [0,N])$,
we define $(\lp^{N}_t)_{t \geq 0}$
to be the unique solution to the martingale problem
with initial condition $\lp_0$ and generator $A^{N}$ that satisfies
\begin{equation*}
\begin{split}
& A^{N}f(\lp ) \\
&\quad =
    f(\lp)
    \,\sum_{(x,u)\in \lp}\,
    2 \gamma(x, \eta)
    \Bigg\{ \frac{1}{2N} \int_u^N g(x,u_1) du_1
            \frac{
                \theta \int_{\IR^d}
                (g(y,u) - g(x,u))
                r(y, \eta) q_{\theta}(x,dy)
            }{ g(x,u) }
        \\
    &\qquad\qquad\qquad\qquad\qquad\qquad\qquad {} +
        \frac{\theta}{N} \int_u^N
        \int_{\IR^d}\left(
            \frac{ g(y,u_1) + g(x,u_1) }{ 2 } - 1
        \right)
        r(y, \eta) q_{\theta}(x,dy)
        du_1
    \Bigg\}\\
    &\qquad\qquad {} +
    f(\lp) \sum_{(x,u)\in\lp}\,
    \left(
        \frac{\theta}{N} \gamma(x,\eta) \int_{\IR^d} r(y, \eta) q_\theta(x, dy) u^2 -b_{\theta}(x,\eta)u
    \right)
    \frac{\partial_u g(x,u)}{g(x,u)}
    ,
\end{split}
\end{equation*}
for all smooth functions $g \in C^{2}_{0}(\IR^d)$,
where $f(\lp) = \prod_{(x, u) \in \lp} g(x, u)$ as defined in \eqref{eqn:test_functions},
and $\eta(\cdot) = \lp(\cdot \times [0, N]) / N$ is the scaled spatial pushforward of $\lp$,
as before.
\end{definition}

Similarly, 
we define the generator for the lookdown representation of the limiting spatial model.

\begin{definition}[Martingale Problem Characterisation, $N=\infty$]
    \label{defn:limiting_lookdown_mgale}
    For a given value of $\alpha \in [0, \infty)$,
    define the operator $A$ acting on test functions of the form
    $f(\xi) = \prod_{(x, u) \in \xi} g(x, u)$ with $g \in C_0^2(\IR^d \times [0, \infty))$
    and for which there exists a $u_0$ with $g(x, u) = 1$ for all $u > u_0$.
    Then, let $A f$ be
    \begin{equation}
    \begin{aligned} \label{eqn:limiting_lookdown_generator}
    A f(\lp)
    =&
        f(\lp)  \sum_{(x, u) \in \xi}
        \gamma(x, \eta)
            \frac{
                \DG(g(\cdot, u) r(\cdot, \eta))(x) - g(x,u) \DG r(x,\eta)
            }{
                g(x, u)
            }
    \\ &+
        f(\lp) \sum_{(x, u) \in \xi}
        2 \alpha \gamma(x,\Xi)r(x,\Xi) \int_u^\infty (g(x, u_1) - 1) du_1
    \\ &+
        f(\lp) \sum_{(x, u) \in \xi}
        \left(
            \alpha \gamma(x,\Xi)r(x,\Xi) u^2
            -
            \left\{
                \gamma(x, \eta) \DG r(x, \eta) + F(x, \eta)
            \right\} u
        \right)
        \frac{\partial_u g(x, u)}{ g(x,u) }  .
    \end{aligned}
    \end{equation}
   
    We say that a $\lpmeasures$-valued process $(\xi_t)_{t \ge 0}$
    is a solution to the $(A, \lp_0)$ martingale problem
    if it has initial distribution $\lp_0$
    and $f(\lp_t) - \int_0^t Af(\lp_s) ds$ is a martingale for all $f$
    as defined above.
\end{definition}
   
\subsubsection{Exact Statement on Markov Mapping Theorem results}

\begin{theorem}
\label{teo: lookdown on pre-limit well defined}
For each $N \in \mathbb{N}$,
there exists a solution $(\lp^N_t)_{t \geq 0}$ to the $(A^N, \lp^N_0)$ martingale problem.
Furthermore, if we define $\Xi^N_t = \frac{1}{N}\lp^N_t(dx \times [0,N])$ for $t \geq 0$,
then the pre-limiting model $(\eta^N_t(dx))_{t \geq 0}$ defined in Definition \ref{defn:mgale_construction}
and $(\Xi^N_t(dx))_{t \geq 0}$
shares the same distribution on $\mathcal{D}_{\mathcal{M}(\mathbb{R}^d)}[0,\infty)$.

Finally, 
for all $h\in \mathcal{C}(\mathbb{R}^d \times [0, N])$
\begin{equation}\label{Markov Mapping Theorem Cox Condition}
\mathbb{E}\left[e^{-\int_{\mathbb{R}^d \times [0,N]} h(x,u) \xi^{N}(dx \times du)}\bigg| \mathcal{F}^{\Xi^{N}}_t\right]=e^{-\int F^{N}_h(x) \Xi^{N}_t(dx)},
\end{equation}
where 
$$F^{\epsilon,\theta,N}_h(x):=-N\log \left(\frac{1}{N}\int_{0}^{N}e^{-h(x,u)}du\right)=-N\log \left(1-\frac{1}{N}\int_{0}^{N}(1-e^{-h(x,u)})du\right).$$
In other words,
conditioned on the spatial configuration $\Xi^N_t$,
the levels of the particles must be i.i.d uniformly between $[0,N]$.

\end{theorem}

The statement for the limiting case is similar,
with the only difference being the conditional distribution 
of the levels given the spatial projection.

\begin{theorem}
\label{teo: lookdown on limit well defined}
There exists a solution $(\lp_t)_{t \geq 0}$ to the $(A, \lp_0)$ martingale problem.

Furthermore, if we define $\Xi_t = \lim_{u_0 \to \infty} \frac{1}{u_0}\lp_t(dx \times [0,u_0])$,
then the limit model $(\eta_t(dx))_{t \geq 0}$ defined in Theorem \label{thm:nonlocal_convergence}
and $(\Xi_t(dx))_{t \geq 0}$
shares the same distribution on $\mathcal{D}_{\mathcal{M}(\mathbb{R}^d)}[0,\infty)$.

Finally, 
for all $h\in \mathcal{C}(\mathbb{R}^d)$ and for all $K>0$,
\begin{equation}\label{Markov Mapping Theorem Cox Condition limit}
\mathbb{E}\left[e^{-\int_{\mathbb{R}^d \times [0,K]} h(x) \xi(dx \times du)}\bigg| \mathcal{F}^{\Xi}_t\right]=e^{- K \int_{\mathbb{R^d}}(1-e^{h(x)}) \Xi_t(dx)}.
\end{equation}
In other words,
conditionally on $(\Xi_s)_{0\leq s \leq t}$
for each $s \in[0,t]$,
$\xi_s$ is conditionally Poisson with Cox measure $\Xi_s(dx) \times du$.
\end{theorem}
    

\subsection{Markov Mapping Theorem Application on Generator $A^{N}$}\label{sec: Markov Mapping Theorem Application - Prelimit}

We now introduce some notations to simplify our calculations.
For a finite atomic measure $\xi=\sum \delta_{(x_i,u_i)} \in \mathcal{M}(\mathbb{R}^d \times [0,N])$,
we introduce the test functions
 $$f_g(\xi)= \prod_{(x_i,u_i)\in \xi} g(x_i,u_i) = \exp (\langle \log g, \xi \rangle),$$
 where $g \in \mathcal{C}(\mathbb{R}^d \times [0,\infty))$ and $g(x,u)=1$ for all $u \geq N$.
 
 To avoid any ambiguities in the proofs,
 we will strictly use the notation $\Xi^N$  
to represent the scaled push-forward measure of $\lp$,
i.e. $\Xi^N := \frac{1}{N}\sum_{(x,u) \in \lp} \delta_x$,
and only use $(\eta^N_t)_{t \geq 0}$
to refer to the PDE solutions defined in Definition  \ref{defn:mgale_construction}.

 
We also write $\textbf{x}=(x_1,x_2,...), \textbf{u}=(u_1,u_2,...)$ to be the collections of particles locations and levels respectively.
With this notation we will write $f_g(\xi), f_g(\textbf{x},\textbf{u})$ interchangeably.

We write $\Gamma^N (d\textbf{u})$ as the probability kernel
that corresponds to assigning each particle an i.i.d. uniform $[0,N]$-level, i.e.
$$\int f_g(\textbf{x},\textbf{u})\Gamma^N (d\textbf{u})=\frac{1}{N^{|\xi|}}\int_{[0,N]^{|\xi|}}f_g(\textbf{x},\textbf{u})d\textbf{u},$$
and we denote
$$\hat{g}(x):=\frac{1}{N}\int_{0}^{N}g(x,u)du,
 ~~\hat{f}_g(\xi)=\int f_g(\xi) \Gamma (d\textbf{u})$$ 
to be the spatial test functions with averaged level.
By independence of the levels of the particles,
 we have that
 \begin{align*}
\hat{f}_g(\xi)
=&\int \prod_{(x,u) \in \xi}g(x,u) \Gamma(d\textbf{u})\\
=&\prod_{(x,u)\in \xi}\frac{1}{N}\int_{0}^{N} g(x,u) du\\
=&\prod_{(x_i,u_i)\in \xi }\hat{g}(x_i)\\
=& \exp(N\log \hat{g},\Xi^N).
 \end{align*}

Furthermore,
when expressed as a function of $\Xi^N$ explicitly,
we use the notation $F_{\hat{g},N}(\Xi^N):=\hat{f}_g(\xi)$.


We will now apply Markov Mapping Theorem
while keeping notations consistent to that introduced in Section A.5 in \cite{kurtz/rodrigues:2011}
with the aid of Table \ref{Markov Mapping Theorem Notation Table}.
\begin{table}[!h]
    \centering
    \begin{tabular}{c|c}
    Notation in \cite{kurtz/rodrigues:2011} & Notation in this proof \\
    \hline\\
      $S$   &  $\mathcal{M}(\IR^d \times [0, N])$\\\\
       $S_0$  &  $\mathcal{M}(\IR^d)$\\\\
       $A$ & $A^{N}$\\\\
       $(X_t)_{t \geq 0}$ & $(\xi^N_t)_{t \geq 0}$\\\\
       $\mu_0$ & $\eta^N_0 \in \mathcal{M}(\IR^d)$\\\\
       $\nu_0$ & $\xi^N_0 \in \mathcal{M}(\IR^d \times [0,N])$\\\\
       $f(\xi)$ & $f_g(\xi) := \prod_{(x,u) \in \xi} g(x,u)$\\\\
        $\gamma_M$ & $ \mathcal{M}(\IR^d \times [0, N]) \ni \xi \to \frac{1}{N}\sum_{(x,u) \in \lp(\cdot, [0,N))} \delta_{x} \in \mathcal{M}(\IR^d)$\\\\
       $\alpha_M$ & $ \alpha_M(\Xi^N,d \lp) =     \begin{cases}
      0 & \text{if } \frac{1}{N}\sum_{(x,u) \in \lp(\cdot, [0,N))} \delta_x \neq \Xi^N\\
       \prod_{(u_i,x_i) \in \nu} \frac{1}{N} \ind_{[0,N]}du_i & \text{if } \frac{1}{N}\sum_{(x,u) \in \lp(\cdot, [0,N))} \delta_x = \Xi^N,\\
    \end{cases} $\\
&      where $du_i$ is the standard Lebesgue measure.
    \end{tabular}
    \caption{Markov Mapping Theorem Correspondence}
    \label{Markov Mapping Theorem Notation Table}
\end{table}

\begin{remark}
Note that $\gamma,\alpha$ in \cite{kurtz/rodrigues:2011}
is replaced with $\gamma_M,\alpha_M$ to avoid confusion with the $\gamma, \alpha$ used in previous sections.
\end{remark}

First of all,
note that $\gamma_M$ is Borel-measurable
and $\alpha_M$ is a transition function from $\mathcal{M}(\IR^d \times [0, N))$ to $\mathcal{M}(\IR^d)$.

More importantly, we have the following simplified expression
for an integral against $\alpha_M$:
\begin{equation}
\int_{\mathcal{M}(\IR^d\times [0,N])}A^{N}f(\xi)\alpha_M(\Xi^N, d\lp)
= \int   A^Nf(\textbf{x},\textbf{u})\Gamma^N(d\textbf{u}). 
\end{equation}

As a result, we can establish that
\begin{equation}
\begin{aligned}
\alpha_M(\Xi^N,\gamma^{-1}(\Xi^N)) 
&= \int_{\lp \in \gamma^{-1}(y)}\alpha_M (\Xi^N, d\lp) \\
&= \int_{\{\lp: \sum_{(x,u) \in \lp }\delta_x = N\Xi^N \} }\alpha_M (\Xi^N, d\lp)\\
&= \prod_{x_i \in N\Xi^N}\frac{1}{N}\int_{0}^{N} du_i =1
\end{aligned}
\end{equation}
With the simplified integral, it now suffices to show that the Martingale Problem 
\begin{equation}\label{Markov Mapping Theorem Martingale Condition Simplified}
C = \left\{\left(\int   f(\textbf{x},\textbf{u})\Gamma^N(d\textbf{u}),\int   A^Nf(\textbf{x},\textbf{u})\Gamma^N(d\textbf{u})\right)\right\}    
\end{equation}
has a solution.

We now claim the following lemma.

\begin{lemma}
\label{lem: representation of lookdown generator - pre-limit}
Let $\xi$ be a finite atomic measure on $\mathbb{R}^d \times [0,N]$.
For test functions of the form 
$$f_g(\xi)= \prod_{(x_i,u_i)\in \xi} g(x_i,u_i) = \exp (\langle \log g, \xi \rangle),$$
we have the equality
\begin{equation}
\int   A^Nf(\textbf{x},\textbf{u})\Gamma^N(d\textbf{u})
= \int \Pgen^N F_{\hat{g},N}(\Xi^N),
\end{equation}
where $\Pgen^N$ is the generator of the spatial process defined in Definition \ref{def2: MP definition of pre-limit}.
 \end{lemma}

With lemma \ref{lem: representation of lookdown generator - pre-limit},
the martingale problem can then be re-written as 
\begin{equation}\label{Markov Mapping Theorem Martingale Condition, projection form}
C = \left\{\left(\int F_{\hat{g},N}(\Xi^N), \int \Pgen^N F_{\hat{g},N}(\Xi^N)\right)\right\}    
\end{equation}
which has $(\eta^N_t)_{t \geq 0}$,
our spatial population process,
as a solution. 

As a result,
we can apply Markov Mapping Theorem to establish the following statements:
\begin{enumerate}
    \item There exists a $\mathcal{M}_F(\mathbb{R}^d\times[0,N])$-valued process $(\lp^N_t)_{t \geq 0}$
    such that 
    $M^N_t:=    f(\xi^N_t)-   f(\xi^N_0)-\int_{0}^{t}  A^Nf(\xi^N_s)ds$ is a martingale for all $f\in D(A^N)$,
    \item The process $(\gamma_M \circ \xi^N_t)_{t \geq 0}= \left(\Xi^N_t\right)_{t \geq 0}$ 
    has the same distribution in $\mathcal{D}_{\mathcal{M}(\mathbb{R}^d)}[0,\infty)$ as $(\eta^N_t)_{t \geq 0}$ defined in Definition \ref{def: MP definition of pre-limit}.
    \item $\mathbb{P}\{\xi^N_t \in B | \mathcal{F}^{\Xi^N}_t\}=\alpha_M(\Xi^N(t),B), ~~~B \in \mathcal{B}(\mathcal{M}(\IR^d \times [0,N])).$
\end{enumerate}
Note that in particular,
from the third statement,
we can establish that for $h(x,u) \in \mathcal{C}_{c}(\mathbb{R}^d \times[0,N])$ and $h \geq 0$,

\begin{equation}\label{Markov Mapping Theorem Cox Condition}
\begin{aligned}
\mathbb{E}\left[e^{-\int_{S \times [0,N]} h(x,u) \xi^{N}(dx \times du)}\bigg| \mathcal{F}^{\Xi^{N}}_t\right]
&=\mathbb{E}\left[\prod_{(x,u)\in\xi^N_t}e^{-h(x,u)}\bigg| \mathcal{F}^{\Xi^{N}}_t\right]\\
&=\prod_{x \in N\Xi^N_t} \frac{1}{N} \int_{0}^N. e^{-h(x_i,u_i)} du_i
&= e^{-\langle F^N_h(x), \Xi^N(dx)\rangle},
\end{aligned}
\end{equation}
where 
$$F^{N}_h(x):=-N\log \left(\frac{1}{N}\int_{0}^{N}e^{-h(x,u)}du\right)=-N\log \left(1-\frac{1}{N}\int_{0}^{N}(1-e^{-h(x,u)})du\right).$$

Therefore we only need to prove lemma \ref{lem: representation of lookdown generator - pre-limit}.
The procedure to prove Lemman \ref{lem: representation of lookdown generator - pre-limit}
is rather standard and set out in \cite{kurtz/rodrigues:2011},
albeit we will face more technicalities for this setting.

For tidy computations in this proof, 
we recall the notation that an individual at $x$ gives birth at rate
$$
    \gamma^{\mathfrak{m}}_{\theta}(x,\eta) := \theta \gamma(x, \smooth{\gamma} \eta(x))
    \int r(y, \smooth{r} \eta(y)) q_{\theta}(x, dy) ,
$$
and that offspring disperse according to the kernel
$$
    q_\theta^\mathfrak{m}(x,\eta,  dy)
    :=
    \frac{
        r(y, \smooth{r} \eta(y)) q_\theta(x, dy)
    }{
        \int r(z, \smooth{r} \eta(z)) q_\theta(x, dz)
    } .
$$

We will also write $\Xi, \Gamma$ instead of $\Xi^N, \Gamma^N$,
bearing in mind the dependence of $\Xi, \Gamma$ on $N$.

\begin{proof}[Proof to Lemma \ref{lem: representation of lookdown generator - pre-limit}]

Recall that the lookdown generator of our spatial-level process satisfies
\begin{align}
A^Nf(\xi)=&
f(\xi)\sum_{(x,u)\in\xi}2N^{-1}\theta\gamma^{\mathfrak{m}}_{\theta}(x,\Xi)\times
\int_u^{
N}\Bigg(\frac 12\frac{g(x,v_1)}{g(x,u)}\int_{\IR^d} (g(y,u)-g(x,u))q^{\mathfrak{m}}_{\theta}(x,\Xi ,dy) \nonumber \\
& \qquad \qquad \qquad \qquad +\int_{\IR^d} \left(\frac{g(y,v_1)+g(x,v_1)}{2}-1\right)q^{\mathfrak{m}}_{\theta}(x,\Xi,dy)\Bigg)dv_1 \nonumber\\
&+f(\xi )\sum_{(x,u)\in\xi}\,\left(N^{-1} \theta \gamma^{\mathfrak{m}}_{\theta}(x,\Xi) u^2 -b_{\theta}(x,\Xi)u\right)\frac {\partial_u g(x,u)}{g(x,u)},
\end{align}
where $f(\xi) =\prod_{(x,u) \in \xi}g(x,u)$.
 
For easier computation,
we break the generator into three parts, 
\begin{equation}
\begin{aligned}
A^N_1f(\xi)=&f(\xi)\sum_{(x,u)\in\xi}2N^{-1}\theta\gamma^{\mathfrak{m}}_{\theta}(x,\Xi)\times
\int_u^{
N}\Bigg(\frac 12\frac{g(x,v_1)}{g(x,u)}\int_{\IR^d} (g(y,u)-g(x,u))q^{\mathfrak{m}}_{\theta}(x,\Xi ,dy)\Bigg)dv_1,\\
A^N_2f(\xi)=&f(\xi)\sum_{(x,u)\in\xi}2N^{-1}\theta\gamma^{\mathfrak{m}}_{\theta}(x,\Xi)\times
\int_u^{
N}\Bigg(\frac{1}{2}\int_{\IR^d} \left(\frac{g(y,v_1)+g(x,v_1)}{2}-1\right)q^{\mathfrak{m}}_{\theta}(x,\Xi,dy)\Bigg)dv_1,\\
A^N_3f(\xi)=&f(\xi )\sum_{(x,u)\in\xi}\,\left(N^{-1} \theta \gamma^{\mathfrak{m}}_{\theta}(x,\Xi) u^2 -b_{\theta}(x,\Xi)u\right)\frac {\partial_u g(x,u)}{g(x,u)},
\end{aligned}
\end{equation}
such that 
$$A^Nf(\xi)=A^N_1f(\xi)+A^N_2f(\xi)+A^N_3f(\xi).$$

We now calculate each generator evaluated under our probability kernel $\Gamma$.
\footnotesize 
\begin{align*}
&\int A^N_1f(\xi) \Gamma(d\mathbf{u})\\ &=\int \Bigg\{f(\xi)\sum_{(x,u)\in\xi}2N^{-1}\theta\gamma^{\mathfrak{m}}_{\theta}(x,\Xi)
\int_u^{
N}\Bigg(\frac 12\frac{g(x,v_1)}{g(x,u)}\int_{\IR^d}(g(y,u)-g(x,u))q^{\mathfrak{m}}_{\theta}(x,\Xi ,dy)\Bigg)dv_1\Bigg\} \Gamma(d\mathbf{u})\\
&=\sum_{(x,u)\in\xi} \int \Bigg\{\frac{f(\xi)}{g(x,u)}2N^{-1} \theta \gamma^{\mathfrak{m}}_{\theta}(x,\Xi)
\int_u^{
N}\Bigg(\frac 12 g(x,v_1)\int_{\IR^d} (g(y,u)-g(x,u))q^{\mathfrak{m}}_{\theta}(x,\Xi ,dy)\Bigg)dv_1\Bigg\} \Gamma(d\mathbf{u})\\
&=\sum_{(x,u)\in\xi}N^{-1}\theta\gamma^{\mathfrak{m}}_{\theta}(x,\Xi)\prod_{(x_i,u_i)\neq (x,u)}\hat{g}(x_i)\frac{1}{N}\int_{0}^{N}\left\{
\int_u^{
N} g(x,v_1)dv_1\times \int_{\IR^d} (g(y,u)-g(x,u))q^{\mathfrak{m}}_{\theta}(x,\Xi ,dy)\right\}du\\
&=\sum_{(x,u)\in\xi}N^{-1}\theta\gamma^{\mathfrak{m}}_{\theta}(x,\Xi)\prod_{(x_i,u_i)\neq (x,u)}\hat{g}(x_i)\frac{1}{N}\int_{\IR^d} \left\{\int_{0}^{N}
\int_u^{
N} g(x,v_1) (g(y,u)-g(x,u))dv_1du\right\}q^{\mathfrak{m}}_{\theta}(x,\Xi ,dy)
\end{align*}

\normalsize
For the second generator, we have
\footnotesize
\begin{align*}
&\int A^N_2f(\xi) \Gamma(d\mathbf{u})\\&=\int \Bigg\{f(\xi)\sum_{(x,u)\in\xi}2N^{-1}\theta\gamma^{\mathfrak{m}}_{\theta}(x,\Xi)\times
\int_u^{
N}\Bigg(\int_{\IR^d} \left(\frac{g(y,v_1)+g(x,v_1)}{2}-1\right)q^{\mathfrak{m}}_{\theta}(x,\Xi,dy)\Bigg)dv_1\Bigg\} \Gamma(d\mathbf{u})\\
&=\sum_{(x,u)\in\xi}N^{-1}\theta\gamma^{\mathfrak{m}}_{\theta}(x,\Xi)\int \Bigg\{f(\xi)\times
\int_u^{
N}\Bigg(\int_{\IR^d}\left(g(y,v_1)+g(x,v_1)-2\right)q^{\mathfrak{m}}_{\theta}(x,\Xi,dy)\Bigg)dv_1\Bigg\} \Gamma(d\mathbf{u})\\
&=\sum_{(x,u)\in\xi}N^{-1}\theta\gamma^{\mathfrak{m}}_{\theta}(x,\Xi)\prod_{(x_i,u_i)\neq (x,u)}\hat{g}(x_i) \frac{1}{N}\int_{\IR^d}\Bigg\{\int_{0}^{N}
\int_u^{
N}g(x,u) \left(g(y,v_1)+g(x,v_1)-2\right)dv_1 du\Bigg\} q^{\mathfrak{m}}_{\theta}(x,\Xi,dy).
\end{align*}  

\normalsize
For the third generator we have that 
\begin{align*}
 &\int A^N_3f(\xi)\Gamma(d\mathbf{u})\\
 =&\int f(\xi )\,\left(N^{-1} \sum_{(x,u)\in\xi}\theta \gamma^{\mathfrak{m}}_{\theta}(x,\Xi) u^2 -b_{\theta}(x,\Xi)u\right)\frac {\partial_u g(x,u)}{g(x,u)}\Gamma(d\mathbf{u}) \\
 =&\sum_{(x,u)\in\xi}\prod_{(x_i,u_i)\neq (x,u)}\hat{g}(x_i)\frac{1}{N}\int_{0}^{N}  \left(N^{-1} \theta \gamma^{\mathfrak{m}}_{\theta}(x,\Xi) u^2 -b_{\theta}(x,\Xi)u\right)\partial_u g(x,u)du \\
 =&-\sum_{(x,u)\in\xi}\prod_{(x_i,u_i)\neq (x,u)}\hat{g}(x_i)\frac{1}{N}\int_{0}^{N}  \left(2uN^{-1} \theta \gamma^{\mathfrak{m}}_{\theta}(x,\Xi) -b_{\theta}(x,\Xi)\right)\left(g(x,u)-1\right)du \\
 =&-\sum_{(x,u)\in\xi}N^{-1} \theta \gamma^{\mathfrak{m}}_{\theta}(x,\Xi)\prod_{(x_i,u_i)\neq (x,u)}\hat{g}(x_i)\frac{1}{N}\int_{0}^{N}   2ug(x,u)du \\
 &+\sum_{(x,u)\in\xi}N^{-1} \theta \gamma^{\mathfrak{m}}_{\theta}(x,\Xi)\prod_{(x_i,u_i)\neq (x,u)}\hat{g}(x_i)\frac{1}{N}\times N^2+\sum_{(x,u)\in\xi}\prod_{(x_i,u_i)\neq (x,u)}\hat{g}(x_i)b_{\theta}(x,\Xi)(\hat{g}(x)-1)
\end{align*}

By symmetry, we have
\begin{align*}
&\int_{0}^{N}
\int_u^{
N} g(x,v_1) (g(y,u)-g(x,u))dv_1du+\int_{0}^{N}
\int_u^{
N}g(x,u) \left(g(y,v_1)+g(x,v_1)-2\right)dv_1 du
\\&= \int_{0}^{N}
\int_u^{
N}g(x,u)g(y,v_1)+g(y,u)g(x,v_1)-2g(x,u) dv_1 du\\
&=\frac{1}{2}\int_{0}^{N}\int_{0}^{N}g(x,u)g(y,v_1)+g(y,u)g(x,v_1)dv_1 du - 2 \int_{0}^{N}(N-u)g(x,u)du\\
&=N^2 \hat{g}(x)\hat{g}(y) -2N^2 \hat{g}(x)+2\int_{0}^{N}ug(x,u)du.
\end{align*}

Combining the last equations,
we have 
\begin{align*}
&\int \Bigg(A^N_1f(\xi)+A^N_2f(\xi)+A^N_3f(\xi)\Bigg)\Gamma(d\mathbf{u})\\
=&  \sum_{(x,u)\in\xi}\theta\gamma^{\mathfrak{m}}_{\theta}(x,\Xi)\prod_{(x_i,u_i)}\hat{g}(x_i) \left( \int_{\IR^n} \hat{g}(y) q^{\mathfrak{m}}_{\theta}(x,\Xi,dy) - 2+\frac{1}{\hat{g}(x)}\right)\\
&+\sum_{(x,u) \in \xi } \prod_{(x_i,u_i)}\hat{g}(x_i)b_{\theta}(x,\Xi)\left(1-\frac{1}{\hat{g}(x)}\right)\\
=&  \hat{f}(\xi)\sum_{(x,u)\in\xi}\theta\gamma^{\mathfrak{m}}_{\theta}(x,\Xi) \left(\int_{\IR^n} \hat{g}(y) q^{\mathfrak{m}}_{\theta}(x,\Xi,dy) -1\right)\\
&+\hat{f}(\xi)\sum_{(x,u) \in \xi } (b_{\theta}(x,\Xi)-\theta\gamma^{\mathfrak{m}}_{\theta}(x,\Xi))\left(1-\frac{1}{\hat{g}(x)}\right)\\
=&  F_{\hat{g},N}(\Xi^N)\sum_{x \in \Xi}\theta\gamma^{\mathfrak{m}}_{\theta}(x,\Xi) \left(\int_{\IR^n} \hat{g}(y) q^{\mathfrak{m}}_{\theta}(x,\Xi,dy) -1\right) \\
&-F_{\hat{g},N}(\Xi^N)\sum_{x \in \Xi } \theta \mu_{\epsilon,\theta}(x,\Xi)\left(1-\frac{1}{\hat{g}(x)}\right)\\
=& \Pgen^N F_{\hat{g},N}(\Xi^N).
\end{align*}

The last equality holds
as the second last expression corresponds to a spatial-branching process
that branches at rate $\theta\gamma^{\mathfrak{m}}_{\theta}(x,\Xi)$,
leaving offspring around with kernel $ q^{\mathfrak{m}}_{\theta}(x,\Xi,dy)$,
with each particle having death rate given by $\theta \mu_{\epsilon,\theta}(x,\Xi)$.
This is precisely the martingale problem in Definition \ref{def: MP definition of pre-limit}.

\end{proof}


%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Markov Mapping Theorem Application on Generator $A$}\label{sec: Markov Mapping Theorem application - limit}

Now,
we turn to the generator for the limiting case,
which has the form
    \begin{equation}
    \begin{aligned}
    A f(\lp)
    =&
        f(\lp)  \sum_{(x, u) \in \xi}
        \gamma(x, \eta)
            \frac{
                \DG(g(\cdot, u) r(\cdot, \eta))(x) - g(x,u) \DG r(x,\eta)
            }{
                g(x, u)
            }
    \\ &+
        f(\lp) \sum_{(x, u) \in \xi}
        2 \alpha \gamma(x, \Xi) r(x, \Xi ) \int_u^\infty (g(x, u_1) - 1) du_1
    \\ &+
        f(\lp) \sum_{(x, u) \in \xi}
        \left(
            \alpha \gamma(x, \Xi) r(x, \Xi ) u^2
            -
            \left\{
                \gamma(x, \eta) \DG r(x, \eta) + F(x, \eta)
            \right\} u
        \right)
        \frac{\partial_u g(x, u)}{ g(x,u) }  .
    \end{aligned}
    \end{equation}

To apply Markov Mapping Theorem,
we consider Table \ref{Markov Mapping Theorem Notation Table Limit}.

\begin{table}[!h]
    \centering
    \begin{tabular}{c|c}
    Notation in \cite{kurtz/rodrigues:2011} & Notation in this proof \\
    \hline\\
      $S$   &  $\mathcal{M}(\IR^d \times [0, \infty])$\\\\
       $S_0$  &  $\mathcal{M}(\IR^d)$\\\\
       $A$ & $A$\\\\
       $(X_t)_{t \geq 0}$ & $(\xi_t)_{t \geq 0}$\\\\
       $\mu_0$ & $\xi_0 \in \mathcal{M}(\IR^d)$\\\\
       $\nu_0$ & $\xi_0 \in \mathcal{M}(\IR^d \times [0,\infty])$\\\\
       $f(\xi)$ & $f_g(\xi) := \prod_{(x,u) \in \xi} g(x,u)$\\\\
        $\gamma_M$ & $ \mathcal{M}(\IR^d \times [0, \infty]) \ni \xi \to \lim_{u_0 \to \infty}\frac{1}{u_0}\sum_{(x,u) \in \lp(\cdot, [0,u_0))} \delta_{x} \in \mathcal{M}(\IR^d)$\\\\
       $\alpha_M$ & $ \Gamma^M(\Xi,d \lp) \sim PP(\Xi(dx)\times \Lambda(du)) ~~ \text{if } \gamma_M(\xi)=\Xi$,\\
       &      where $\Lambda(du)$ is the standard Lebesgue measure on $[0,\infty)$.
    \end{tabular}
    \caption{Markov Mapping Theorem Correspondence}
    \label{Markov Mapping Theorem Notation Table Limit}
\end{table}

Once again,
we need to find solutions to the Martingale Problem 
\begin{equation}
C = \left(\int f_g(\xi) \Gamma^M(\cdot, d\xi), \int Af_g(\xi) \Gamma^M(\cdot, d\xi))\right),
\end{equation}
where $\Gamma^M(\Xi, d\lp)$ is the law of $\xi$,
which is conditionally Poisson with Cox measure $\Xi(dx) \times \Lambda(du)$.

Since we impose that $\xi$ is conditionally Poisson with Cox measure $\Xi(dx) \times \Lambda(du)$,
for any test function $l(x,u) > 0$,
\begin{equation}
\mathbb{E}\left[e^{-\int_{\mathbb{R}^d \times [0,\infty]} l(x,u) \xi(dx,du)}\big| \Xi \right] = e^{-\int_{\mathbb{R}^d} \int_{0}^{\infty} (1-e^{-l(x,u)}) du \Xi(dx)}.
\end{equation}

Note that 
$\prod_{(x,u) \in \xi} g(x,u) = e^{-\int_{\mathbb{R}^d \times [0,\infty]} -\log g(x,u) \xi(dx,du)},$
so taking $l(x,u)= -\log g(x,u)$ and $h(x,u)= \int_{0}^{\infty} (1 -g(x,u))du$,
we have the equation
\begin{equation}
\begin{aligned}
\int f_g(\xi) \alpha_M(\Xi, d\xi) 
=&  e^{-\int_{\mathbb{R}^d} \int_{0}^{\infty} (1-e^{-l(x,u)}) du \Xi(dx)}\\
=&  e^{-\int_{\mathbb{R}^d} \int_{0}^{\infty} (1-g(x,u)) du \Xi(dx)}\\
=& \exp(\langle -h(x), \Xi(dx)\rangle),
\end{aligned}
\end{equation}
and we will denote the last term as $\mathcal{E}_{-h}(\Xi)$.
Note that this notation is consistent to the test function
$F_f(\eta)$ laid out in \ref{def: MP definition of limit},
if we take $F(\cdot)=\mathcal{E}(\cdot) := \exp(\cdot)$ and $f=-h$.

We now state the crucial lemma that allows us to apply Markov Mapping Theorem.
\begin{lemma}
\label{lem: MMT for limit}
Let $\lp$ be a finite measure on $\mathbb{R}^d \times [0,\infty)$,
$\Xi$ be a finite measure on $\mathbb{R}^d$,
and $\Lambda(dx)$ be the Lesbesgue measure on  $[0,\infty)$.

We consider test functions 
$f(\xi):= \prod_{(x_i,u_i) \in \xi} g(x_i,u_i)$
with $g \in C^2_{0}(\mathbb{R}^d \times[0,\infty))$
and for which there exists $u_0$ with 
$g(x,u)=1$ for all $u>u_0$.
Furthermore, we define 
$h(x,u)= \int_{0}^{\infty} (1 -g(x,u))du$
and $\mathcal{E}_{-h}(\Xi)= \exp(\langle -h(x), \Xi(dx)\rangle)$.

We write $\Gamma^{\Xi}(d\lp)$ to be the law
on $\mathcal{M}(\mathbb{R}^d \times[0,\infty))$ 
for a conditionally Poisson random measure $\lp$
on $\IR^d\times [0,\infty)$
with Cox measure $\Xi(dx)\times \Lambda(du)$.

Then,
for generator $A$ defined in Equation \eqref{eqn:limiting_lookdown_generator},
\begin{equation}
\int Af(\lp) \Gamma^{\Xi}(d\lp) = \Pgen \mathcal{E}_{-h}(\Xi),
\end{equation}
where $\Pgen$ is defined in Definition \ref{def2: MP definition of limit}.
\end{lemma}

Given the above lemma, 
the following three statements hold true.
\begin{enumerate}
    \item There exists a $\mathcal{M}_F(\mathbb{R}^d\times[0,\infty])$-valued process $(\lp_t)_{t \geq 0}$
    such that 
    $M_t:=    f(\xi_t)-   f(\xi_0)-\int_{0}^{t}  Af(\xi_s)ds$ is a martingale for all $f\in D(A)$,
    \item The process $(\gamma_M \circ \xi_t)_{t \geq 0}= \left( \lim_{u_0 \to \infty}\frac{1}{u_0}\sum_{(x,u) \in \lp_t(\cdot, [0,u_0))} \delta_{x} \right)_{t \geq 0}$ 
    has the same distribution in $\mathcal{D}_{\mathcal{M}(\mathbb{R}^d)}[0,\infty)$ as $(\eta_t)_{t \geq 0}$ defined in Definition \ref{def: MP definition of limit}.
    \item $\mathbb{P}\{\xi_t \in B | \mathcal{F}^{\Xi_t}\}=\Gamma^{\Xi_t}(B), ~~~B \in \mathcal{B}(\mathcal{M}(\IR^d \times [0,\infty])).$
\end{enumerate}

Finally,
we recall an important equality for conditionally Poisson point processes (\cite{kurtz/rodrigues:2011} Lemma A.3.)
which will be used extensively in the upcoming proof. 

\begin{lemma}
Let $\lp =\sum_{i}\delta_{Z_i}$ be a Poisson random measure with mean measure $\nu$, 
then for $\ell \in L^{1}(\nu)$ and $g\geq0$ with $\log g \in L^{1}(\nu)$,
\begin{equation}
\mathbb{E}[\sum_{j} \ell(Z_j) \prod_{i}g(Z_i)] = \int \ell g d\nu e^{\int (g-1) d \nu}.
\end{equation}
\end{lemma}

In particular,
taking $\nu(dx,du)=\Xi(dx) \times \Lambda(du)$,
we have the equation 
\begin{equation}
\label{eq: cond_poi_sum_prod}
\begin{aligned}
\int f(\xi) \sum_{(x,u)\in \xi} \ell(x,u)\Gamma^{\Xi}(d\xi)
=& \mathbb{E}_{\xi \sim PP(\Xi \times \Lambda)}\left[\sum_{(x_j,u_j)\in \xi} \ell(x_j,u_j) \prod_{(x_i,u_i) \in \xi}g(x_i,u_i)\right]\\
 =& \left \langle \int_{0}^{\infty} \ell(x,u) g(x,u) du ,\Xi(dx) \right\rangle \exp(\langle -h(x), \Xi(dx) \rangle) \\
 =& \left \langle \int_{0}^{\infty} \ell(x,u) g(x,u) du ,\Xi(dx) \right\rangle \mathcal{E}_{-h}(\Xi)
\end{aligned}
\end{equation}
where $h(x)=\int_{0}^{\infty}(1-g(x,u))du$.

We are now ready to prove Lemma \ref{lem: MMT for limit}.

\begin{proof}[Proof to Lemma \ref{lem: MMT for limit}]
We write
$$ Af(\xi)=A_1f(\xi)+A_2f(\xi)+A_3f(\xi),$$
where the first term is
\begin{equation}
\begin{aligned}
A_1f(\xi) =&
        f(\lp)  \sum_{(x, u) \in \xi}
             \frac{
            	\gamma(x, \Xi)
                \left(\DG(g(\cdot, u) r(\cdot, \Xi))(x) - g(x,u) \DG r(x,\Xi)\right)
            }{
                g(x, u)
            }\\
            = & f(\lp)  \sum_{(x, u) \in \xi} \frac{\ell_1(x,u)}{g(x,u)},
\end{aligned}    
\end{equation}
the second term is
\begin{equation}
\begin{aligned}
A_2f(\xi)  = &
  f(\lp) \sum_{(x, u) \in \xi}
        2 \alpha \gamma(x, \Xi) r(x, \Xi ) \int_u^\infty (g(x, u_1) - 1) du_1\\
         =&  f(\lp)  \sum_{(x, u) \in \xi} \ell_2(x,u),
\end{aligned}    
\end{equation}
and the final term satisfies
\begin{equation}
\begin{aligned}
A_3f(\xi)  =&f(\lp) \sum_{(x, u) \in \xi}
                \frac{\left(
            \alpha \gamma(x, \Xi) r(x, \Xi ) u^2
            -
            \left\{
                \gamma(x, \Xi) \DG r(x, \Xi) + F(x, \Xi)
            \right\} u
        \right)
        \partial_u g(x, u)}{ g(x,u) }\\
        =& f(\lp) \sum_{(x, u) \in \xi} \frac{\ell_3(x,u)}{g(x,u)}  .
\end{aligned}    
\end{equation}

Applying Equation \eqref{eq: cond_poi_sum_prod}
to the above expressions give 
\begin{equation}
\int Af(\xi)\Gamma^{\Xi}(d\xi)
= \left \langle \int_{0}^{\infty}\left(\ell_1(x,u) + \ell_2(x,u) g(x,u) + \ell_3(x,u)\right)du ,\Xi(dx) \right\rangle \mathcal{E}_{-h}(\Xi)
\end{equation}

First of all, 
the operator $\DG$ acts on space while the integral is over levels,
so we can commute the two operators to obtain
\begin{equation}
\begin{aligned}
\int_{0}^{\infty} \ell_1(x,u) du 
=& \int_{0}^{\infty} \gamma(x, \Xi)( \DG((g(\cdot, u)-1) r(\cdot, \Xi))(x) - (g(x,u)-1) \DG r(x,\Xi))  du \\
=& \gamma(x, \Xi)\left\{ \DG\left(\int_{0}^{\infty} (g(\cdot, u)-1)du \times r(\cdot, \Xi)\right)(x) - \int_{0}^{\infty} (g(x,u)-1)du \DG r(x,\Xi)\right\} \\
=& -\gamma(x, \Xi)\{ \DG(h(\cdot) r(\cdot, \Xi))(x) - h(x) \DG r(x,\Xi)\},
\end{aligned}
\end{equation}
as $h(x,u)= \int_{0}^{\infty} (1 -g(x,u))du$.

Similarly,
\begin{equation}
\begin{aligned}
\int_{0}^{\infty} \ell_2(x,u)g(x,u) du
=&  \alpha\gamma(x, \Xi) r(x, \Xi ) \times 2 \int_{0}^{\infty} g(x,u)  \int_u^\infty (g(x, u_1) - 1) du_1  du\\
=&  \alpha\gamma(x, \Xi) r(x, \Xi ) \times I_1.
\end{aligned}
\end{equation}

Finally,
integrating $\ell_3$ by parts, 
we have
\begin{equation}
\begin{aligned}
\int_{0}^{\infty} \ell_3(x,u)du
=& \int_{0}^{\infty} \left(
            \alpha \gamma(x, \Xi) r(x, \Xi ) u^2
            -
            \left\{
                \gamma(x, \Xi) \DG r(x, \Xi) + F(x, \Xi)
            \right\} u
        \right)
        \partial_u g(x, u) du\\
=& - \alpha\gamma(x, \Xi) r(x, \Xi ) \int_{0}^{\infty} 2u (g(x, u)-1) du\\
  &+ \left\{\gamma(x, \Xi) \DG r(x, \Xi) + F(x, \Xi) \right\} \int_{0}^{\infty}(g(x,u)-1) du\\
  =&  - \alpha \gamma(x, \Xi) r(x, \Xi ) I_2 - \left\{\gamma(x, \Xi) \DG r(x, \Xi) + F(x, \Xi) \right\} h(x).
\end{aligned}
\end{equation}

Note that $I_1-I_2$ satisfies
\begin{equation}
\begin{aligned}
I_1-I_2 
=& 2 \int_{0}^{\infty} g(x,u)  \int_u^\infty (g(x, u_1) - 1) du_1  du - 2 \int_{0}^{\infty} u (g(x, u)-1) du\\
=& 2 \int_{0}^{\infty} g(x,u)  \int_u^\infty (g(x, u_1) - 1) du_1  du - 2 \int_{0}^{\infty} (g(x, u)-1) \int_{0}^{u}dv  du\\
=& 2 \int_{0}^{\infty} g(x,v)  \int_v^\infty (g(x, u) - 1) du  dv - 2 \int_{0}^{\infty} \int_{v}^{\infty}(g(x, u)-1)  dudv\\
=& 2 \int_{0}^{\infty} (g(x,v)-1)  \int_v^\infty (g(x, u) - 1) du  dv \\
=& h^2(x)
\end{aligned}
\end{equation}

Combining all the Equations above, 
we have 
\begin{equation}
\begin{aligned}
& \int_{0}^{\infty}\left(\ell_1(x,u) + \ell_2(x,u) g(x,u) + \ell_3(x,u)\right)du \\
=& -\gamma(x, \Xi)\DG(h(\cdot) r(\cdot, \Xi))(x) + \gamma(x, \Xi) h(x) \DG r(x,\Xi) -  \gamma(x, \Xi) \DG r(x, \Xi)h(x) - F(x, \Xi)  h(x) \\
& +\alpha\gamma(x, \Xi) r(x, \Xi )(I_1-I_2)\\
=&\alpha \gamma(x, \Xi) r(x, \Xi ) h^2(x) -\gamma(x, \Xi)\DG(h(\cdot) r(\cdot, \Xi))(x)  - F(x, \Xi)  h(x).
\end{aligned}
\end{equation}

To conclude, 
\begin{equation}
\begin{aligned}
&\int Af(\xi)\Gamma^{\Xi}(d\xi)\\
= & \left(\alpha \gamma(x, \Xi) r(x, \Xi ) h^2(x) -\gamma(x, \Xi)\DG(h(\cdot) r(\cdot, \Xi))(x)  - F(x, \Xi)  h(x)\right)\mathcal{E}_{-h}(\Xi)\\
= & \left(\gamma(x, \Xi)\DG(-h(\cdot) r(\cdot, \Xi))(x)  + F(x, \Xi)  (-h(x))\right)\mathcal{E}'(\langle -h, \Xi \rangle )\\
   &+ \left(\alpha \gamma(x, \Xi) r(x, \Xi ) h^2(x) \right)\mathcal{E}''(\langle -h, \Xi \rangle )\\
=& \Pgen \mathcal{E}_{-h}(\Xi),
\end{aligned}
\end{equation}
so we have established our equality. 
\end{proof}

\subsection{Convergence of Lookdown Representation}
\comment{
    I did this informal writeup of convergence of the lookdown generator
    without looking at the previous version
    mostly to double-check what we have there
    (which is probably better).
}

In Definition~\ref{defn:lookdown_mgale} we saw that the generator
of $(\lp_t)_{t \ge 0}$ is the sum of two parts,
equations \eqref{eqn:birth_generator} and \eqref{eqn:level_generator}.
Assume that there is a $u_*$ such that $g(x, u) = 1$ for all $u \ge u_*$.
We consider these two in turn.

First, the contribution from births is
\begin{align*}
\begin{split}
f(\lp)
&\mapsto
    f(\lp)
    \sum_{(x, u) \in \lp}
    2 \gamma(x, \eta)
    \bigg\{
        \frac{1}{2 N}
        \int_u^N
        g(x, u_1) du_1
        \frac{
            \theta \int_{\IR^d} (g(y, u) - g(x, u)) r(y, \eta) q(x, dy)
        }{
            g(x, u)
        }
    \\ & \qquad \qquad \qquad {}
        + \frac{\theta}{N}
        \int_u^N \int_{\IR^d}
        \left( \frac{g(y, u_1) + g(x, u_1)}{2} - 1 \right)
        r(y, \eta) q(x, dy)
    \bigg\}
    .
    \end{split}
\end{align*}
We have that
\begin{align*}
    &\theta \int_{\IR^d} (g(y, u) - g(x, u)) r(y, \eta) q(x, dy) \\
    &\qquad =
    \theta \int_{\IR^d} (r(y, \eta) g(y, u) - r(x, \eta) g(x, u)) q(x, dy)
    - \theta \int_{\IR^d} g(x, u) (r(y, \eta) - r(x, \eta)) q(x, dy) \\
    &\qquad \to
    \DG(gr)(x) - g(x) \DG r(x) \qquad \text{as }N \to \infty.
\end{align*}
i.e., to
$\DG\left(g(\cdot, u) r(\cdot, \smooth{r}\eta(\cdot))\right)(x)
- g(\cdot, u) \DG\left(r(\cdot, \smooth{r}\eta(\cdot))\right)(x)$.
Furthermore, by our assumption on $g$,
$\int_u^N g(x, u_1) du_1 / N \to 1$ for all $x$ and $u$,
so the first term here converges to
\begin{align*}
    \gamma(x, \eta)
        \frac{
            \DG\left( g(\cdot, u) r(\cdot, \smooth{r}\eta(\cdot))\right)(x)
            -
            g(\cdot, x) \DG\left( r(\cdot, \smooth{r}\eta(\cdot))\right)(x)
        }{
            g(x, u)
        } .
\end{align*}
This corresponds to each particle moving according to the motion induced by
$g \mapsto \gamma \DG(gr) - g \DG r$.
\comment{TODO: explain the $r$-tilted $\DG$ motion somewhere earlier, introducing notation for it.}
As for the second term,
since $\int g(y, u_1) q(x, dy) \to g(x, u_1)$
and $\theta/N \to \alpha$,
it converges to
\begin{align*}
    2 \alpha
    \gamma(x, \eta)
    r(x, \eta) 
    \int_u^\infty
    \left( g(x, u_1) - 1 \right)
    du_1
\end{align*}
This corresponds to births at higher levels at rate $2 \alpha \gamma r$.


The remaining term is \eqref{eqn:level_generator}:
\begin{align*}
    f(\lp)
    \mapsto
    f(\lp)
    \sum_{(x, u) \in \lp}
    \left(
    \theta
        N^{-1} \gamma(x,\eta) \int_{\IR^d} r(y, \eta) q(x, dy) u^2
        -
        b(x, \eta)u
    \right)
    \frac{\partial_u g(x,u)}{g(x,u)} .
\end{align*}
The first part only uses the fact that $\theta/N \to \alpha$,
while as noted in \eqref{eqn:b_limit}, $b$ converges to $\gamma \DG r + F$,
so in total this term converges to
\begin{align*}
    f(\lp)
    \mapsto
    f(\lp)
    \sum_{(x, u) \in \lp}
    \left(
    \alpha
        \gamma(x,\eta) \int_{\IR^d} r(y, \eta) q(x, dy) u^2
        -
        \left\{
            \gamma(x, \eta) \DG r(x, \eta) + F(x, \eta)
        \right\} u
    \right)
    \frac{\partial_u g(x,u)}{g(x,u)} .
\end{align*}
In other words, the level of a particle at $x$ evolves according to
\begin{align*}
    \dot u
    =
    \alpha
        \gamma(x,\eta) \int_{\IR^d} r(y, \eta) q(x, dy) u^2
        -
        \left\{
            \gamma(x, \eta) \DG r(x, \eta) + F(x, \eta)
        \right\} u .
\end{align*}

\comment{From Terence: Note that the convergence result above gives the coefficient $\gamma(x,\eta) \int_{\IR^d} r(y, \eta) q(x, dy)$ in front of $u^2$.
As a result, the contribution by death to the quadratic variation of the superprocess is gone in this limit!
I cannot find a way to resolve the inconsistency between this limiting result and the spatial model.
Of course, we can take $\alpha=0$, so we are taking a deterministic limit. In this case, we have consistent result. 
We should discuss this in our next meeting.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage


\section{Technical Lemmas}
\subsection{Gaussian Kernel Estimates}

\begin{lemma}\label{eq: Gaussin density estimates}
\begin{equation}
 \theta H_{\theta}(x,w)\rho_r(x-w)\leq \rho_{\gamma}(x-w),
\end{equation}
where
$$H_{\theta}(x,w)=\int_{\IR^d}
    \left(
        \frac{\rho_r(y-w)}{\rho_r(x-w)}
        -
        1
    \right)^2
q_{\theta}(x,dy).$$
\end{lemma}
\begin{proof}
   
Writing out $H$,
\begin{align}
H_\theta(x, w)
=
\int
    \left(
        \exp\left(
            - \frac{1}{2\epsilon_r^2}
            \left\{
                ||y - w||^2 - ||x - w||^2
            \right\}
        \right)
        -
        1
    \right)^2
    \frac{
        e^{-\theta (y - x-\meanq)^{T}\covq^{-1}(y - x-\meanq)  / 2}
    }{
        (2 \pi/ \theta)^{d/2} |\covq|^{1/2}
    }
dy.
\end{align}
Writing $z=y-x-\meanq$, we have that 
\begin{equation}
\begin{aligned}
||y - w||^2 - ||x - w||^2 
=& ||y||^2-||x||^2 -2\langle y-x, w\rangle \\
=&||x+z+ \meanq||^2-||x||^2-2 \langle z+ \meanq, w\rangle\\
=&2\langle x, z+ \meanq \rangle + ||z+ \meanq||^2-2 \langle z+ \meanq, w\rangle \\
=& 2 \langle x-w, z+ \meanq \rangle + ||z+ \meanq||^2\\
=& 2\langle x-w+ \meanq,\meanq \rangle + 2\langle x-w+ \meanq, z \rangle +||z||^2
\end{aligned}
\end{equation}
As a result, we have 
\begin{align}
H_\theta(x, w)
=&
\int
    \left(
        \exp\left(
            - \frac{1}{2\epsilon_r^2}
            \left\{
                2 \langle x-w, z+ \meanq \rangle + ||z+ \meanq||^2
            \right\}
        \right)
        -
        1
    \right)^2
    \frac{
        e^{-\theta z^{T}\covq^{-1}z  / 2}
    }{
        (2 \pi/ \theta)^{d/2} |\covq|^{1/2}
    }
dz\\
=&\exp\left(-\frac{2}{\epsilon_r^2}\langle x-w+ \meanq,\meanq \rangle \right)\\
& \times \int
        \frac{
        \exp\left(
            - \frac{2}{\epsilon_r^2} \langle x-w+ \meanq, z \rangle
            - \frac{1}{\epsilon_r^2} ||z||^2
            \right)
            e^{-\theta z^{T}\covq^{-1}z  / 2}
    }{
        (2 \pi/ \theta)^{d/2} |\covq|^{1/2}
    }
dz\\ \label{eq: Second moment term matrix calculation}
&-2\exp\left(-\frac{1}{\epsilon_r^2}\langle x-w+ \meanq,\meanq \rangle \right)\\
& \qquad \qquad \times     \int
    \frac{ \exp\left(
             -\frac{1}{\epsilon_r^2}\langle x-w+ \meanq, z \rangle
              -\frac{1}{2\epsilon_r^2} ||z||^2
            \right)e^{-\theta z^{T}\covq^{-1}z  / 2}
        }{
        (2 \pi/ \theta)^{d/2} |\covq|^{1/2} \label{eq: first moment term matrix calculation}
    }
dz\\
&+1.
\end{align}
We now consider the integral of the form 
\begin{equation}
\int \exp(v^{T}z-C||z||^2) \frac{
        e^{-\theta z^{T}\covq^{-1}z  / 2}
    }{
        (2 \pi/ \theta)^{d/2} |\covq|^{1/2}
    }
dz   
\end{equation}
for $v \in \IR^d$ and $C>0$. 
We have 
\begin{equation}
\begin{aligned}
&\int \exp(v^{T}z-C||z||^2) \frac{
        e^{-\theta z^{T}\covq^{-1}z  / 2}
    }{
        (2 \pi/ \theta)^{d/2} |\covq|^{1/2}
    }
dz\\
=&\frac{1}{|\covq|^{1/2}|  |\covq^{-1}(I+2C\covq/\theta )|^{1/2}  }\int \exp(v^{T}z) \frac{
        e^{-\theta z^{T}\covq^{-1}(I+2C\covq/\theta )z  / 2}
    }{
        (2 \pi/ \theta)^{d/2}|\covq^{-1}(I+2C\covq/\theta )|^{-1/2} 
    }
dz\\
=& \frac{1}{|I+2C\covq/\theta |^{1/2}} \mathbb{E}_{Z}[\exp (v^{T}Z)]\\
=&\frac{1}{|I+2C\covq/\theta |^{1/2}}
\exp 
\left(
\frac{1}{2}v^{T}\covq/\theta(I+2C\covq/\theta )^{-1}v
\right),
\end{aligned}
\end{equation}
where $Z \sim N(0, \covq/\theta \times (I+2C\covq/\theta )^{-1})$.

Now writing $v=\frac{2}{\epsilon_r^2}( w-x-\meanq)$
and $C=\frac{1}{\epsilon_r^2}$,
we have the expression in Equation
\eqref{eq: Second moment term matrix calculation} is 
$$\frac{1}{|I+2\covq/(\epsilon_r^2\theta) |^{1/2}}
\exp 
\left(
\frac{2}{\epsilon_r^4}( w-x-\meanq)^{T}\covq/\theta\left(I+2\covq/(\epsilon_r^2\theta)\right)^{-1}( w-x-\meanq)
\right).$$
Similarly, writing $v=\frac{1}{\epsilon_r^2}( w-x-\meanq)$
and $C=\frac{1}{2\epsilon_r^2}$,
we have the expression in Equation
\eqref{eq: first moment term matrix calculation} is 
$$\frac{1}{|I+\covq/(\epsilon_r^2\theta) |^{1/2}}
\exp 
\left(
\frac{1}{2\epsilon_r^4}( w-x-\meanq)^{T}\covq\left(I+\covq/\theta/(\epsilon_r^2\theta)\right)^{-1}( w-x-\meanq)
\right).$$
As a result, we have 
\begin{align*}
&H_{\theta}(x,w)\\
=& \frac{1}{|I+2\covq/(\epsilon_r^2\theta) |^{1/2}}
\exp 
\left(
\frac{2}{\epsilon_r^4}( w-x-\meanq)^{T}\covq/\theta\left(I+2\covq/(\epsilon_r^2\theta)\right)^{-1}( w-x-\meanq)
\right)\\
&-2\frac{1}{|I+\covq/(\epsilon_r^2\theta) |^{1/2}}
\exp 
\left(
\frac{1}{2\epsilon_r^4}( w-x-\meanq)^{T}\covq/\theta\left(I+\covq/(\epsilon_r^2\theta)\right)^{-1}( w-x-\meanq)
\right)\\
&+1.
\end{align*}
If we write $F(1/\theta)=H_{\theta}(x,w)$, i.e.
\begin{align*}
&F(s)\\
=& \frac{1}{|I+2/\epsilon_r^2 \covq s |^{1/2}}
\exp 
\left(
\frac{2}{\epsilon_r^4}( w-x-\meanq)^{T}\covq s \left(I+2/\epsilon_r^2 \covq s\right)^{-1}( w-x-\meanq)
\right)\\
&-2\frac{1}{|I+2/\epsilon_r^2 \covq s |^{1/2}}
\exp 
\left(
\frac{1}{2\epsilon_r^4}( w-x-\meanq)^{T}\covq s \left(I+2/\epsilon_r^2 \covq s\right)^{-1}( w-x-\meanq)
\right)\\
&+1\\
&=G_1(s)+2G_2(s)+1
\end{align*}
we can check that $F(0)=0$.
As a result, 
$$\theta H_{\theta}(x,w)\rho_r(x-w)= \theta F(1/\theta)\rho_r(x-w)=F'(s)\rho_r(x-w),$$
for some $s \in [0, 1/\theta]$.

In Equation \eqref{eq: gamma-r convolution comparison}, we have established that 
$\rho_r(x-w) / \rho_\gamma(x-w) = C' \exp(-K (x-w)^2 / 2)$
with $K = \frac{1}{\epsilon_r^2} - \frac{1}{\epsilon_\gamma^2}$.
Therefore it suffices to show that for all $x,w \in \IR^d$ and $s \in [0, 1/\theta]$, 
\begin{align}\label{eqn:goal3}
    F'(s) \exp\left(-A (x-w)^2 \right) \le C .
\end{align}

We consider how to differentiate function $G$ of the form 
$$G(s):=\frac{1}{|I+As|^{1/2}}\exp(v^{T}Bs(I+Cs)^{-1}v),$$
where $A,B,C$ are matrices with non-negative entries and $I$ is the identity matrix.

Since 
\begin{equation}
\begin{aligned}
G'(s)=& - \frac{|A|}{2|I+As|^{3/2}}\exp(v^{T}Bs(I+Cs)^{-1}v)\\
      & + \frac{1}{|I+As|^{1/2}}\exp(v^{T}Bs(I+Cs)^{-1}v)\\
      & \qquad \times \left\{ v^{T}B(I+Cs)^{-1}v
                            - sv^{T}BC(I+Cs)^{-2}v
                    \right\}\\
      &=\left(- \frac{|A|}{2|I+As|}+v^{T}B(I+Cs)^{-2}v
        \right)\\
      & \qquad \times
        \frac{1}{|I+As|^{1/2}}\exp(v^{T}Bs(I+Cs)^{-1}v),
\end{aligned}
\end{equation}
using non-negativity of $A,B,C$ we have
\begin{equation}
\label{eq: bounds on G'}
|G'(s)| \leq (|A|+||B||_{2}||v||_2^2)G(s).    
\end{equation}
As a result, we have that 
\begin{equation}
\label{eq: bounds on G}
G(s) \leq K e^{|A|+||B||_{2}||v||_2^2)s}, ~~   G'(s)(|A|+||B||_{2}||v||_2^2)K e^{|A|+||B||_{2}||v||_2^2)s}.  
\end{equation}

Finally note that, 
$$F'(s) \leq G_1'(s)+2G_2'(s),$$
where 
$$A_1= 2\epsilon_r^{-2} \covq, ~ v_1=\epsilon_r^{-2}(w-x-\meanq), ~B_1 = 2\covq, ~C_1=2\epsilon_r^{-2} \covq,$$
and 
$$A_2= 2\epsilon_r^{-2} \covq, ~ v_2=\epsilon_r^{-2}(w-x-\meanq), ~B_2 = \covq/2, ~C_2=2\epsilon_r^{-2} \covq,$$
Substituting these quantities on Equation \eqref{eq: bounds on G} shows that Equation \eqref{eqn:goal3} is satisfied as long as $\epsilon_r < \epsilon_\gamma$.
 
\end{proof}
\section{Topological Results on Lookdown Representations}
\label{sec: Topologies on Lookdown}
\subsection{Topology for convergence of lookdown representations}

Recall that
the lookdown representation has a spatial component and a level component.
Specifically, for fixed $N>0$ and $t>0$,
the lookdown representation $\xi^N(t)$ 
is a finite measure on $\overline{\mathbb{R}^d} \times [0,N]$.
Furthermore, the level of a particle in our lookdown representation can shoot to infinity in finite time.
Therefore, to characterise convergence of $\xi^N(t)$ as $N \to \infty$,
we need to impose an appropriate vague topology on $\mathcal{M}(\overline{\mathbb{R}^d} \times [0,\infty))$,
the space of finite measures on $\overline{\mathbb{R}^d} \times [0,\infty)$.
Similarly, to characterise convergence of the level process of individual lineages, we consider the vague topology on $[0,\infty)$.



We now refer to the vague topology on measure spaces introduced in P564, \S A2 in \cite{kallenberg1997foundations}.
Let $S$ be a locally compact Polish space and $\hat{S}$ be the class of bounded sets in $S$.
We consider the family $C^{+}_c(S)$ of positive continuous functions with compact support. It is known that $C^{+}_c$ is separable in the uniform metric.
We now consider $\mathcal{M}(S)$, the class of measures on $S$ that are locally finite.
The \textit{vague topology} on $\mathcal{M}(S)$ is generated by the the maps $\mu \to \mu f := \int f d \mu$, where $f\in C^{+}_{c}(S)$.
In other words, convergence of $\mu^n \to \mu$ in the vague topology holds if 
$$\int f d\mu^n \to \int f d\mu$$
for all $f\in C^{+}_{c}(S)$.

We now introduce an important theorem on compactness in the vague topology.
\begin{theorem}[Theorem A.23 in \cite{kallenberg1997foundations}]
For any locally compact, second-countable Hausdorff Space $S$, 
we have
\begin{itemize}
\item $\mathcal{M}(S)$ is Polish in the vague topology,
\item a set $A\subset \mathcal{M}(S)$ is vaguely relatively compact iff $\sup_{\mu\in A} \mu f < \infty$ for all $f \in C^{+}_{c}$
\item if $\mu_n \to \mu$ in the vague topology and $B \in \hat{S}$ with $\mu \partial B = 0$, then $\mu_n B \to \mu B$
\end{itemize}
\end{theorem}

To characterise convergence of lookdown representations,
we take $S = \overline{\mathbb{R}^d} \times [0,\infty)$.
We now identify compact sets under the vague topologies.

\begin{lemma}
\label{lem: vague compact sets}
For any fixed $N.0, K>0$, the set 
$$\Gamma_{N,K} := \left\{\mu \in \mathcal{M}(\overline{\mathbb{R}^d} \times [0,\infty)):  \frac{1}{N}\mu(\overline{\mathbb{R}^d}\times [0,N]) \leq K \right\}$$
is vaguely compact in $\mathcal{M}(S)$.
\end{lemma}

\begin{proof}
It suffices to show that $\Gamma_K$ is vaguely relatively compact and vaguely closed.
For any fixed $f \in C^{+}_c$, $f$ is bounded above uniformly by $||f||_{\infty}$.
Therefore, 
we have that $ \sup_{\mu \in  \Gamma_K} \mu f \leq  ||f||_{\infty} \mu(\overline{\mathbb{R}^d}\times [0,N])  \leq ||f|| K N < \infty$,
so $\Gamma_{N,K}$ is vaguely relatively compact.

Now it suffices to prove that $\Gamma_K$ is closed under the vague topology.
Consider a vaguely converging sequence $(\mu_n)_{n \geq 1}$ in $\Gamma_{N,K}$
with limit $\mu_n \to \mu \in \mathcal{M}(S)$.
We pick some test functions $\{f_m : m \geq 0 \}$ monotonically increasing to $1_{\overline{\mathbb{R}^d}\times[0,N]}(x)$.

Since for each $n>0$ and $m > 0$,
$\frac{1}{N}\langle f_m , \mu_n \rangle < K$,
by Dominating Convergence Theorem,
\begin{equation}
\frac{1}{N}\mu(\overline{\mathbb{R}^d}\times [0,N])
= \lim_{m \to \infty} \frac{1}{N}\langle f_m , \mu \rangle
=\lim_{m \to \infty} \lim_{n \to \infty} \frac{1}{N}\langle f_m , \mu_n \rangle
\leq K,
\end{equation}
and so $\mu \in \Gamma_{N,K}$.
Therefore $\Gamma_{N,K}$ is vaguely compact. 
\end{proof}


We now include another theorem that is very relevant to establishing tightness of $\mathcal{M}(S)$-valued processes in the Skorokhod topology. It can be found as Theorem 3.9.1 in \cite{EK}.

\begin{theorem}[\cite{EK} Theorem 3.9.1]
\label{teo: EK tightness theorem}
Let $(S,r)$ be complete and separable, and let $\{X_{\alpha}\}$ be a family of processes with sample paths in $D_{S}([0,\infty))$.
Suppose that the compact containment condition holds. That is, for every $\eta > 0$ and $T > 0$,
there exists a compact set $\Gamma_{\eta, T} \subset E$ for which 
\begin{equation}
\inf_{\alpha} P \{ X_{\alpha}(t) \in \Gamma_{\eta,T}  ~~ \text{for } 0\leq t \leq T\} \geq 1- \eta
\end{equation}
Let $H$ be a dense subset of $\overline{C}(E)$ (bounded continuous functions on $E$) in the topology of uniform convergence on compact sets. 
Then $\{X_{\alpha}\}$ is relatively compact if and only if $\{f(X_{\alpha})\}$ is relatively compact (as a family of processes with sample paths in $\mathcal{D}_{\mathbb{R}}([0,\infty))$) for each $f \in H$.
\end{theorem}


To apply Theorem \ref{teo: EK tightness theorem},
we need to identify dense subset of $\overline{C}(\mathcal{M}(\overline{\mathbb{R}^d} \times [0,\infty)))$.
Since the space $\mathcal{M}(\overline{\mathbb{R}^d} \times [0,\infty))$ is not compact but locally compact,
we will apply Stone-Weierstrass theorem for locally compact space.

\begin{lemma}[Stone-Weierstrass Theorem for locally compact space]
\label{lem: SW locally compact}
Let $X$ be a locally compact Hausdorff space, and consider the set  $C_{\infty}(X, \mathbb{R})$ of real-valued continuous functions which vanishes at infinity. 
Let $\mathcal{A}$ be a subalgebra in $C_{\infty}(S)$. Then $\mathcal{A}$ is dense in $C_{\infty}(X, \mathbb{R})$ if and only if it separates points and vanishes nowhere.
\end{lemma}

With this version of Stone-Weierstrass Theorem, we can prove the following lemma.
\begin{lemma}
\label{lem: uniform compact density}
We write $\mathcal{A}$ for the algebra (closed under multiplication and addition) generated by 
the collection of functions $\xi \mapsto \langle f, \xi \rangle$, where $ f \in C_c(\overline{\mathbb{R}^d} \times [0,\infty))$ is differentiable in $u$ and smooth in $x$ with bounded support.
Then $\mathcal{A}$ is dense in $C(\mathcal{M}(\overline{\mathbb{R}^d} \times [0,\infty)))$ under the topology of uniform convergence on compact sets.
\end{lemma}
\begin{proof}
Since $\mathcal{A}$ contains the functions $\langle \rho_{\varepsilon}*\mathbbm{1}_{A} , \mu \rangle$,
where $\rho$ is some mollifier and $A$ is a compact set, it clearly
separates points and vanishes nowhere.
Therefore, $\mathcal{A}$ is dense in $C_{\infty}(\mathcal{M}(\overline{\mathbb{R}^d} \times [0,\infty)), \mathbb{R})$.

Furthermore, under the topology of uniform convergence on compact sets, 
$C_{\infty}(\mathcal{M}(\overline{\mathbb{R}^d} \times [0,\infty)), \mathbb{R})$ 
contains the mollified indicator functions $\langle \rho_{\varepsilon}*\mathbbm{1}_{A} , \mu \rangle$,
which are dense in $C(\mathcal{M}(\overline{\mathbb{R}^d} \times [0,\infty)))$.
\end{proof}

Finally, we include another technical lemma on the products of real-valued stochastic processes. 
\begin{lemma}
\label{teo: product tightness}
Let $\{(X^n_t)_{t \geq 0}: n \geq 0\}, \{(Y^m_t)_{t \geq 0}: m \geq 0\}$ be two tight sequences of real-valued process under the Skorokhod topology.
Then the sequence of processes $\{(X^n_tY^n_t)_{t \geq 0}: n \geq 1\}$ is tight under the Skorokhod topology.
\end{lemma}

\begin{proof}
We apply the Aldous Criterion of tightness on the real-valued sequences
$\{(X^n_tY^n_t)_{t \geq 0}: n=1,2,.. \}$. 
It suffices to show that
for fixed sequences of stopping time $\tau_n$ bounded above by $T$,
for each $\varepsilon>0$,
there exists $\delta > 0$ and $n_0$ such that
\begin{equation}
\sup_{n \geq n_0}\sup_{\theta \in [0, \delta]} \mathbb{P}\left[|X^n_{\tau_n+\theta}Y^n_{\tau_n+\theta} - X^n_{\tau_n}Y^n_{\tau_n} | > \varepsilon \right] \leq \varepsilon.
\end{equation}

First note that 
\begin{equation}
|X^n_{\tau_n+\theta}Y^n_{\tau_n+\theta} - X^n_{\tau_n}Y^n_{\tau_n} |
= |X^n_{\tau_n+\theta}(Y^n_{\tau_n+\theta}-Y^n_{\tau_n}) +Y^n_{\tau_n}( X^n_{\tau_n+\theta}- X^n_{\tau_n})|,
\end{equation}
therefore
\begin{equation}
\begin{aligned}
& \mathbb{P}\left[|X^n_{\tau_n+\theta}(Y^n_{\tau_n+\theta}-Y^n_{\tau_n}) +Y^n_{\tau_n}( X^n_{\tau_n+\theta}- X^n_{\tau_n})| > \varepsilon \right]\\
 \leq &
\mathbb{P}\left[\max\{|X^n_{\tau_n+\theta}(Y^n_{\tau_n+\theta}-Y^n_{\tau_n})|, | Y^n_{\tau_n}( X^n_{\tau_n+\theta}- X^n_{\tau_n}) |\}> \varepsilon/2 \right]\\
\leq & \mathbb{P}\left[|X^n_{\tau_n+\theta}(Y^n_{\tau_n+\theta}-Y^n_{\tau_n})|> \varepsilon/2 \right] + \mathbb{P}\left[ | Y^n_{\tau_n}( X^n_{\tau_n+\theta}- X^n_{\tau_n}) |> \varepsilon/2 \right]
\end{aligned}
\end{equation}

By Corollary 3.7.1 in \cite{EK}, both $\{(X^n_t)_{t \geq 0}: n \geq 0\}, \{(Y^m_t)_{t \geq 0}: m \geq 0\}$ satisfies the compact containment condition, i.e. so for any $\varepsilon'>0$ and fixed time $T> 0$, there exists $K_{\varepsilon',T}>0$ such that 

\begin{equation}
 \limsup_{n \to \infty} \mathbb{P}\left\{ \sup_{t\in [0,T]}X^n_t > K_{\varepsilon',T} \right\} < \epsilon',
\end{equation}
and 
\begin{equation}
 \limsup_{n \to \infty} \mathbb{P}\left\{ \sup_{t\in [0,T]}Y^n_t > K_{\varepsilon',T} \right\} < \epsilon'.
\end{equation}

Conditioning on the event $\left\{ \sup_{t\in [0,T]}Y^n_t > K_{\varepsilon',T} \right\}$ and $\left\{ \sup_{t\in [0,T]}Y^n_t > K_{\varepsilon',T} \right\},$

\begin{equation}
\begin{aligned}
& \mathbb{P}\left[|X^n_{\tau_n+\theta}(Y^n_{\tau_n+\theta}-Y^n_{\tau_n})|> \varepsilon/2 \right] + \mathbb{P}\left[ | Y^n_{\tau_n}( X^n_{\tau_n+\theta}- X^n_{\tau_n}) |> \varepsilon/2 \right]\\
\leq & \mathbb{P}\left[|(Y^n_{\tau_n+\theta}-Y^n_{\tau_n})|> \frac{\varepsilon}{2K_{\varepsilon',T}}\right] + \mathbb{P}\left[ | Y^n_{\tau_n}( X^n_{\tau_n+\theta}- X^n_{\tau_n}) |> \frac{\varepsilon}{2K_{\varepsilon',T}}\right] + 2 \varepsilon'\\
\leq & \frac{\varepsilon}{K_{\varepsilon',T}}+2 \varepsilon',
\end{aligned}
\end{equation}
and by tightness of $(X^n_t, Y^n_t)_{t \geq 0}$,
there exists $\delta > 0$ and $n \geq n_0$ such that the last term is smaller than $\varepsilon$ for 
for all $\theta < \delta$ and $n\geq n_0$.
\end{proof}


When we work with lookdown representations, we work on test functions of the form
\begin{equation}
F_g(\xi) = \prod_{(x,u) \in \xi} g(x,u),
\end{equation}
where $g(x,u)$ is differentiable in $u$, smooth in $x$.
and satisfies the property that $0\leq g(x,u) \leq 1$,
and that there exists $u^*$ such that for all $u > u^*$,
$g(x, u) =1$.

In fact, there is a one-to-one correspondence between $C^{+}_c(S)$
and test functions of the form $F_g$.
For all $f \in C^{+}_c(S)$,
if we write $g = e^{-f}$,
then
\begin{equation}
F_g(\xi)= e^{-\langle f, \xi \rangle}.
\end{equation}

Furthermore,
we know that for any $x_1, x_2 \in [0,N]$,
$$|x_1-x_2|\geq |e^{-x_1}-e^{-x_2}| \geq e^{-N}|x_1-x_2|,$$
so we can control $\langle f, \xi^n_t \rangle $ by controlling $F_g(\xi^n_t)$ as long as 
there exists some $N > 0$ such that $\langle f, \xi^n_t \rangle < N$.

With the above observation,
we have the following lemma.

\begin{lemma}
We consider a sequence $(\xi^n_t)_{t\geq 0}$ of $\mathcal{M}(\overline{\mathbb{R}^d} \times [0,\infty))$-valued cadlag processes.

Assume that the sequence $(\xi^n_t)_{t\geq 0}$ satisfies the compact containment condition. That is,
for all $\varepsilon > 0$ and fixed time $T > 0$, there exists $N, K> 0$ such that 
\begin{equation}
\liminf_{n \to \infty} \mathbb{P}\{ \xi^n_t \in \Gamma_{N,K} \text{ for all } t\in [0,T]\} > 1-\varepsilon,
\end{equation}
where $\Gamma_{N,K}$ is the compact set in Lemma \ref{lem: vague compact sets}

Assume that for all test functions of the form $F_g$,
the sequence $\{(F_g(\xi^n_t))_{t \geq}: n=1,2,...\}$ of real-valued cadlag processes are relatively compact. 

Then  $(\xi^n_t )_{t\geq 0}$ is relatively compact in $\mathcal{D}_{\mathcal{M}(\overline{\mathbb{R}^d} \times[0,\infty))}([0,T])$.
\end{lemma}

\begin{proof}
By Lemma  \ref{lem: uniform compact density}, \ref{teo: product tightness}, and Theorem 3.9.1 in \cite{EK},
 $(\xi^n_t )_{t\geq 0}$ is relatively compact in $\mathcal{D}_{\mathcal{M}(\overline{\mathbb{R}^d} \times[0,\infty))}([0,T])$
 if we can prove tightness on the real-valued sequences
$\{(f(\xi^n_t))_{t \geq 0}: n=1,2,.. \}$.

Applying the Aldous Criterion, it suffices to show that
for fixed sequences of stopping time $\tau_n$ bounded above by $T$,
for each $\varepsilon>0$,
there exists $\delta > 0$ and $n_0$ such that
\begin{equation}
\sup_{n \geq n_0}\sup_{\theta \in [0, \delta]} \mathbb{P}\left[|\langle f, \xi^n_{\tau_n+\theta}\rangle - \langle f, \xi^n_{\tau_n}\rangle | > \varepsilon \right] \leq \varepsilon.
\end{equation}

First we pick $N,M>0$ such that 
 $$\sup_{n \geq M} \mathbb{P}\left\{ \sup_{t\in [0,T]}\langle f, \xi^n_t \rangle > N \right\} < \epsilon /2 .$$
Then, we can partition the event $\{|\langle f, \xi^n_{\tau_n+\theta}\rangle - \langle f, \xi^n_{\tau_n}\rangle | > \varepsilon\}$ into two subsets according to the event 
$ \sup_{t\in [0,T]}\langle f, \xi^n_t \rangle > N$.

By the compact containment condition, 
we know that for any $f \in C^{+}_{c}(\overline{\mathbb{R}^d} \times [0,\infty))$ and $\epsilon > 0$, there exists $N>0$ (dependent on $f$ and $\epsilon$), 
such that 
\begin{equation}
\label{eq: vague compact containment condition}
 \limsup_{n \to \infty} \mathbb{P}\left\{ \sup_{t\in [0,T]}\langle f, \xi^n_t \rangle > N \right\} < \epsilon.
\end{equation}

Therefore, for all $n>M$,
\begin{equation}
\begin{aligned}
 &\mathbb{P}\left[|\langle f, \xi^n_{\tau_n+\theta}\rangle - \langle f, \xi^n_{\tau_n}\rangle | > \varepsilon \right]\\
 =& \mathbb{P}\left[|\langle f, \xi^n_{\tau_n+\theta}\rangle - \langle f, \xi^n_{\tau_n}\rangle | > \varepsilon ,  \sup_{t\in [0,T]}\langle f, \xi^n_t \rangle > N \right]\\
 & + \mathbb{P}\left[|\langle f, \xi^n_{\tau_n+\theta}\rangle - \langle f, \xi^n_{\tau_n}\rangle | > \varepsilon,  \sup_{t\in [0,T]}\langle f, \xi^n_t \rangle \leq N \right]\\
< & \varepsilon/2 + \mathbb{P}\left[|e^{-\langle f, \xi^n_{\tau_n+\theta}\rangle} - e^{-\langle f, \xi^n_{\tau_n}\rangle }| > e^{-N}\varepsilon,  \sup_{t\in [0,T]}\langle f, \xi^n_t \rangle \leq N \right]\\
< & \varepsilon/2 + \mathbb{P}\left[|F_g(\xi^n_{\tau_n+\theta})-F_g(\xi^n_{\tau_n})| > e^{-N}\varepsilon\right].
\end{aligned}
\end{equation}

Finally, by tightness of the processes $\{F_g(\xi^n_t): n = 1,2,...\}$, 
we can pick large enough $n_0 > M$ and $\delta > 0$
such that 
$$\sup_{n \geq n_0}\sup_{\theta \in [0,\delta]}\mathbb{P}\left[|F_g(\xi^n_{\tau_n+\theta})-F_g(\xi^n_{\tau_n})| > e^{-N}\varepsilon\right]< e^{-N}\varepsilon,$$
so
\begin{equation}
\sup_{n \geq n_0}\sup_{\theta \in [0, \delta]} \mathbb{P}\left[|\langle f, \xi^n_{\tau_n+\theta}\rangle - \langle f, \xi^n_{\tau_n}\rangle | > \varepsilon \right] \leq \varepsilon / 2 + e^{-N} \varepsilon < \varepsilon.
\end{equation}

We have proved relative compactness of $\{(\langle f, \xi^n_t \rangle )_{t \geq 0}: n=1,2,.. \}$, and thus tightness of 
 $\{(\xi^n_t )_{t \geq 0}: n=1,2,.. \}.$
\end{proof}


\bibliographystyle{plainnat}
\bibliography{refs.bib,plr_refs.bib}


\end{document}
